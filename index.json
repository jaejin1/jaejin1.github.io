[{"categories":null,"content":"Jaejin Lee developer 👨‍💻 updated: Jan 09, 2021 ","date":"2021-01-09","objectID":"/about/:0:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Work History 🔥 ","date":"2021-01-09","objectID":"/about/:1:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Cloud Engineer @ DreamusCompany(FLO) 2020.2.17 ~ Present ","date":"2021-01-09","objectID":"/about/:1:1","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Junior Software Developer @ ICONLOOP 2018.10 ~ 2020.2.14 ","date":"2021-01-09","objectID":"/about/:1:2","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Intern @ ICONLOOP 2018.07 ~ 2018.09 ","date":"2021-01-09","objectID":"/about/:1:3","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Undergraduate Research Assistant @ UNClab 2016.06 ~ 2018.06 ","date":"2021-01-09","objectID":"/about/:1:4","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Education 🎓 2013-2019 Hongik University Computer Information Communications Engineering ","date":"2021-01-09","objectID":"/about/:2:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Links 🔗 Github https://www.github.com/jaejin1 Blog https://jaejin1.github.io/ Linkedin https://www.linkedin.com/in/jaejin-lee-425161173/ Thanks for reading! 👋 ","date":"2021-01-09","objectID":"/about/:3:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":["os","linux"],"content":"파티션이란 연속된 저장 공간을 하나 이상의 연속되고 독립된 영역으로 나누어서 사용할 수 있도록 정의한 규약.","date":"2020-06-28","objectID":"/partition/","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파티션이란 연속된 저장 공간을 하나 이상의 연속되고 독립된 영역으로 나누어서 사용할 수 있도록 정의한 규약. partitionpartition \" partition 파티션을 나누기 위해서는 저장장치에 연속된 공간에 있어야한다. 하나의 하드디스크에는 여러 개의 파티션을 나눌 수 있지만, 두 개의 하드디스크를 가지고 하나의 파티션을 만들 순 없다. ","date":"2020-06-28","objectID":"/partition/:0:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"사용 용도 하나의 물리적인 디스크를 여러 논리 영역으로 나누어 관리를 용이하게 함. OS영역과 Data 영역으로 나누어 OS 영역만 따로 포맷 및 관리를 하기 위해 사용 여러 OS를 설치하기 위해 사용 하드 디스크의 물리적인 배드 섹터로 특정 영역을 잘라서 사용하기 위해 사용한다. ","date":"2020-06-28","objectID":"/partition/:1:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"partition vs volume 볼륨은 OS나 Application등에서 이용할 수 있는 저장공간, 즉 섹터의 집합이다. 연속된 공간이 아니여도 볼륨으로 볼 수 있다. 즉 2개의 하드디스크를 사용하는 경우 하나의 하드디스크처럼 인식하여 사용할 수 있다. 보통 partition에 FS를 설정해주면 volume으로 보기에 partition역시 volume으로 볼 수 있다. hard diskhard disk \" hard disk ","date":"2020-06-28","objectID":"/partition/:2:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"MBR Master Boot Record 각 boot record는 각 partition의 첫 번째 섹터에 위치하며, 주로 해당 partition의 설치된 OS를 부팅하는 역할을 하게 된다. 즉 OS 실행을 위한 boot loader를 호출하는 것이다. partition을 나누지 않은 경우라면 boot record는 MBR에 있을 것이다. 단일 partition을 사용하는 경우 (플로피) boot record는 1개만 있을 것이므로 MBR이 필요없다. MBRMBR \" MBR MBR 호출과정 MBR 호출과정MBR 호출과정 \" MBR 호출과정 ","date":"2020-06-28","objectID":"/partition/:3:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"linux partition Primary partition 주 영역 파티션 4개까지 생성가능 (1개 ~ 4개) Extend partition 확장 영역 파티션 1개 까지 생성가능 Ligical partition 논리 영역 파티션 SCSI 한 개의 총 partition 15개만 넘지 않게 사용가능 12개 이상은 좋지 않다고함 ","date":"2020-06-28","objectID":"/partition/:4:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"실습 ","date":"2020-06-28","objectID":"/partition/:5:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파티션 나누기 aws에 ec2하나 띄워놓고 파티션을 나눠보장 먼저 파티션할 volume을 하나 생성하고 해당 ec2와 연결한다. 16GB짜리 disk를 연결했다. diskdisk \" disk $ fdisk -l fdisk -lfdisk -l \" fdisk -l 파티션 생성을 위해 파티션 설정으로 들어가자. $ fdisk /dev/xvdd partition 설정partition 설정 \" partition 설정 Primary 2GB Extended 4GB Logical 2GB Logical 2GB 사용안함 10GB 로 설정해보자. 먼저 2G 짜리 Primary 파티션을 만들기 위해 n 입력 partition 설정partition 설정 \" partition 설정 다음 primary 파티션을 생성해야하므로 p를 입력 순서대로 partition 번호는 1, sector 시작은 default, primary를 2GB로 설정하기위해서 last sector는 +2G 로 설정했다. partition 설정partition 설정 \" partition 설정 설정이 완료되었고 다시 p를 입력해 확인해보면 생성된 것을 확인 할 수 있다. partition 설정partition 설정 \" partition 설정 primary를 생성 했으므로 extended 파티션을 생성해보자. 똑같이 n 명령으로 생성하자. partition 설정partition 설정 \" partition 설정 xvdd2 이름으로 4GB Extended 파티션이 생성된 것을 확인 할 수 있다. 이제 Extended 파티션에 2개의 logical 파티션을 생성해보자. Extended 파티션을 생성하고 n 명령어를 입력하면 extended 대신 logical이 보일 것이다. partition 설정partition 설정 \" partition 설정 이제 2개의 logical 파티션을 생성하자 partition 설정partition 설정 \" partition 설정 2개째 설정하는 단계에서 Value out of range 에러가 발생했는데 Extended 파티션을 4GB로 설정했지만 완벽히 4GB가 아닐꺼라 남은 용량이 2GB보다 적어서 발생하는 에러 일 것이므로 default로 설정해주었더니 잘 할달 받은 것을 볼 수 있다. partition 설정partition 설정 \" partition 설정 w 명령으로 저장한다. partition 설정partition 설정 \" partition 설정 혹시 모르니 partprobe 명령으로 os에게 변경사항을 알려준다. $ partprobe 다시 확인하면 잘 나눠진것을 볼 수 있다. partition 설정partition 설정 \" partition 설정 ","date":"2020-06-28","objectID":"/partition/:5:1","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파일 시스템 설정 파티션 생성을 완료하면 물리적인 공간만 나눠놓았고 파일시스템을 지정해줘야한다. ext4 파일 시스템으로 /dev/xvdd1에 적용한다. $ mkfs.ext4 /dev/xvdd1 fs 설정fs 설정 \" fs 설정 xvdd5, xvdd6도 설정해주자. xvdd5는 ext4로 , xvdd6은 ext3으로 설정해봤다. fs 설정fs 설정 \" fs 설정 ","date":"2020-06-28","objectID":"/partition/:5:2","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"마운트 파일시스템 설정까지 해줬으니 마운트를 해줘서 파티션한 공간을 사용해보자. /dev/xvdd1, /dev/xvdd5, /dev/xvdd6 을 연결하기 위해 3개의 마운트 포인트가 필요하다. mount 설정mount 설정 \" mount 설정 mount 설정mount 설정 \" mount 설정 마운트가 잘 된것을 볼 수 있고 user를 할당해서 home 디렉토리로 설정해보장 user 설정user 설정 \" user 설정 $ tail /etc/passwd user 설정user 설정 \" user 설정 mount 확인mount 확인 \" mount 확인 ","date":"2020-06-28","objectID":"/partition/:5:3","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"sector란 물리 디스크에 입출력을 요청하는 최소 단위이다.","date":"2020-06-28","objectID":"/sector/","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"sector란 물리 디스크에 입출력을 요청하는 최소 단위이다. 전통적인 하드 디스크는 512 Byte 크기의 sector들로 구성 되어 있다고 한다. 큰 데이터에 접근 하고자 할때 sector의 크기가 작으면 많은 입출력 요청이 발생해서 디스크 에서는 퍼포먼스를 올리기 위해 sector 자체 크기를 늘리고자 했다. 파일의 크기가 작아 하나의 sector를 채우지 못하는 파일들은 0으로 나머지 sector 부분을 보유하고 있다. 또한 한 sector에 동시에 2 종류의 정보는 들어 갈 수 없다. 2011년 1월에 모든 하드 디스크 제조 업체가 sector의 크기를 4096 KByte를 표준으로 하는 것에 합의 했다는데 밑의 사진만 봐도 512 bytes로 나온다. sector sizesector size \" sector size 그 이유가 물리적으로는 4096 Byte인 것으로 입출력을 하지만 하드 디스크에 내장된 컨트롤러가 논리적으로 sector의 크기가 512Byte인 것과 같이 advanced format의 디스크가 나왔다고 한다. 실제 물리 디스크에 입출력을 요청하는 단위를 물리 sector 라 부르고 4096 Byte, 디스크 상단에 에뮬레이션 되는 단위를 논리 sector 라 하고 512 Byte이다. ","date":"2020-06-28","objectID":"/sector/:0:0","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"디스크 구조 ","date":"2020-06-28","objectID":"/sector/:1:0","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"LBA방식 현재의 하드 디스크는 LBA방식으로 접근하는 것이 보통이다. 이것은 하드 디스크의 전체 sector에 0부터 sector번호를 붙여서 이 번호를 이용해 접근하는 방식이다. sector번호와 물리적 sector위치를 짝 지우는 일은 하드 디스크 컨트롤러에 내장된 펌웨어가 담당한다. sectorsector \" sector partition 포스트에서 나누었던 partition을 그대로 가져왔다. 각 파티션을 보면 시작점과 종료점이 sector 번호로 표시된다. /dev/xvdd1 은 2048 sector 부터 시작해 0~1047 sector는 파티션으로 사용되지 않음을 볼 수 있다. 디스크의 처음에 MBR이 있고, 그 뒤에 GRUB의 스테이지 1.5가 기록된다고 한다. ","date":"2020-06-28","objectID":"/sector/:1:1","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"4KB sector disk 아까 위에서 기존의 하드 디스크는 1sector의 사이즈가 512Byte로 정해져 있다고 했다. 또, 물리적으로는 4KB씩 읽고쓰는데 논리적으로 512Byte로 에뮬레이션 하는 방식으로 되어 있다고도 했다. 예로 /dev/xvdd1이 2048 sector부터 시작한다면 256번째 4KB sector 시점과 일치한다. 파티션의 sector 수도 8의 배수이므로 파티션의 종료 위치도 딱 4KB sector의 마지막 지점이 된다. 장래에 4KB sector를 채용한 하드가 늘어날 것을 고려해 기본적으로 파티션의 시작 지점과 사이즈는 논리 sector 수로 생각한 8의 배수로 준비하는게 좋다. 이 작업을 파티션 정렬(alignment) 라고 한다. ","date":"2020-06-28","objectID":"/sector/:1:2","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"파티션 정렬 파티션 정렬의 설명을 좀 더 추가하자면 사진의 0~62 sector들이 있고 그 뒤에 63번 sector부터 파티션이 시작되는 것을 볼 수 있는데, 논리적으로 봤을때는 데이터가 잘 위치한 것으로 보이지만 물리적 섹터의 경우 경계에 맞물려 데이터가 담긴 블록이 위치하게 된다. naver cloudnaver cloud \" naver cloud sector크기가 4096 Byte일 때 512 Byte 크기의 sector크기로 에뮬레이션 하나 실제로 4096 Byte 크기의 sector단위로 입출력 되기 때문에 물리 sector의 8번, 9번 sector에 각각 접근해야한다. naver cloudnaver cloud \" naver cloud 다음과 같이 8번 물리 sector의 남은 512Byte 만큼을 indent 한다면 그림과 같다. 1개의 논리 sector만큼 indent하여 물리 섹터의 경계에 정렬한다. 따라서 9번 물리 sector에만 입출력을 요청하면 된다. 이렇게 물리 sector 경계에 맞춰주는 것이 파티션의 정렬이다. 즉 파티션 교체 할때 파티션 정렬을 안하면 디스크 I/O 지연이 발생하겠다. ","date":"2020-06-28","objectID":"/sector/:2:0","tags":["os","linux","setor"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"FS는 `저장장치 내에서 데이터를 읽고 쓰기 위해 미리 정해진 약속` 이라고 볼 수 있다.","date":"2020-06-21","objectID":"/filesystem/","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"FS는 저장장치 내에서 데이터를 읽고 쓰기 위해 미리 정해진 약속 이라고 볼 수 있다. 또한 컴퓨터에서 파일이나 자료를 쉽게 발견 및 접근할 수 있도록 보관 또는 조직하는 체제를 가리킨다. ","date":"2020-06-21","objectID":"/filesystem/:0:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"소개 파일 시스템은 일반적으로 크기가 일정한 블록들의 배열에 접근할 수 있는 자료 보관 장치 위에 생성되어 이러한 배열들을 조직함으로 파일이나 디렉터리를 만들며 어느 부분이 파일이고 어느 부분이 공백인지 구분하기 위하여 각 배열에 표시를 해둔다. 자료를 클러스터 또는 블록 이라고 불리는 일정한 단위에 새겨 넣는데 이것이 바로 파일 하나가 필요로 하는 디스크의 최소 공간이다. ","date":"2020-06-21","objectID":"/filesystem/:1:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"분류 일반적인 파일 시스템 일반적인 FS는 하드디스크와 같은 저장장치에서 주로 사용된다. 그 종류를 간단히 보면 다음과 같다. FAT (File Allocation Table) FAT은 어느 영역에 파일이 속해 있는지, 공간에 여유가 있는지, 또 어디에 각 파일이 디스크에 저장되어 있는지에 대한 정보를 중심으로 하는 테이블을 이용하는 것에서 비롯한다. 상대적으로 간단한 파일 시스템. 성능은 상대적으로 다른 파일 시스템보다 좋지 않다. 너무나도 단순한 자료구조를 사용하고 조그만 파일이 많으면 공간 활용률이 적어지기 때문이다. FAT12, FAT16, FAT32, exFAT등 종류가 더 있다. HPFS (High Performance FileSystem) NTFS (New Technology FileSystem) FAT32의 약점을 보완하기 위해 개발된 FS 드라이브 최대용량 256TB 파일 하나당 저장할 수 있는 최대 크기 16TB 윈도우에서는 최적화 되어 있지만, MAC, android, linux와 같은 기기는 제한되어있음. UFS (Unix FileSystem) ext ext, ext2, ext3, ext4의 종류들이 있다. 리눅스용 파일 시스템 가운데 하나 로 오늘날 많은 리눅스 배포판에서 주 파일 시스템으로 쓰이고 있다. APFS 애플 파일 시스템은 애플에서 macOS, iOS, watchOS, tvOS등에서 범용으로 사용하고자 만든 FS Flash FileSystem Network FileSystem 네트워크 파일시스템은 원격에 위치한 파일시스템을 로컬 파일시스템처럼 이용할 수 있도록 개발한 프로토콜이다. 단순히 파일 공유가 아니라 NFS도 파일 시스템임을 인지 해야 하기 때문에 원격 파일시스템이 mount되면 mount 지점 아래 위치한 파일에 접근을 하는 경우 NFS가 파일시스템 레벨에서 system call을 받아 직접 네트워크 파일을 수신해 쓰거나 실행할 수 있도록 한다. Virtual FileSystem OS차원에서 가상 파일시스템이라는 상위 레벨의 파일시스템 인터페이스가 존재하기 때문에 응용프로그램에서는 아무 구분 없이 OS의 system call을 호출하면 커널은 미리 등록 되어 있는 파일시스템 함수를 호출해 같은 경과를 얻을 수 있다. ","date":"2020-06-21","objectID":"/filesystem/:2:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"요소 ","date":"2020-06-21","objectID":"/filesystem/:3:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"클러스터 클러스터는 파일을 저장하는 단위로 1개 또는 복수의 섹터로 이루어진다. 여러 개의 클러스터를 사용할 때 반드시 연결되어 있지 않다. 여기저기 흩어져 있어도, 그 위치와 순서를 기록한 FAT등에 의하여 관리되므로 한 번에 파일의 전체 내용을 읽을 수 있다. OS가 파일시스템 생성 시 저장장치의 크기를 고려해 클러스터의 크기를 조절한다. 저장장치의 크기 및 사용 용도에 따라 달라져야한다. OS에 의해 데이터를 읽고 쓰는 과정에서 파일시스템은 미리 정해져 있는 클러스터의 크기를 기본단위로 하여 읽고 쓰는 과정에서 파일시스템은 미리 정해져있는 클러스터의 크기를 기본단위로 하여 입출력을 하게 된다. 클러스터의 크기가 4096Byte라면 1Byte를 읽더라도 4096Byte를 읽어야한다. processprocess \" process 크기가 작은 파일을 저장할 경우 낭비되는 영역이 생기는데 이 부분의 공간은 사용이 불가능 해진다. 낭비되어도 성능적인 측면에서 I/O의 비용이 커서 요즘의 대용량 하드라면 성능을 위해 무시할 정도다 ","date":"2020-06-21","objectID":"/filesystem/:3:1","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"파일 파일시스템은 결국 파일을 기록하기 위한 것이므로 파일을 이루는 구조와 관리할 수 있는 추가적인 방법을 제시한다. 파일은 속성을 기록하는 메타 데이터, 실제 데이터를 기록하는 데이터 영역으로 나뉜다. 파일 정보 요청 프로세스 파일정보요청 → Meta Data 로 파일 경로 요청을 보낸다. Meta Data → 요청 파일 경로 안내를 해준다. Meta Data로 부터 받은 경로로 실제 파일로 접근한다. 참고 https://blog.naver.com/bitnang/70183421214 ","date":"2020-06-21","objectID":"/filesystem/:3:2","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os"],"content":"Application memory 구조의 Heap과 Stack을 살펴보자","date":"2019-11-20","objectID":"/memory/","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"Application memory 구조의 Heap과 Stack을 살펴보자. memory 이전글에서 Process/Thread를 얘기할때 Memory에 대한 언급을 잠깐 했다. 여기서 사용하는 Memory는 Virtual Memory라고.. 모든 프로세스가 실제의 메모리를 사용하게 되면 용량 문제가 발생하기 때문에 페이징 기법과 가상메모리를 사용한다. 흔히 코드에서 출력하는 주소값들은 가상주소이다. ","date":"2019-11-20","objectID":"/memory/:0:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"1 페이징기법 계산기의 GUI가 있다고하자. +, -, *, % 등의 연산을 할때 이는 페이징 파일에 저장된다. 페이징 파일에 아직 동작하지 않은 주소를 저장 시켜서 잠시 가지고 있는것.. 이와같이 페이징 파일을 가지고 있기에 모든 프로세스가 더 적게 메모리를 사용하는 것이다. ","date":"2019-11-20","objectID":"/memory/:1:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2 가상메모리 프로그램을 동작하면 가상 메모리 공간이 생성된다. 그 메모리 공간은 상위, 하위 메모리로 나눠진다. ","date":"2019-11-20","objectID":"/memory/:2:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2.2 상위 메모리 Stack의 메모리 공간이 할당된다. 지역변수, 리턴값등을 저장.. 값이 싸다. 이유는 할당과 해제는 CPU 명령어 2개로 끝난다고 한다. (할당, 해제) 코드로 예를 들면 함수가 종료되면 함수에 있던 모든 변수가 Stack에서 Pop된다고 생각하면 된다. 함수가 사라지면 외부에서 참조 못하는 것처럼 여기서 학부시절에 그렇게 많이 들은 Stack Overflow, Underflow등이 일어나게 되는 것이다. ","date":"2019-11-20","objectID":"/memory/:2:1","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2.3 하위 메모리 Heap이 생성된다. 프로그램이 동작할 때 필요한 데이터 정보를 임시로 저장. heap할당은 비싸다. C에서 보면 malloc(), free()등으로 조작가능. Go, Python같은 언어들은 자동으로 GC가 작동하기 때문에 개발자가 신경 안써도 되는 경우도 있다. 하지만 코드를 작성할때 신경써야하는 부분이 있을 수도 있는데 Go를 예로 들어보면 2.3.1 Go에서 heap Go에서 포인터를 사용하면 대부분 heap에 할당 된다고 한다. 가능한 안쓰는게 좋겠지..? 포인터를 사용하지 않고 값을 복사하는 것이 memory를 작게써 CPU 캐시 적중률이 오른다고한다. 또한 포인터를 포함하지 않는 메모리 영역은 GC가 생략할 수 있다. 반대로 포인터가 있으면 GC가 스캔 할 필요가 있다. 이렇게 GC가 돌면 메모리 상에 흩어진 영역을 계속 탐색하기 때문에 무거워 지기 때문에 개발자는 조금이라도 가볍게 개발하기 위해서는 알아야 하는 개념이다. ","date":"2019-11-20","objectID":"/memory/:2:2","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"갑자기 컴퓨터 구조에 대해서 정리해보고 싶어서 머리속에 있는것을 더해 끄적여본다.","date":"2019-11-06","objectID":"/computerstructure/","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"갑자기 컴퓨터 구조에 대해서 정리해보고 싶어서 머리속에 있는것을 더해 끄적여본다. 대학교에서 전공과목으로 컴퓨터 구조를 배웠지만 생각이 나지 않고 뭘배웠는지 기억을 못한다… 그나마 남아있는것을 정리하려고한다. ㅜㅜ.. 컴퓨터는 가장 크게 H/W, S/W로 나눌 수 있고 이를 좀더 자세히 나눠서 보자 ","date":"2019-11-06","objectID":"/computerstructure/:0:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"H/W 말그대로 하드웨어 물리적인 컴퓨터 자원을 말한다. CPU, GPU, RAM등등 본체에 꼽혀있는것들.. ","date":"2019-11-06","objectID":"/computerstructure/:1:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"S/W 소프트웨어는 물리적 반대로 논리적(Logical)하다 라고 많이 불리고 이는 IT쪽에서 Virtual과 비슷한 의미를 가진다. 더 작게 나눠보자 ","date":"2019-11-06","objectID":"/computerstructure/:2:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"Kernel 커널은 운영체제의 핵심 부분으로서, 운영체제의 다른 부분 및 응용 프로그램 수행에 필요한 여러가지 서비스를 제공한다. 라고 위키백과에 나와있다. 간단히 말하면 이 커널 영역에서 흔히 하드웨어 장치에 필요한 Driver가 있어야 하고, 그에 따른 구성요소가 포함된다고 보면 될 것 같다. 예를들어 Disk라는 물리적인 저장공간을 가지고있고 이에 따른 Driver가 있고 이 Driver는 File System(NTFS) 구성요소와 연결 되고, File System은 우리가 흔히 쓰는 File과 연결되어 처리가 된다. 우리가 컴퓨터로 파일을 옮기고 삭제하는등의 행동을 취했을때 내부적으로는 File -\u003e File System -\u003e Driver -\u003e Disk 또는 SSD등 이렇게 이뤄지는 것이당 File system부터 Disk까지의 행동을 커널 영역에서 다룬다. 음.. 다른 예를 하나더 들어보자. 제목에 socket을 넣어서 잠깐 다뤄보면 우리가 흔히 쓰는 Socket도 결국 위에서 말한 File을 이용한다. 이게 무슨? 소리징하고 처음엔 생각했다. 그 이유는 유닉스의 리소스 구성은 파일로 이뤄져있다. 파일, 하드웨어, 파이프, 소켓등등 밑의 사진을 보면 하드웨어들이 file로 저장되어 있는것을 볼 수 있다. HwfileHwfile \" Hwfile 다시 Socket으로 돌아와서 커널 영역에서 뭘하는지 보면, Socket(file) -\u003e Protocol(TCP/IP) -\u003e Driver -\u003e NIC 이렇게 될듯 하다. NIC는 하드웨어로 랜카드이다. TMI로 NIC -\u003e L2 -\u003e R -\u003e internet이 되겠지요? Kernel 영역에서 하는 일은 위의 예시들로 알 수 있을 것이다. 백신 프로그램을 잠깐 보면 어떻게 동작하는지 짐작 할 수 있을 것 같은데 아까 위에서 File -\u003e 구성요소 -\u003e Driver이렇게 흘러간다고 얘기를 많이 했다. 그 중간에 Filter라는 것이 존재하는데 이 Filter는 구성요서 앞/뒤 쯤 올 수 있다. 이 Filter를 백신 업체에서 제공 해주면 Filtering이 되겠거니 싶다. 물론 Filter를 이용하다 보니 속도는 느려 질 수 있을 것이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:1","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"User User영역은 아까 많이 언급한 File이 속한다. 또한 백신 얘기할때 anti virus가 속할 수 있고 이 anti virus는 아까 Filter와 주거니 받거니 할 것이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:2","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"Process/Thread 가장 많이 접한 Process와 Thread이다. 이 둘을 간단히 비교해보면 Process안에 Thread가 있고 Thread는 프로세스 내에서 실행되는 여러 흐름의 단위라고 구글에 검색하면 많이 나올 것이다. 엄청 간단히 예시를 들면 집(house)가 Process 그 안의 구성원 엄마, 아빠 등이 Thread라고 생각하면 될 것 같다. Process끼리는 독립된 Memory를 가져서 서로 데이터 공유가 안된다. 물론 Thread는 Process 내에서 동작하기 때문에 Process내의 Thread끼리는 같은 메모리 공간을 이용한다. 따라서 동기화가 중요 하다 ! 실생활의 예를 들어보면 집(process)에 내가(thread) 화장실을 사용하고 있을때 문을 잠그고 이용하게 되면 다른 가족구성원(thread)는 화장실을 사용하지 못한다. 만약, 문을 안잠그고 사용하다가 큰일을 보는 중에 문을 열어버린다던지 그러면 안되는 것 처럼 이해 하면 될 것 같다. 여기서 화장실 문을 잠그는 행위는 Lock을 거는 행위이고 이는 mutex를 이용해 Lock, UnLock을 하는 것과 같다고 보면 될 듯 하다. 여기서 말한 Memory는 아 ~~ 까 말한 S/W는 Virtual == Logical 을 잠깐 인용하자면 Process에 할당된 Memory는 virtual memory이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:3","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os","linux"],"content":"linux에서 container가 내부적으로 어떻게 동작하는지 알아보자.","date":"2019-10-27","objectID":"/namespace/","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"linux에서 container가 내부적으로 어떻게 동작하는지 알아보자. namespace는 k8s에서 먼저 많이 접해봤는데, 각각 별개의 독립된 공간인 것처럼 환경을 제공하는 가상화 기술이다. container기술이 이를 통해 만들어 졌다. Linux namespace는 6가지 종류가 있다. UTS IPC PID NS (FS) NET USER ","date":"2019-10-27","objectID":"/namespace/:0:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"UTS 독립적인 Hostname할당 현재 hostname은 다음과 같이 알 수 있다. uname -n unshare로 namespace를 만드는데 -u를 주면 UTS가 부모 프로세스와 공유가 안된 상태로 생성된다. unshare -u /bin/bash ","date":"2019-10-27","objectID":"/namespace/:1:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"IPC inter process communication IPC는 프로세스간 서로 데이터를 주고 받는 경로를 뜻한다. IPC는 Signal, Socket, pipe, semaphore, file locking, mutex등이 있다. 또한 프로세스간 데이터 교환 및 프로세스와 쓰레드간의 작업을 동기화 하는기능을 제공한다. ","date":"2019-10-27","objectID":"/namespace/:2:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"PID Linux에서 실행되는 모든 process들은 각각에 고유한 PID가 부여된다. kernel에서는 이 process들을 Tree 형태로 관리한다. pstreepstree \" pstree 최상위 process는 init process라 하고 PID를 “1\"을 가진다. PID namespace에서 가지는 특징이 이 PID 1프로세스를 독립적으로 추가 할당해 주는 것이다. PID를 1받았다고 해서 init process가 되는건 아니고 init process와 비슷하게 수행된다. 이는 OS에서 systemd 프로세스 뿐만 아니라 PID 충돌없이 실행 가능하게 해준다. ","date":"2019-10-27","objectID":"/namespace/:3:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"NS (FS) process Tree의 PID 1프로세스를 분기 시켜도 FS는 그대로 공유하고 있을것이다. 이유는 /proc를 아직 공유하고 있기 때문이다. 새로운 폴더를 만들고 /proc폴더를 mount시켜주면 독립적으로 FS를 관리할 수 있을 것이다. ","date":"2019-10-27","objectID":"/namespace/:4:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"NET namespace간 Network 충돌 방지한다. $ ip link list 명령어로 현재 서버의 interface 정보를 확인 할 수 있다. $ ip netns add {namespace} 새로운 namespace를 생성하고 다음과 같이 확인 할 수있다. $ ip netns 독립된 namespace간 내부 통신도 가능한데 자세한건 다음 글에서 작성한다. ","date":"2019-10-27","objectID":"/namespace/:5:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"USER 프로세스가 namespace 내부와 default namespace간에 각기 다른 사용자 및 그룹 ID를 가질 수 있게한다. ","date":"2019-10-27","objectID":"/namespace/:6:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["kubernetes"],"content":"K8s를 사용하면서 사용자별로 Role을 정해줄 수 있다.","date":"2019-06-27","objectID":"/serviceaccount/","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["kubernetes"],"content":"K8s를 사용하면서 사용자별로 Role을 정해줄 수 있다. ServiceAccount 생성 Role 생성 RoleBinding 생성 ServiceAccount의 token을 얻어서 kubectl에 사용해보자. ","date":"2019-06-27","objectID":"/serviceaccount/:0:0","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["kubernetes"],"content":"ServiceAccount, Role, RoleBinding apiVersion:v1kind:ServiceAccountmetadata:namespace:testname:jaejin---apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:testname:pod-readerrules:- apiGroups:[\"\"]resources:[\"pods\"]verbs:[\"get\",\"watch\",\"list\"]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:read-podsnamespace:testsubjects:- kind:ServiceAccountname:jaejinnamespace:testroleRef:kind:Rolename:pod-readerapiGroup:rbac.authorization.k8s.io 위의 코드를 보고 설명하면, 먼저 계정? 으로 쓰일 ServiceAccount를 생성한다. test namespace에 jaejin이란 이름으로 생성된다. Role은 pods의 정보를 가져온다는 것을 의미한다. RoleBinding은 ServiceAccount와 Role을 연결한다. Note 물론 test란 namespace는 미리 생성했다. $ kubectl get sa -n test NAME SECRETS AGE default 1 16s jaejin 1 13s jaejin 이란 ServiceAccount가 생성되었다. Role과 RoleBinding도 확인하자. $ kubectl get role -n test NAME AGE pod-reader 51s $ kubectl get rolebinding -n test NAME AGE read-pods 77s 이제 이것들을 가지고 token을 뽑아 내자. $ export serviceaccount=jaejin $ export namespace=test $ export server=$(kubectl config view | grep server | cut -f 2- -d \":\" | tr -d \" \") $ export name=$(kubectl get secret -n $namespace | grep $serviceaccount-token | cut -f -1 -d \" \") $ export ca=$(kubectl get secret/$name -n $namespace -o jsonpath='{.data.ca\\.crt}') $ export token=$(kubectl get secret/$name -n $namespace -o jsonpath='{.data.token}' | base64 -D) 위의 명령을 실행하면 token, ca 등의 정보를 가져올 수 있다. kubectl의 config 파일을 만들어보자. echo \" apiVersion: v1 kind: Config clusters: - name: default-cluster cluster: certificate-authority-data: ${ca}server: ${server}contexts: - name: default-context context: cluster: default-cluster namespace: ${namespace}user: default-user current-context: default-context users: - name: default-user user: token: ${token}\" \u003e config config 파일이 만들어지고 이를 ~/.kube/config의 경로로 저장한다. 이제 kubectl 명령을 날려보자. test namespace의 pods의 정보만 가져 올 수 있을 것이다. serviceaccountserviceaccount \" serviceaccount Role을 이용해 사용자별로 권한을 다르게 줄 수 있다 !! ","date":"2019-06-27","objectID":"/serviceaccount/:1:0","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["kubernetes"],"content":"GCP, AWS와 같은 Cloud 환경이 아닌 물리 서버에 kubernetes cluster 구성해보자.","date":"2019-03-10","objectID":"/kubeadm/","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"GCP, AWS와 같은 Cloud 환경이 아닌 물리 서버에 kubernetes cluster 구성해보자. Creating a single master cluster with kubeadm ","date":"2019-03-10","objectID":"/kubeadm/:0:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Maser node setting $ sudo kubeadm --kubernetes-version=1.13.2 init init을 실행하면 worker node 에서 master node로 접속할 수 있는 token hash 값을 내준다. 이것을 이용해 master node에 접속한다. ex) kubeadm join \u003cmaster-ip\u003e:\u003cmaster-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e Notice! 만약 [ERROR Swap]: running with swap on is not supported. Please disable swap 에러가 발생한다면 swapoff를 해주면된다. $ sudo swapoff -a To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2019-03-10","objectID":"/kubeadm/:1:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Installing a pod network add-on 문서에 보면 여러가지 network를 설정할 수 있는 것들이 존재하는데 Canal을 사용하였다. $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml 잘 작동하고 있는지 pod을 확인하자. $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE canal-vb5fw 0/3 ContainerCreating 0 27s coredns-86c58d9df4-mf68s 0/1 Pending 0 2s coredns-86c58d9df4-zbxnp 0/1 Pending 0 2s etcd-eta 1/1 Running 0 26s kube-apiserver-eta 1/1 Running 0 45s kube-controller-manager-eta 1/1 Running 0 33s kube-proxy-tww4r 1/1 Running 0 82s kube-scheduler-eta 1/1 Running 0 24s Notice! 만약 coredns가 fail이 나거나 error가 발생하면 다음 설정으로 해결할 수 있다. $ kubectl -n kube-system edit configmap coredns loop 삭제 $ kubectl -n kube-system delete pod -l k8s-app=kube-dns ","date":"2019-03-10","objectID":"/kubeadm/:2:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Joining your nodes 아까 master node에서 init할 때 나온 token hash값을 이용해서 node들을 join 시켜보자. worker node에서는 init 할 필요없이 바로 join 명령어만 실행 하면된다. $ kubeadm join \u003cmaster-ip\u003e:\u003cmaster-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e master에서 확인해보면, $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE canal-6gg89 3/3 Running 0 24m canal-7dhgw 3/3 Running 0 24m canal-vb5fw 3/3 Running 0 24m coredns-86c58d9df4-mf68s 1/1 Running 0 24m coredns-86c58d9df4-zbxnp 1/1 Running 0 24m etcd-eta 1/1 Running 0 24m kube-apiserver-eta 1/1 Running 0 25m kube-controller-manager-eta 1/1 Running 0 24m kube-proxy-57qh5 1/1 Running 0 24m kube-proxy-tww4r 1/1 Running 0 25m kube-proxy-x6rng 1/1 Running 0 24m kube-scheduler-eta 1/1 Running 0 24m node 개수 만큼 canal, proxy등이 추가 된것을 확인 할 수 있다. $ kubectl get nodes NAME STATUS ROLES AGE VERSION chi Ready \u003cnone\u003e 14m v1.13.2 delta Ready \u003cnone\u003e 14m v1.13.2 eta Ready master 16m v1.13.2 ","date":"2019-03-10","objectID":"/kubeadm/:3:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"pod test하기 조대협님 블로그 node.js 코드 참조 Deployment를 올리고 apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:jaejin-node-testspec:replicas:3template:metadata:labels:app:myappspec:containers:- name:stupid-serverimage:opiximeo/jaejintest:v1imagePullPolicy:Alwaysports:- containerPort:8080 외부에서 접속하기 위해 service를 올린다. apiVersion:v1kind:Servicemetadata:name:hello-node-svcspec:selector:app:myappports:- port:5920protocol:TCPtargetPort:8080externalIPs:- {your server ip} cloud에서 service에 type: LoadBalancer를 설정해주면 자동으로 외부 IP를 가져오지만 cloud에서 올린게 아니라서 자동으로 가져오지는 못한다. 따라서 externalIPs 옵션을 줘서 접속할 수 있는 IP를 명시해주면 된다. http://{your server ip}:5920 ","date":"2019-03-10","objectID":"/kubeadm/:4:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"argo를 알아보자.","date":"2019-01-13","objectID":"/argo/","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"argo를 알아보자. argo는 컨테이너 워크플로우 솔루션이다. 워크플로우 스펙을 YAML 파일로 정의하고, 실행할때 마다 컨테이너를 생성해서 작업을 수행한다. Open source container-native workflow engine for Kubernetes. argo argo 공식홈페이지에 나와있는 example을 실행해보자. 먼저 yaml 파일을 다음과 같이 생성한다. apiVersion:argoproj.io/v1alpha1kind:Workflow #new type of k8s specmetadata:generateName:hello-world- #name of workflow specspec:entrypoint:whalesay #invoke the whalesay templatetemplates:- name:whalesay #name of templatecontainer:image:docker/whalesaycommand:[cowsay]args:[\"hello world\"]resources:#don't use too much resourceslimits:memory:32Micpu:100m 생성후 kubernetes에 submit 한다. $ argo submit hello-world.yaml 현재 워크플로우 list를 보려면 list 명령어를 이용한다. $ argo list NAME STATUS AGE DURATION hello-world-s6d7p Running 10m 10m 생성된 pod 의 상태를보려면 get 명령어를 이용 $ argo get hello-world-s6d7p Name: hello-world-s6d7p Namespace: default ServiceAccount: default Status: Running Created: Sun Jan 13 14:56:43 +0900 (11 minutes ago) Started: Sun Jan 13 14:56:52 +0900 (10 minutes ago) Duration: 10 minutes 52 seconds STEP PODNAME DURATION MESSAGE ● hello-world-s6d7p hello-world-s6d7p 10m 마찬가지로 log를 보려면 log 명령어를 이용 $ argo logs -w hello-world-xxx # get logs from all steps in a workflow argo logs -w hello-world-s6d7p hello-world-s6d7p: _____________ hello-world-s6d7p: \u003c hello world \u003e hello-world-s6d7p: ------------- hello-world-s6d7p: \\ hello-world-s6d7p: \\ hello-world-s6d7p: \\ hello-world-s6d7p: ## . hello-world-s6d7p: ## ## ## == hello-world-s6d7p: ## ## ## ## === hello-world-s6d7p: /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === hello-world-s6d7p: {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- hello-world-s6d7p: \\______ o __/ hello-world-s6d7p: \\ \\ __/ hello-world-s6d7p: \\____\\______/ $ argo logs hello-world-xxx-yyy # get logs from a specific step in a workflow argo logs hello-world-s6d7p _____________ \u003c hello world \u003e ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- \\______ o __/ \\ \\ __/ \\____\\______/ delete $ argo delete hello-world-xxx ","date":"2019-01-13","objectID":"/argo/:0:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"ArgoUI CLI가 아닌 GUI로 확인하기 위해 argo-ui를 로컬 PC 로 포워딩 한다. $ kubectl -n argo port-forward deployment/argo-ui 8001:8001 argoargo \" argo ","date":"2019-01-13","objectID":"/argo/:1:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"연속작업 다음과 같이 워크플로우를 작성한다. apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:steps-spec:entrypoint:hello-hello-hello# This spec contains two templates: hello-hello-hello and whalesaytemplates:- name:hello-hello-hello# Instead of just running a container# This template has a sequence of stepssteps:- - name:hello1 #hello1 is run before the following stepstemplate:whalesayarguments:parameters:- name:messagevalue:\"hello1\"- - name:hello2a #double dash =\u003e run after previous steptemplate:whalesayarguments:parameters:- name:messagevalue:\"hello2a\"- name:hello2b #single dash =\u003e run in parallel with previous steptemplate:whalesayarguments:parameters:- name:messagevalue:\"hello2b\"# This is the same template as from the previous example- name:whalesayinputs:parameters:- name:messagecontainer:image:docker/whalesaycommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] Metadata에서 generateName: steps- 는 워크플로우 이름이다. 워크플로우는 templates부분에 정의 되어있는데 hello1과 hello2a를 보면 앞에 - - 2개의 -가 정의되어있고 hello2b 같은경우는 1개의 -로 되어있는 것을 볼 수 있다. 이는 hello2a, hello2b를 동시에 실행하는 것을 의미한다. 다시 실행해보자. $ argo submit --watch helloworld-multi.yaml Name: steps-nt72p Namespace: argo ServiceAccount: default Status: Succeeded Created: Sun Jan 13 15:39:55 +0900 (5 seconds ago) Started: Sun Jan 13 15:39:55 +0900 (5 seconds ago) Finished: Sun Jan 13 15:40:00 +0900 (now) Duration: 5 seconds STEP PODNAME DURATION MESSAGE ✔ steps-nt72p ├---✔ hello1 steps-nt72p-1112559031 1s └-·-✔ hello2a steps-nt72p-220736832 1s └-✔ hello2b steps-nt72p-271069689 2s --watch 옵션은 각 단계별 실행 상태와 워크플로우 구조를 볼 수 있다. argoargo \" argo step-~ 이라는 워크플로우가 생성되었다. Warning Notice: istio가 설치되어있는환경에서는 계속 running에서 대기중이던데 확인이 필요할 거 같다. istio가 적용안되어있는 namespace를 이용했더니 잘 작동했다. argoargo \" argo ","date":"2019-01-13","objectID":"/argo/:2:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"DAG를 이용한 워크플로우 정의 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:dag-diamond-spec:entrypoint:diamondtemplates:- name:echoinputs:parameters:- name:messagecontainer:image:alpine:3.7command:[echo, \"{{inputs.parameters.message}}\"]- name:diamonddag:tasks:- name:Atemplate:echoarguments:parameters:[{name: message, value:A}]- name:Bdependencies:[A]template:echoarguments:parameters:[{name: message, value:B}]- name:Cdependencies:[A]template:echoarguments:parameters:[{name: message, value:C}]- name:Ddependencies:[B, C]template:echoarguments:parameters:[{name: message, value:D}] DAG를 이용하면 좀 더 명시적으로 워크플로우 정의가 가능하다. A인 task가 실행하게 하고 B,C는 A에 의존성을 가지도록 하고 D는 B,C의 의존성을 가지게 해서 실행 순서는 A → B , C → D 형태가 된다. argoargo \" argo 이후 진행은 조대협님 블로그를 참고하였습니다. 컨테이너 기반 워크플로우 솔루션 Argo ","date":"2019-01-13","objectID":"/argo/:3:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"입력/출력값 전달 argo의 개념과 워크플로우의 개념을 이해했으면 task간 데이터를 어떻게 전달하는지 살펴보자. 입력값 전달 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:hello-world-parameters-spec:# invoke the whalesay template with# \"hello world\" as the argument# to the message parameterentrypoint:whalesayarguments:parameters:- name:messagevalue:hello worldtemplates:- name:whalesayinputs:parameters:- name:message #parameter declarationcontainer:# run cowsay with that message input parameter as argsimage:docker/whalesaycommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] 먼저 spec.arguments 부분에서 message라는 변수를 선언 하였고, 그값은 hello world로 되어 있다. 그리고 워크플로우의 whalesay 태스크에서 message 변수를 input 변수로 사용하도록 선언했다. input.parameters.message를 참조하여 message 변수의 값을 도커 컨테이터의 실행 변수로 넘기도록 하였다. CLI에서 값을 변경하려면 $ argo submit file.yaml -p {parameter name}={value} ","date":"2019-01-13","objectID":"/argo/:4:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"출력값 사용 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:output-parameter-spec:entrypoint:output-parametertemplates:- name:output-parametersteps:- - name:generate-parametertemplate:whalesay- - name:consume-parametertemplate:print-messagearguments:parameters:# Pass the hello-param output from the generate-parameter step as the message input to print-message- name:messagevalue:\"{{steps.generate-parameter.outputs.parameters.hello-param}}\"- name:whalesaycontainer:image:docker/whalesay:latestcommand:[sh, -c]args:[\"echo -n hello world \u003e /tmp/hello_world.txt\"]#generate the content of hello_world.txtoutputs:parameters:- name:hello-param #name of output parametervalueFrom:path:/tmp/hello_world.txt #set the value of hello-param to the contents of this hello-world.txt- name:print-messageinputs:parameters:- name:messagecontainer:image:docker/whalesay:latestcommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] 먼저 whalesay를 보면 outputs.parameters에 hello-param이라는 이름으로 output 변수를 정의하였고, output 내용은 /tmp/hello_world.txt 파일 내용으로 채워진다. 다음 print-message 컨테이너 정의 부분을 보면 input param으로 message라는 변수를 정의하였다. steps를 보면 print-message를 실행할때 message 변수의 값을 {{steps.generate-parameter.outputs.parameters.hello-param}} 로 정의해 print-message의 이전 스탭인 generate-parameter의 output param 중에 hello-param이라는 변수의 값으로 채우는 것을 볼 수 있다. ","date":"2019-01-13","objectID":"/argo/:5:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"Artifact 워크 플로우 태스크에 대한 입/출력값을 parameter로 전달할 수 도 있지만, CI/CD 빌드 파이프라인에서는 소스코드, 빌드 바이너리가 될수 도 있고 빅데이터 파일일 수도 있다. 이 경우 parameter를 이용해서 넘기기는 부담이 된다. 이런경우 artifact라는 기능을 이용한다. 외부 저장소 AWS S3나 GCP GCS같은 곳에 저장할 수 있게하고 반대로 읽기도 가능하다. 나중에 kubeflow에서 텐서플로우 학습을 시키는 파이프라인이 있을때 학습된 모델을 S3나 GCS에 저장하도록 하는 작업을 할 수 있다. parameter → artifact로만 변경하면 된다. apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:artifact-passing-spec:entrypoint:artifact-exampletemplates:- name:artifact-examplesteps:- - name:generate-artifacttemplate:whalesay- - name:consume-artifacttemplate:print-messagearguments:artifacts:# bind message to the hello-art artifact# generated by the generate-artifact step- name:messagefrom:\"{{steps.generate-artifact.outputs.artifacts.hello-art}}\"- name:whalesaycontainer:image:docker/whalesay:latestcommand:[sh, -c]args:[\"cowsay hello world | tee /tmp/hello_world.txt\"]outputs:artifacts:# generate hello-art artifact from /tmp/hello_world.txt# artifacts can be directories as well as files- name:hello-artpath:/tmp/hello_world.txt- name:print-messageinputs:artifacts:# unpack the message input artifact# and put it at /tmp/message- name:messagepath:/tmp/messagecontainer:image:alpine:latestcommand:[sh, -c]args:[\"cat /tmp/message\"] ","date":"2019-01-13","objectID":"/argo/:6:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"Istio란 sidecar pattern을 이용한 service mesh architecture 의 구현체인 오픈 플랫폼이다.","date":"2018-12-30","objectID":"/istio/","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio란 sidecar pattern을 이용한 service mesh architecture 의 구현체인 오픈 플랫폼이다. Istio #4 - Istio 설치와 BookInfo 예제 조대협님 블로그를 참고하였습니다. Istio 설치 쿠버네티스 클러스터가 준비되어 있다면, Istio를 설치한다. ","date":"2018-12-30","objectID":"/istio/:0:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Helm 설치 helm은 kubernetes의 nodejs의 npm 정도라고 생각하면 된다. $ cd ~ $ curl -L https://git.io/getLatestIstio | sh - $ cd istio-{version} $ export PATH=$PWD/bin:$PATH ","date":"2018-12-30","objectID":"/istio/:1:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Helm초기화 helm용 서비스 어카운트를 생성하고 helm을 초기화 한다. $ kubectl create -f install/kubernetes/helm/helm-service-account.yaml $ helm init --service-account tiller ","date":"2018-12-30","objectID":"/istio/:2:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio 설치 설치시 모니터링을 위해 모니터링 도구인 kiali, servicegraph, grafana 설치 옵션을 설정한다. helm install install/kubernetes/helm/istio \\ --name istio \\ --namespace istio-system \\ --set tracing.enabled=true \\ --set global.mtls.enabled=true \\ --set grafana.enabled=true \\ --set kiali.enabled=true \\ --set servicegraph.enabled=true 약간의 시간이 지나고 설치되어있는지 pod 목록을 출력하면 다음과 같이 나온다. pod목록pod목록 \" pod목록 ","date":"2018-12-30","objectID":"/istio/:3:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio 삭제 uninstall using kubectl $ kubectl delete -f $HOME/istio.yaml uninstall using Helm $ helm delete --purge istio If your Helm version is less than 2.9.0, then you need to manually cleanup extra job resource before redeploy new version of Istio chart: $ kubectl -n istio-system delete job --all delete the CRDs $ kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system BookInfo 서비스 예제 설치 ","date":"2018-12-30","objectID":"/istio/:4:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio의 sidecar injection 활성화 Istio는 Pod에 envoy를 sidecar 패턴으로 삽입하여, 트래픽을 컨트롤하는 구조이다. Istio는 이 sidecar를 Pod 생성시 자동으로 주입 (inject)하는 기능이 있는데, 이 기능을 활성화 하기 위해서는 kubernetes의 namespace에 istio-injection=enabled 라는 라벨을 추가해야한다. $ kubectl label namespace default istio-injection=enabled 추가 했다면, 다음과 같이 확인할 수 있다. labellabel \" label ","date":"2018-12-30","objectID":"/istio/:5:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Bookinfo 어플리케이션 배포 Bookinfo yaml 파일은 다음경로를 참고하면된다. istio/istio bookinfo.yaml 파일을 생성하고 다음 명령어로 실행하면 된다. $ kubectl apply -f bookinfo.yaml 배포 완료 후 kubectl get pod 명령어를 실행해보면 다음과 같이 productpage, detail, rating 서비스가 배포되고, reviews 서비스는 v1 ~ v3 까지 배포되는걸 확이 할 수 있다. podpod \" pod kubectl get svc를 이용해 배포되어 있는 서비스를 확인하자. svcsvc \" svc 모두 clusterIP 타입으로 배포 되어 있기 때문에 외부에서는 접근 불가하다. ","date":"2018-12-30","objectID":"/istio/:6:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio gateway 설정 이 서비스를 외부로 노출 시키는데, kubernetes의 Ingress나 Service는 사용하지 않고, Istio의 gateway 를 이용한다. Istio의 gateway는 kubernetes의 커스텀 리소스 타입으로, Istio로 들어오는 트래픽을 받아주는 엔드포인트 역할을 한다. 여러 방법이 있지만 Istio에서는 디폴트로 배포되는 Gateway는 pod 형식으로 배포되어 Load Balancer 타입의 서비스로 서비스 된다. 먼저 Istio Gateway를 등록한 후에, Gateway를 통해 서비스할 호스트를 Virtaul Service로 등록한다. Virtaul ServiceVirtaul Service \" Virtaul Service 조대협님 블로그 아래는 bookinfo에 대한 Gateway를 등록하는 yaml 파일이다. apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# use istio default controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- \"*\" selector를 이용해서 gateway 타입을 istio에서 디폴트로 제공하는 Gateway를 사용하였다. 그리고 HTTP 프로토콜을 80 포트에서 받도록 했다. apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \"*\"gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- uri:exact:/logout- uri:prefix:/api/v1/productsroute:- destination:host:productpageport:number:9080 spec에서 gateways 부분에 앞에서 정의한 bookinfo-gateway를 사용하도록 한다. 이렇게 하면 앞에서 만든 Gateway로 들어오는 트래픽은 이 Virtual Service로 들어와서 서비스 되는데, 여기서 라우팅 룰을 정의한다. /productpage /login /logout /api/v1/products URL은 productpage:9080 으로 포워딩해서 서비스를 제공한다. Istio에 미리 설치 되어 있는 gateway를 살펴보면, Istio default gateway는 pod로 배포되어 있는데, istio=ingressgateway 라는 라벨이 적용되어 있다. gatewaygateway \" gateway service 도 확인해보면 다음과 같다. gatewaygateway \" gateway 이제 bookinfo를 istio gateway에 등록해서 외부로 서비스를 제공해보자. $ istioctl create -f bookinfo-gateway.yaml bookinfo-gateway.yaml 파일은 다음 링크에 있다. istio/istio create gatewaycreate gateway \" create gateway 게이트웨이 배포가 끝나면 위의 service의 EXTERNAL-IP를 이용해서 접속해보자. http://IP/productpage 로 접속해보면 아래와 같이 정상적으로 작동한다. productpageproductpage \" productpage ","date":"2018-12-30","objectID":"/istio/:7:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"모니터링 툴 서비스 설치가 끝났으면 간단한 테스트와 함께 모니터링 툴을 이용하여 서비스를 살펴보자. Istio를 설치하면 Prometheus, Grafana, Kiali, Jaeget 등의 모니터링 도구가 기본적으로 인스톨 되어 있다. 각각의 도구를 이용해서 지표들을 모니터링 해보자. Grafana를 이용한 서비스별 지표 모니터링 Grafana를 이용해서 각 서비스들의 지표를 상세하게 모니터링 할 수 있다. 먼저 확인전에 아래 스크립트를 이용해 간단하게 부하를 주자. for i in {1..100}; do curl -o /dev/null -s -w \"%{http_code}\" http://35.197.159.13/productpage done 다음 Grafana 웹 콘솔에 접근해야하는데, Grafana는 외부 서비스로 노출이 안되도록 설정이 되어 있기 때문에 kubectl을 이용해서 Grafana 콘솔에 트래픽을 포워딩 하도록 하자. Grafana는 3000번 포트에서 돌고 있기 때문에, localhost:3000 → Grafana Pod의 3000번 포트로 트래픽을 포워딩 하도록 설정하자. kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 다음 localhost:3000 으로 접속해보면 Grafana 화면이 나올 것이다. grafanagrafana \" grafana grafanagrafana \" grafana Jaeger를 이용한 트렌젝션 모니터링 다음은 jaeger를 이용해 개별 분산 트렌젝션에 대해서 각 구간별 응답 시간을 모니터링 할 수 있다. Istio는 각 서비스별로 소요 시간을 수집하는데, 이를 jaeger 오픈소스를 쓰면 쉽게 모니터링이 가능하다. 마찬가지로 jaeger pod로 포트를 포워딩 해야한다. kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 \u0026 이후에 접속하면 Jaeger화면이 보인다. 하지만 나는 error 가 발생한다. .. jaegerjaeger \" jaeger 천천히 해결해야겠다..ㅠㅜ Servicegraph를 이용한 서비스 토폴로지 모니터링 마이크로 서비스는 서비스간 호출 관계가 복잡해서, 각 서비스의 관계를 시각화 해주는 툴이 있으면 유용한데, 대표적인 도구로는 service graph 라는 툴과 kiali 라는 툴이 있다. Bookinfo 예제를 위한 Istio 설정에는 servicegraph가 디폴트로 설치되어 있다. 마찬가지로 외부에 노출하기 위해 포트 포워딩 한다. kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath='{.items[0].metadata.name}') 8088:8088 \u0026 http://localhost:8088/dotviz 로 접속해보면 서비스들의 관계를 볼 수 있다. kialikiali \" kiali 어렵다..ㅠ ","date":"2018-12-30","objectID":"/istio/:8:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"MacOS에서 cluster를 구성한뒤 AWS에 노드들을 띄우는 작업을 해봅시다.","date":"2018-12-21","objectID":"/kops/","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"MacOS에서 cluster를 구성한뒤 AWS에 노드들을 띄우는 작업을 해봅시다. ","date":"2018-12-21","objectID":"/kops/:0:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"kubernetes cluster setting for mac ","date":"2018-12-21","objectID":"/kops/:1:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"설치 먼저 kubernetes를 다루기 위해 kubectl을 다운로드 한다. curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 에러가 날 것인데 최신 릴리즈 버전을 확인하자 curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt 예를들어 위의 결과가 이렇게 나왔다면. v1.12.2 curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.12.2/bin/darwin/amd64/kubectl 이런식으로 다운로드 받으면 된다. chmod +x ./kubectl 실행 모드로 변경하고 sudo mv ./kubectl /usr/local/bin/kubectl Move the binary in to your PATH ","date":"2018-12-21","objectID":"/kops/:1:1","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"설치 확인 kubectl version \u003e Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.2\", GitCommit:\"17c77c7898218073f14c8d573582e8d2313dc740\", GitTreeState:\"clean\", BuildDate:\"2018-10-24T06:54:59Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.6\", GitCommit:\"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\", GitTreeState:\"clean\", BuildDate:\"2018-07-26T10:04:08Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} 이런식으로 client와 server가 둘다 나와야 잘 설치 된 것이다. ","date":"2018-12-21","objectID":"/kops/:1:2","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"AWS CLI AWS에 노드를 띄우기 위해 AWS의 설정을 해준다. $ sudo apt update $ sudo apt install -y awscli $ aws configure \u003e public, secret key 입력 ","date":"2018-12-21","objectID":"/kops/:1:3","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Kops 쉽게 클러스터를 전개 하기 위해 kops를 설치한다. $ wget https://github.com/kubernetes/kops/releases/download/1.10.0/kops-darwin-amd64 $ chmod +x kops-darwin-amd64 $ sudo mv kops-darwin-amd64 /usr/local/bin/kops 설치 완료되면 $ kops 명령어가 잘 먹을 것이다. kops로 전개한 AWS EC2에 접속하기 위한 key 생성을 해보자 ","date":"2018-12-21","objectID":"/kops/:1:4","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Key $ ssh-keygen -q -f ~/.ssh/id_rsa -N \"\" or $ kops create secret --name awskrug.k8s.local sshpublickey admin -i ~/.ssh/id_rsa.pub 이제 본격적인 Cluster를 만들어보자. ","date":"2018-12-21","objectID":"/kops/:1:5","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Cluster $ export KOPS_CLUSTER_NAME=awskrug.k8s.local $ export KOPS_STATE_STORE=s3://kops-awskrug-test $ aws s3 mb ${KOPS_STATE_STORE} --region us-east-1 환경 변수 설정을 해주고 S3에 데이터를 저장하기위해 버킷을 생성을 한다. kops create cluster \\ --cloud=aws \\ --name=${KOPS_CLUSTER_NAME} \\ --state=${KOPS_STATE_STORE} \\ --master-size=t2.medium \\ --node-size=t2.medium \\ --node-count=2 \\ --zones=us-east-1b,us-east-1b \\ --ssh-public-key=~/.ssh/k8s.pub \\ --network-cidr=10.10.0.0/16 원하는 대로 설정이 가능하고 --ssh-public-key=~/.ssh/k8s.pub 부분은 위에 Key 생성한 걸로 해주면 노드에 접속할 수 있다. 노드 접속은 ssh -i ~/.ssh/k8s admin@184.73.133.76 이런식으로 EC2 instance의 IP를 써주면 접속이 가능하다. ","date":"2018-12-21","objectID":"/kops/:1:6","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Update Cluster 클러스터 생성 하기 전 수정하기 $ kops edit cluster --name=${KOPS_CLUSTER_NAME} ","date":"2018-12-21","objectID":"/kops/:1:7","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Create Cluster kops update에 –yes 옵션을 주면 실제 클러스터가 생성된다. $ kops update cluster --name=${KOPS_CLUSTER_NAME} --yes ","date":"2018-12-21","objectID":"/kops/:1:8","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Validate Cluster kops validate 명령으로 생성이 완료 되었는지 확인 할 수 있다. $ kops validate cluster --name=${KOPS_CLUSTER_NAME} create cluster 했는데 생성이 안되면 기다려야한다. 클러스터 생성까지 5~10분 정도소요되기 때문이다. ","date":"2018-12-21","objectID":"/kops/:1:9","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"클러스터 확인 및 수정 다음 명령어로 클러스터(master \u0026 node) 확인 할 수 있다. $ kops get ig AWS 인스턴스들을 정지 시키고 싶을때 혹은 다시 실행 시키고 싶을 때 $ kops edit ig nodes $ kops edit ig \u003cmaster_name\u003e maxSize, minSize를 0으로 바꾸면 인스턴스 생성이 중지된다. 또는 1이나 2같은 숫자를 쓰면 그만큼 생성 된다. update cluster configuration $ kops update cluster --yes Rollling update $ kops rolling-update cluster --yes $ kops rolling-update cluster --cloudonly --force --yes kops로 cluster를 구성하게되면 cluster의 정보가 amazon S3에 저장 되기 때문에 어느 환경에서든 kops와 kubectl만 깔려있으면 어디서든 cluster를 조작할 수 있다. ","date":"2018-12-21","objectID":"/kops/:2:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"kubernetes","date":"2018-12-20","objectID":"/basic/","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"쿠버네티스의 클러스터 구조는 다음과 같다. 클러스터 전체를 관리하는 컨트롤러로써 마스터가 존재한다. 컨테이너가 배포되는 머신인 노드가 존재한다. 구성구성 \" 구성 조대협님 블로그 ","date":"2018-12-20","objectID":"/basic/:0:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"오브젝트 가장 기본적인 구성단위가 되는 기본 오브젝트와, 이 기본 오브젝트를 생성하고 관리하는 추가적인 기능을 가진 컨트롤러로 이루어진다. 그리고 이러한 오브젝트의 스펙 이외에 추가정보인 메타 정보들로 구성이 된다고 보면 된다. ","date":"2018-12-20","objectID":"/basic/:1:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"오브젝트 스펙 ( object spec ) 오브젝트들은 모두 오브젝트의 특성을 기술한 오브젝트 스펙으로 정의가 되고, 커맨드 라인을 통해서 오브젝트 생성시 인자로 전달하여 정의를 하거나 또는 yaml 이나 json 파일로 스펙을 정의할 수 있다. ","date":"2018-12-20","objectID":"/basic/:1:1","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"기본 오브젝트 ( basic object ) 쿠버네티스에 의해서 배포 및 관리되는 가장 기본적인 오브젝트는 컨테이너화되어 배포되는 애플리케이션의 워크로드를 기술하는 오브젝트로 pod, service, volume, namespace 4가지가 있다. pod → 컨테이너화된 애플리케이션 volume → 디스크 service → 로드밸런서 namespace → 패키지명 정도로 생각하면된다. ","date":"2018-12-20","objectID":"/basic/:1:2","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Pod pod는 쿠버네티스에서 가장 기본적인 배포 단위로 컨테이너를 포함하는 단위이다. apiVersion:v1kind:Podmetadata:name:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:8090 pod은 여러개의 컨테이너를 가질 수 있는데 왜 여러개로 나눠서 배포를 하냐? pod 내의 컨테이너는 IP와 port를 공유한다. 두개의 컨테이너가 하나의 pod를 통해서 배포되었을때 localhost를 통해서 통신이 가능하다. pod내에 배포된 컨테이너간에는 디스크 볼륨을 공유할 수 있다. ","date":"2018-12-20","objectID":"/basic/:2:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"volume pod가 동작할때 디폴트로 컨테이너마다 로컬 디스크를 생성해서 기동되는데 영구적이지 못하다. 따라서 DB같이 영구적으로 파일을 저장해야 하는 경우에는 컨테이너 리스타트에 상관 없이 파일을 영속적으로 저장해야 하는데 이러한 형태를 볼륨이라한다. 쿠버네티스는 github같은 오픈소스 기반의 외장 스토리지를 지원한다.. ","date":"2018-12-20","objectID":"/basic/:3:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"service pod 경우에는 동적으로 생성되고 자동으로 리스타트가 되면서 IP가 바뀌기 때문에, 로드밸런서에서 pod의 목록을 지정할 떄는 IP주소를 이용하는 것은 어렵다. 따라서 유연하게 선택해줘야하는데 이것이 라벨 과 라벨셀렉터 라는 개념이다. 서비스를 정의할때 어떤 pod를 서비스로 묶을 것인지 정의하는데 이를 셀렉터라고 한다. 서비스는 라벨 셀렉터에서 특정 라벨을 가지고 있는 pod만 선택하여 서비스에 묶게 된다. ","date":"2018-12-20","objectID":"/basic/:4:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"name space 네임스페이스는 한 쿠버네티스 클러스터내의 논리적인 분리단위라고 보면 된다. ","date":"2018-12-20","objectID":"/basic/:5:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"컨트롤러 앞의 4개의 기본오브젝트로 애플리케이션을 설정하고 배포하는 것이 가능하다. 이를 더 편하게 관리하기 위해 컨트롤러 라는 개념을 사용한다. Repication controller 지정된 숫자로 pod를 기동 시키고 관리하는 역할을 한다. Replicaset replication controller의 새버전이라 생각하면된다. Deployment Replication controller와 Replica Set의 좀더 상위 추상화 개념이다. 실제 운영은 이를 많이 사용한다. ","date":"2018-12-20","objectID":"/basic/:6:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"고급 컨트롤러 RC,RS,Deployment는 웹서버와 같은 일반적인 워크로드에 대해 pod를 관리하기 위한 컨트롤러이다. 실제 운영환경에서는 웹서버와 같은 일반적인 워크로드 이외에, 데이타베이스, 배치 작업, 데몬 서버와 같이 다양한 형태의 워크로드 모델이 존재한다. ","date":"2018-12-20","objectID":"/basic/:7:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Daemonset pod가 각각의 노드에서 하나씩만 돌게 하는 형태로 Pod를 관리하는 컨트롤러 이다. RC나 RS에 의해서 관리되는 Pod는 여러 노드의 상황에 따라 일반적으로 비균등적으로 배포가 되지만, DS에 의해 관리되는 pod는 모든 노드에 균등하게 하나씩만 배포 된다. 이런 형태의 워크로드는 서버의 모니터링이나 로그 수집 용도로 많이 사용되는데, DS의 다른 특징중 하나는, 특정 Node들에만 Pod가 하나씩만 배포 되도록 설정이 가능하다. 또한 DS는 특정 노드에만 pod를 배포할 수 있도록, pod의 node selector를 이용해서 라벨을 이용하여 특정 노드만을 선택할 수 있게 지원한다. ","date":"2018-12-20","objectID":"/basic/:7:1","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Job 워크로드 모델중에서 배치나 한번 실행되고 끝나는 형태의 작업이 있다. 예를 들어 원타임으로 파일 변환 작업을 하거나, 또는 주기적으로 ETL 배치 작업을 하는 경우에는 웹서버 처럼 계속 pod가 떠 있을 필요없이 작업할때만 pod를 띄우면된다. 이를 지원하는 컨트롤러를 job이라한다. Job을 정의할때는 보통 아래와 같이 컨테이너 스펙 부분에 image뿐만 아니라 컨테이너에서 Job을 수행하기 위한 커맨드를 입력한다. apiVersion:batch/v1kind:Jobmetadata:name:pispec:template:spec:containers:- name:piimage:perlcommand:[\"perl\",\"-Mbignum=bpi\",\"-wle\",\"print bpi(2000)\"]restartPolicy:NeverbackoffLimit:4 Job 컨트롤러에 의해서 실행된 pod는 command의 실행 결과에 따라서 job이 실패한지 성공한지를 판단한다. 만약 Job이 끝나기전에 비정상적으로 종료된다면? RC/RS에 의해서 관리 되고 있다면 계속 생성될것이다. 장애시 다시 시작하게 하거나 또는 장애시 다시 시작하지 않게 할 수 있다. ","date":"2018-12-20","objectID":"/basic/:7:2","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Cron job unix cron 처럼 시간에 따른 실행조건을 정의해놓을 수 있고, 이에 따라 job 컨트롤러를 실행하여, 정의된 pod를 실행할 수 있게 한다. apiVersion:batch/v1beta1kind:CronJobmetadata:name:hellospec:schedule:\"*/1 * * * *\"jobTemplate:spec:template:spec:containers:- name:helloimage:busyboxargs:- /bin/sh- -c- date; echo Hello from the Kubernetes clusterrestartPolicy:OnFailure 다른 점은 cronjob 스펙 설정 부분에 schedule 이라는 항목이 있고 반복 조건을 unix cron과 같이 설정하면 된다. ","date":"2018-12-20","objectID":"/basic/:7:3","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Statefulset RS/RC나 다른 컨트롤러로는 데이타베이스와 같이 상태를 가지는 애플리케이션을 관리하기가 어렵다. 그래서 이렇게 데이타 베이스등과 같이 상태를 가지고 있는 Pod를 지원하기 위해서 Statefulset이라는 것이 소개 되었는데 볼륨할때 다시 본다. ","date":"2018-12-20","objectID":"/basic/:7:4","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"}]