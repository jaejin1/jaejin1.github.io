[{"categories":["aws"],"content":"AWS Auto Scaling은 Lifecyle을 추가할 수 있는 기능을 제공한다. Lifecycle 이벤트가 발생할 때 인스턴스에 대한 사용자 정의 작업을 수행할 수 있다. Amazon EC2 Auto Scaling Lifecycle Hook AWS EC2 ASG에서 인스턴스 시작 및 종료에 대한 이벤트를 잡아서 어떠한 작업을 수행하고 싶을 때가 있다. 예를 들어 ASG와 CodeDeploy가 설정되어 있고, ASG에서 Scale Out 이벤트가 발생 했을 때 CodeDeploy에서 배포가(인스턴스가 생성될 때 CodeDeploy 배포가 실행됨) 일어나기 전 어떤 스크립트를 실행하여 어플리케이션의 필요한 소프트웨어 패키지를 다운로드하여 트래픽 수신을 시작하기 전에 준비를 마치도록 할 수 있는 작업을 설정 할 수 있다. Sclae In 시점에 인스턴스가 종료되기 전 EventBridge로 인스턴스에 연결하여 종료되기 전에 로그파일을 다운로드 받는 로직을 추가할 수도 있고 Lambda를 이용해 추가 알림을 보낼 수 있는 작업을 할 수 있다. AWS 문서에서 Lifecycle의 일반적인 용도는 인스턴스가 ELB에 등록되는 시점을 제어하는 것이라고 한다. Licecycle Hook 작동 방식 lifecycle hooklifecycle hook \" lifecycle hook 출처 - https://github.com/donnemartin/system-design-primer#database 참고 https://docs.aws.amazon.com/ko_kr/autoscaling/ec2/userguide/lifecycle-hooks.html https://docs.aws.amazon.com/ko_kr/autoscaling/ec2/userguide/lifecycle-hooks-overview.html ","date":"2023-01-07","objectID":"/asg-lifecycle-hook/:0:0","tags":["cloud","aws","asg"],"title":"ASG Lifecycle Hook","uri":"/asg-lifecycle-hook/"},{"categories":["docker"],"content":"docker file system","date":"2021-08-16","objectID":"/docker-filesystem/","tags":["docker"],"title":"docker file system","uri":"/docker-filesystem/"},{"categories":["docker"],"content":"docker에서 사용하고 있는 Union File System(UFS)에 대해 이해해보자. UFS는 쉽게 여러 개의 File System을 하나로 결합하여 취급할 수 있도록 해주는 FS라 보면 된다. ","date":"2021-08-16","objectID":"/docker-filesystem/:0:0","tags":["docker"],"title":"docker file system","uri":"/docker-filesystem/"},{"categories":["docker"],"content":"UFS docker image를 이용해 container를 띄운다는 것은 밑의 그림과 같이 image layer(read-only layer)위에 container layer(writable layer)를 얹은 후 사용자에게 root filesystem으로 제공하는 것이다. container를 띄울 때 사용하는 image들은 변경되지 않고 유지되는 이유가 이에 있다. container를 띄우고 변경하는 사항(Diff)들은 하단의 image layer(read-only)를 사용하지 않고 container layer(writable layer)를 이용하기 떄문에.. 다만 container를 내리게 되면 내부에 생성된 파일이라던지 변경된 작업들이 사라지게 된다. 이 변경 사항들(Diff)를 유지하고자 한다면 container에서 docker commit으로 새로운 image를 생성하거나 volume을 구성하여 변경사항은 해당 volume에 저장하도록 하거나 보통은 image를 변경하지 않기 때문에 volume을 많이 이용한다. ","date":"2021-08-16","objectID":"/docker-filesystem/:1:0","tags":["docker"],"title":"docker file system","uri":"/docker-filesystem/"},{"categories":["docker"],"content":"Container size 간단하게 위에서 말한 image layer와 container layer의 size를 확인해보자. image-sizeimage-size \" image-size 현재 sample로 가져온 docker image 크기는 70.3MB이다. container-sizecontainer-size \" container-size 이 image로 띄운 container도 동일하게 70.3MB로 표시되지만 8B정도 container내에서 사용한 것을 볼 수 있다. virtual size : read-only + writable size : writable size 여기서 만약 container내부에 파일을 생성해보면 49B정도 추가된 것을 볼 수 있다. container-sizecontainer-size-file \" container-size 이제 이걸 docker commit으로 새로운 image를 만들어보자. docker-commitdocker-commit \" docker-commit docker history를 조회해보면 맨 상단의 layer가 위에서 추가한 file이 적용 된 것을 볼 수 있다. 지금 이렇게 변경한 사항들은 모두 호스트의 /var/lib/docker 디렉토리에 저장된다. 이 영역을 backing filesystem이라고 한다. ","date":"2021-08-16","objectID":"/docker-filesystem/:1:1","tags":["docker"],"title":"docker file system","uri":"/docker-filesystem/"},{"categories":["docker"],"content":"Storage Driver container에서 사용자가 UFS를 통해 filesystem을 쉽게 사용할 수 있는 것은 storage driver가 있기 떄문이다. storage driver는 file i/o 처리를 하는 녀석이다. 보통 많이 사용하는 storage driver는 overlay를 많이 사용하고 이는 os에 따라 다를 수 있다. container layer(writable layer)는 backing filesystem의 특정 영역에 mount가 되어 있어야하고 아까 본 container의 mount된 것을 확인해보면 밑의 사진과 같이 확인 할 수 있다. mountsmounts \" mounts host의 /var/lib/docker/overlay2/ 해당 폴더를 확인하면 아까 생성한 test.txt가 존재하는 것을 확인할 수 있다. overlayoverlay \" overlay ","date":"2021-08-16","objectID":"/docker-filesystem/:1:2","tags":["docker"],"title":"docker file system","uri":"/docker-filesystem/"},{"categories":["os","linux"],"content":"tty","date":"2021-06-28","objectID":"/tty/","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"리눅스의 /dev 디렉토리에서 볼 수 있는 /dev/tty*, /dev/pts/* 파일들이 입출력에 관련된 파일들이며 외부 터미널 장치를 연결할 때, 리눅스에서 가상콘솔을 제공할 때 xterm이나 telnet, ssh 같은 리모트 로그인 프로그램등에 의해 사용된다. ","date":"2021-06-28","objectID":"/tty/:0:0","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"입출력 장치 사용의 구분 ","date":"2021-06-28","objectID":"/tty/:1:0","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"외부 터미널 장치 연결 case1case1 \" case1 UART driver 물리적으로 bytes를 전송하는 역할을 한다. parity checks나 flow control을 수행 한다. Line discipline low level 장치 드라이버 위에 line discipline이라는 레이어를 하나 더 두면 같은 장치를 여러가지 용도로 사용할 수 있다. 텍스트를 입력할 때 backspace, clear line, reprint등과 같은 line editing기능을 standard line discipline을 통해 제공한다. line_disciplineline_discipline \" line_discipline TTY driver Session managerment기능, job control과 관련된 기능을 제공한다. Ctrl-z를 누르면 실행중인 job을 suspend 시키고, Ctrl-c를 누르면 종료시키고, 사용자 입력은 foreground job에만 전달되게 하고 등등은 모두 TTY driver에서 제공하는 기능이다. ","date":"2021-06-28","objectID":"/tty/:1:1","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"리눅스 virtual console case2case2 \" case2 OS에서 제공하는 가상콘솔이다. 실제 물리적인 장치가 연결된 것이 아니기 때문에 커널에서 터미널을 emulation한다. line discipline, TTY driver의 기능은 위와같고 마찬가지로 background getty 프로세스에 의해 login prompt가 제공된다. /dev/tty{number}파일이 사용됨. tty_numbertty_number \" tty_number ","date":"2021-06-28","objectID":"/tty/:1:2","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"Pseudo TTY (xterm, telnet, ssh …) case3case3 \" case3 앞서 virtual console에서는 커널에서 터미널을 emulation했다면, TTY driver가 제공하는 session management기능과 line discipline을 그대로 사용하면서 사용자 프로그램에서 터미널을 emulation 하는 것이 PTY (Pseudo TTY) 이다. PTY는 master/slave pair로 이뤄지는데 /dev/ptmx (pseudo-terminal master multiplexer) 파일을 open하면 pseudo terminal master(PTM)에 해당하는 file descriptor가 반환되고 pseudo terminal slave(PTS)에 해당하는 device가 /dev/pts/ 디렉토리에 생성된다. 두 ptm과 pts가 open하게 되면 /dev/pts/{number}는 실제 터미널과 같은 인터페이스를 프로세스에 제공한다. xterm같은 터미널을 예로들면 xterm이 ptm에 연결되고 어플리케이션 프로세스 (ex. bash)가 pts에 연결되는 구조이다. xterm에서 사용자가 입력을 하면 ptm -\u003e 커널의 line discipline -\u003e pts를 거쳐 bash 프로세스에 전달되고, 명령 실행 결과는 다시 pts -\u003e line discipine -\u003e ptm을 거쳐 xterm에 전달되면 xterm은 실제 터미널과 같이 화면에 표기하게 된다. ptypty \" pty 만약 ssh접속을 했다면, stdin, stdout, stderr 모두 터미널에 연결되어 있으므로 터미널에서 ls명령을 실행하면 ssh명령의 입력으로 들어가고 네트워크를 거쳐 sshd에 전달되면 ptm -\u003e pts -\u003e shell로 전달된다. 명령의 실행결과는 다시 pts -\u003e ptm -\u003e sshd에 전달되고 네트워크를 거쳐 터미널로 출력하게 된다. ssh_ttyssh_tty \" ssh_tty ","date":"2021-06-28","objectID":"/tty/:1:3","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"Controlling Terminal controlling_terminalcontrolling_terminal \" controlling_terminal controlling terminal은 session leader에 의해 할당되며 이것은 보통 /dev/tty*나 /dev/pts/*와 같은 터미널 device를 말한다. PID와 SID가 같은 프로세스를 session leader라고 하는데 오직 session leader만이 controlling terminal을 획득할 수 있다. session leader를 controlling process라고도 한다. 하나의 session은 하나의 controlling terminal만 가질 수 있다. session은 하나의 foreground process group과 여러개의 background process groups로 구성된다. Ctrl-c를 누르면 SIGINT신호가 foreground process group에 전달 된다. network가 끊기면 SIGHUP신호가 session leader에 전달되고 session leader는 같은 SID를 갖는 프로세스들에게 전달한다. ","date":"2021-06-28","objectID":"/tty/:2:0","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"/dev/tty /dev/tty는 특수 파일로 프로세스의 controlling terminal과 동일하다. 현재 ctty가 /dev/pts/12라면 /dev/tty도 /dev/pts/12와 같다고 할 수 있다. 어떤 프로세스가 /dev/tty를 open 하는데 실패하였다면 ctty를 갖고 있지 않다고 할 수 있다. standard streams이 모두 redirect 되어 있다고 하더라도 /dev/tty로 출력하면 터미널로 출력할 수 있고 /dev/tty를 이용해 터미널로 입력 받을 수 있다. 한가지 주의할 점은 /dev/tty로의 출력은 standard streams을 거치지 않고 직접 터미널 장치에 쓰기 때문에 메시지를 잡아서 redirect할 수 있다. # /dev/tty 로의 출력은 redirect 할수없다. $ { echo hello \u003e /dev/tty ;} \u003e /dev/null hello $ { echo hello \u003e /dev/tty ;} 2\u003e /dev/null hello # 따라서 다음과 같이 파일로 저장할 수도 없습니다. $ cat test.sh echo hello \u003e /dev/tty ------------------------------ $ ./test.sh \u003e out # stdout hello $ ./test.sh 2\u003e out # stderr hello ","date":"2021-06-28","objectID":"/tty/:3:0","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"Configuring TTY device tty 명령으로 현재 shell의 tty device를 조회할 수 있고 stty명령을 이용해 설정 값을 변경할 수 있다. sttystty \" stty 값을 바꿀때가 있을까?? 싶긴한데 첫번째 라인의 rows 30; columns 126;을 보면 해당값은 TTY driver에 해당되는 값으로 터미널 프로그램의 윈도우 사이즈를 조절하면 값이 변경된다. TTY driver는 foreground job에 SIGWINCH신호를 보내게 된다. 이 부분은 회사에서 bastion을 개발할 떄 사용했던 내용이라 밑부분에서 go언어로 작성 했었던 부분을 가져와서 봐보자. 다른 설정들은 그때 검색해서 확인해보면 될 듯 하다. 참고 https://mug896.github.io/bash-shell/tty.html ","date":"2021-06-28","objectID":"/tty/:4:0","tags":["os","linux"],"title":"tty","uri":"/tty/"},{"categories":["os","linux"],"content":"Job Control","date":"2021-05-19","objectID":"/jobcontrol/","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"터미널에서 단순히 한나의 명령을 실행시키는게 아니라 \u0026을 이용해 background job을 생성할 수 있다. ","date":"2021-05-19","objectID":"/jobcontrol/:0:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"Job id와 Job specification 명령을 \u0026을 이용해 backgroud로 실행시키면 결과로 job id와 process id를 보여준다. backgroundbackground \" background [1] 부분이 job id에 해당하고 그 뒤 80284, 80285, 80286은 process id(pid)에 해당된다. 만약 kill을 이용해 job에 신호를 보낼 때 job id를 사용한다면 pid와 구분할 수 없게 된다. 그래서 job id 대신 job specification(jobspec)을 사용하는데 job id앞에 %을 붙여서 만든다. jobspec을 이용해 kill을 하면 pgid (process group id)를 갖는 프로세스 들에게 모두 전달되므로 만약에 child process가 생성되어 실행 중이라면 함께 종료된다. ","date":"2021-05-19","objectID":"/jobcontrol/:1:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"Jobspec과 pid가 다른점 pid는 개별 프로세스를 나타내지만 jobspec은 파이프로 연결된 모든 프로세스를 포함한다. ","date":"2021-05-19","objectID":"/jobcontrol/:2:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"jobs jobsjobs \" jobs 현재 job table 목록을 보여준다. -l을 이용하면 process id도 함께 보여준다. 옆에 +는 current job(가장 최근에 background상태가 된 job을 나타냄) -는 previous job을 나타낸다. fg명령을 통해 +, -위치도 바꿀 수 있다. ","date":"2021-05-19","objectID":"/jobcontrol/:3:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"fg 현재 background에 stopped또는 running상태에 있는 job을 foreground로 실행하고 current job으로 만든다. 이후에 ctrl-z로 stopped되었을때 job table에는 +로 표신된다. ","date":"2021-05-19","objectID":"/jobcontrol/:4:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"bg ctrl-z에 의해 현재 stopped 상태에 있는 background job에 SIGCONT 신호를 보내 background running 상태로 만든다. jobspec을 인수로 주지않으면 current job이 사용된다. ","date":"2021-05-19","objectID":"/jobcontrol/:5:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"job control 관련 키 ctrl-c interrupt신호 (SIGINT)를 foreground job에 보내 종료시킨다. ctrl-z suspend신호 (SIGTSTP)를 foreground job에 보내 suspend 시키고 background에 있던 shell 프로세스를 foreground로 하여 명령을 입력 받을 수 있게 한다. ","date":"2021-05-19","objectID":"/jobcontrol/:6:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["os","linux"],"content":"input and output input 입력은 foreground job에서만 받을 수 있다. background job에서 입력을 받게되면 SIGTTIN 신호가 전달되어 suspend된다. output 출력은 기본적으로 현재 session에서 실행되고 있는 모든 job들이 공유한다. 그러므로 backgound job을 실행할 때 제대로 redirection처리를 하지 않으면 터미널로 출력되는 메시지들이 서로 섞이게 된다. ","date":"2021-05-19","objectID":"/jobcontrol/:7:0","tags":["os","linux"],"title":"Job Control","uri":"/jobcontrol/"},{"categories":["cs"],"content":"browser에서 url을 입력하면 무슨일이 일어날까?","date":"2021-05-10","objectID":"/cs/","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"웹 browser에 url을 입력하면 무슨일이 일어날지 정리해보자. ","date":"2021-05-10","objectID":"/cs/:0:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"url을 주소창에 친다. browser가 url을 해석하고, url문법에 따라서 요청을 하는데 만약 문법이 틀리다면 웹 browser의 기본 검색엔진으로 검색 요청을 한다. url의 문법은 URL 위키백과 에 자세히 나와있다. ","date":"2021-05-10","objectID":"/cs/:1:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"HSTS(HTTP Strict Transport Security) 목록을 로드해서 확인한다. HSTS 목록에 있으면 첫 요청을 HTTPS로 보내고, 아닌경우 밑의 설명과 같이 HTTP로 보내서 HSTS 설정을 가져온다. (설정이 없다면 뭐 HTTP로 통신할 것이다.) ","date":"2021-05-10","objectID":"/cs/:2:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"HSTS란 무엇인지 알아보자. 일반적으로 HTTPS를 강제하게 될 때 서버측에서 302 redirection을 이용해 전환시켜주는데 이것이 취약점이 될 수 있다고 한다. http요청 -\u003e 서버에서 302 redirect -\u003e 클라이언트에서 https 재요청 -\u003e 200 found 따라서 클라이언트에게 강제로 HTTPS를 사용하도록 권장되는데 이것이 HSTS이다. 클라이언트에서 강제하기 때문에 http를 이용한 연결 자체가 최초부터 시도되지 않으며 클라이언트 측에서 차단된다는 장점이 있다. https요청 -\u003e 200 found 사용자가 최초로 사이트에 접속시도를 하게 되면 웹서버는 HSTS설정에 대한 정보를 browser에게 응답하게 된다. browser는 이 응답을 근거로 일정 시간동안 HSTS응답을 받은 웹사이트에 대해서 https접속을 강제화 하게 된다. 실제 Chrome에서는 chrome://net-internals/#hsts 주소에서 확인 할 수 있다. hstshsts \" hsts 또한 몇가지 웹사이트들은 browser자체에 등록되어 있는데 이를 확인하는 주소는 다음과 같다. https://www.chromium.org/hsts preloaded-hstspreloaded-hsts \" preloaded-hsts ","date":"2021-05-10","objectID":"/cs/:2:1","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"HSTS의 설정 방법 HSTS는 위에서 확인한 것과 같이 최초 요청은 서버에서 HSTS설정에 대한 정보를 가져 오기 때문에 서버에서 설정해 주어야 한다. 예제 apache httpd Header always set Strict-Transport-Security “max-age=86400; includeSubdomains; preload” nginx add_header Strict-Transport-Security “max-age=86400; includeSubdomains; preload” 설정을 보면 max-age=86400 : HSTS가 설정된 시간이며 초단위 이다. includeSubdomains : HSTS가 적용될 도메인의 subdomain까지 HSTS를 확장 적용함을 의미한다. preload : HSTS 적용이 클라이언트 측에서 preload로 이루어짐을 의미한다. ","date":"2021-05-10","objectID":"/cs/:2:2","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"DNS(Domain Name Server)를 조회한다. DNS에 요청을 보내기 전에 먼저 browser에 해당 Domain이 cache돼 있는지 확인한다. DNS query가 이 곳에서 가장 먼저 실행된다. 또한 os 캐시를 확인하고, router 캐시를 확인하고 isp 캐시를 확인할 수도 있다. 없을경우 local에 저장돼 있는 /etc/hosts 파일에서 참조할 수 있는 domain이 있는지 확인한다. 1,2가 모두 실패 했을 경우 Network stack에 구성돼 있는 DNS로 요청을 보낸다. ","date":"2021-05-10","objectID":"/cs/:3:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"DNS에 대해서 좀 더 자세히 알아보자. DNS는 url들의 이름과 ip주소를 저장하고 있는 데이터 베이스이다. 모든 url들은 고유의 ip가 지정되어 있고 해당 ip를 통해서 서버에 접근할 수 있다. ip를 확인하는 방법은 nslookup www.google.com 으로 확인할 수 있고 해당 ip를 browser에 쳐보면 google로 바로 접근 가능하다. DNS로 요청을하면 해당 IP주소를 찾기위해서 동작하는 과정을 알아보자. www.google.com과 같은 주소로 접속하고 싶으면 해당 ip를 반드시 알아야하는데, DNS query로 여러 다른 DNS 서버들을 검색해서 해당 사이트의 IP주소를 찾는 것이다. ip주소를 찾을 때 까지 DNS서버에서 다른 dns서버를 오가면서 반복적으로 검색한다. 이때 isp DNS서버를 DNS recursor라고 부른다. url들은 third-level, second-level, top-level domain을 가지고 있다. 각 level별로 자신만의 name서버가 있고 여기서 DNS lookup 프로세스 중에 query가 진행된다. url로 접속하면 처음에 DNS recurser가 root name server에 접근한다. root name 서버는 .com 도메인 name server로 redirect한다. .com name server는 google.com name server로 redirect한다. google.com name server는 DNS 리스트에서 www.google.com에 매칭되는 IP주소를 찾고 DNS recursor로 보내게 된다. 이 요청들은 데이터 패킷들을 통해 보내진다. 패킷 안에는 요청의 내용과 DNS recursor의 ip주소가 포함되어 있다. 이 패킷들은 원하는 DNS 기록을 가진 DNS 서버에 도달할 때 까지 클라이언트와 서버를 여러번 왔다갔다한다. 패킷들이 움직이는 것도 routing table에 기반하고 패킷이 loss되면 request fail error가 발생한다. ","date":"2021-05-10","objectID":"/cs/:3:1","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"ARP로 대상의 IP와 MAC address를 알아낸다. ARP(Address Resolution Protocol)이란 논리적인 IP주소를 기반으로 물리적인 MAC주소로 바꾸어주는 주소 해석 프로토콜이다. ","date":"2021-05-10","objectID":"/cs/:4:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"ARP 캐시 테이블 arp -a arparp \" arp apr -a 명령을 실행하면 IP와 MAC 주소의 대응 관계를 볼 수 있다. 이처럼 대응 관계를 저장한 테이블이 ARP 캐시 테이블이다. ","date":"2021-05-10","objectID":"/cs/:4:1","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"만일 APR캐시 테이블이 비어있는 경우 10.99.192.23이 출발지의 IP주소라고 생각해보자. 구분 출발지 목적지 IP 주소 10.99.192.23 8.8.8.8 MAC 주소 d0:65:ca:83:f7:41 ARP 캐시 테이블이 빈 상황에서 8.8.8.8에 접근하려할 시 운영체제는 출발지 네트워크 ID와 도착지 네트워크 ID를 비교한다. 출발지는 10.99.192이지만 목적지는 8.8.8이므로 네트워크 ID가 다르다. 다른 LAN영역이기 때문에 목적지 IP 주소를 라우터 IP주소로 변경해야한다. 구분 출발지 목적지 IP 주소 10.99.192.23 라우터 IP 주소 MAC 주소 d0:65:ca:83:f7:41 동일한 LAN영역에 위치한 게이트웨이까지 스위칭 통신을 하기 위해 목적지 MAC 주소가 필요하다. 출발지 호스트는 자기가 속한 LAN 영역 전체를 대상으로 라우터 IP 주소에 대응하는 MAC주소를 찾기 위해 ARP질의를 브로드 캐스트 방식으로 전송한다. 구분 출발지 목적지 IP 주소 10.99.192.23 라우터 IP 주소 MAC 주소 d0:65:ca:83:f7:41 26:7a:a:2f:bc:e2 이제 ARP 캐시 테이블에 목적지 MAC 주소가 올라오면 운영체제는 이를 참조해 전송할 데이터를 유니캐스트 방식으로 게이트웨이에 전송한다. 이후, 게이트웨이가 IP주소에 기반한 라우팅 통신을 통해 목적지 호스트가 있는 게이트웨이로 데이터를 전송한다. 이처럼 ARP 요청과 응답이 일어나는 공간을 ARP영역이라 하며, ARP 동작은 동일한 네트워크 ID를 공유하는 호스트를 대상으로 MAC 주소를 구하는 기능인 만큼 ARP영역 자체가 LAN영역을 의미한다. ","date":"2021-05-10","objectID":"/cs/:4:2","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"Browser가 서버와 TCP connection을 한다. browser가 드디어 IP주소를 받게되면 서버와 connection을 빌드한다. TCP segment가 만들어지는 Transport layer로 전달후 대상 포트 80 or 443이 header에 추가되고 source port는 시스템에서 동적 포트 범위내에서 임의 지정 TCP segment를 Network layer로 전송 segment header에 대상 컴퓨터의 IP주소와 현재 컴퓨터의 IP주소가 삽입된 packet 구성. packet이 Link layer로 전달. 시스템의 MAC address와 gateway의 MAC주소를 포함하는 Frame header추가. (이때 gateway의 MAC Address를 모르는경우 위에서 설명한 ARP를 이용해 찾아와야한다.) packet이 ethernet, wifi등 네트워크 매체로 전송 packet이 대상 router에 도착하게되면 packet의 ip header에서 target address를 추출해 다음 hop으로 라우팅 ","date":"2021-05-10","objectID":"/cs/:5:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"만약 HTTPS일 경우 TLS handshake가 추가된다. TLS(Transport layer security) 는 SSL(Secure Sockets Layer)이 표준화 되면서 바뀐 이름이라고 생각하면 될 것 같다. HTTPS로 통신하게 되면 TCP socket 통신과정 전에 다음과 같은 작업이 추가된다. HTTPS 통신에서 이뤄지는 동작들은 다음 페이지를 참조하자. https://jaejin1.github.io/https/ ","date":"2021-05-10","objectID":"/cs/:5:1","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"HTTP 서버가 응답한다. HTTPD(http daemon) 서버는 요청/응답을 처리하는 서버이다. 일반적인 HTTPD서버는 apache, nginx등이 있다. HTTPD서버가 요청을 수신 서버는 요청을 구분 HTTP method 도메인 요청된 경로 서버는 도메인에 해당하는 서버에 구성된 가상 호스트가 있는지 확인 서버는 GET 요청을 수락할 수 있는지 확인 서버는 클라이언트가 IP, 인증 등을 통해 이 method를 사용할 수 있는지 확인 서버에 rewite module이 설치 되어 있으면 요청된 rule중 하나와 일치하도록 시도. 서버는 요청에 해당하는 콘텐츠를 가져옴. 서버는 핸들러에 따라 파일을 구문 분석 ","date":"2021-05-10","objectID":"/cs/:6:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["cs"],"content":"Web browser가 그린다. 서버가 리소스를 browser에 제공하면 화면에 뿌려준다. html, css, js같은 파일들은 또다른 프로세스를 통해 이쁘게 그려줄것이다. 참고 https://mer1.tistory.com/55 https://devjin-blog.com/what-happen-browser-search/ https://owlgwang.tistory.com/1 ","date":"2021-05-10","objectID":"/cs/:7:0","tags":["basic","cs"],"title":"browser에서 url을 입력하면 무슨일이 일어날까?","uri":"/cs/"},{"categories":["system-design"],"content":"Database","date":"2021-05-03","objectID":"/database/","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"databasedatabase \" database 출처 - https://github.com/donnemartin/system-design-primer#database ","date":"2021-05-03","objectID":"/database/:0:0","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"관계형 데이터베이스 관리 시스템 (RDBMS) SQL과 같은 관계형 데이터베이스는 테이블로 구성된 데이터 항목의 모음이다. ACID는 관계형 데이터 베이스 트랜잭션의 속성 집합니다. A(Atomicity) - Each transaction is all or nothing C(Consistency) - Any transaction will bring the database from one valid state to another I(Isolation) - Executing transactions concurrently has the same results as if the transactions were executed serially D(Durability) - Once a transaction has been committed, it will remain so 관계형 데이터베이스를 확장하는 기술은 마스터-슬레이브 복제, 마스터-마스터 복제, federation, sharding, denormalization, SQL tuning등의 많은 기술이 있다. ","date":"2021-05-03","objectID":"/database/:1:0","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Master-slave replication 마스터는 읽기 및 쓰기를 제공하고 읽기만 제공하는 하나이상의 슬레이브에 마스터에 쓰기를 한 데이터를 복제한다. 슬레이브 또한 트리구조로 다른 슬레이브에 복제할 수 있다. 마스터가 오프라인이 되더라도 시스템은 슬레이브가 마스터로 변경되거나, 새로운 마스터 서버가 프로비저닝 될 때 까지 읽기 전용으로 계속 작동 할 수 있다. master-slavemaster-slave \" master-slave 출처 - https://github.com/donnemartin/system-design-primer#database master-slave 구조의 단점 마스터의 서버가 오프라인 되었을 때 슬레이브가 마스터로 변경되는 로직이 추가적으로 필요하다. 또한 밑의 replication의 단점도 포함한다. ","date":"2021-05-03","objectID":"/database/:1:1","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Master-master replication 두 마스터 서버 모두 읽기 및 쓰기를 제공하고 쓰기에서 서로 조정한다. 두 마스터 서버중 한 서버가 다운되더라도 읽기 쓰기 모두 계속 작동 할 수 있다. master-mastermaster-master \" master-master 출처 - https://github.com/donnemartin/system-design-primer#database master-master 구조의 단점 두개의 마스터 서버가 존재하므로(두 서버가 쓰기 가능) 앞에 로드밸런서가 필요하거나 쓰기 작업을 분산하는 어플리케이션 로직을 추가해야한다. 대부분 master-master 구조는 (ACID 위반) 동기화로 인해 쓰기 지연 시간이 늘어난다. 더 많은 쓰기 노드가 추가되고 latency가 증가함에 따라 더 많은 충돌 해결이 필요할 수 있다. 밑의 replication의 단점도 포함한다. replication 단점 새로 쓰여진 데이터를 다른 노드에 복제하기 전에 마스터가 오류가 발생하거나 오프라인이 되면 데이터가 손실 될 수 있다. 쓰기는 읽기 전용 replica 서버에 복제되는데 쓰기가 많은 경우 읽기 전용 replica에 복제되는 데이터가 많아서 읽기 자체가 지연될 수 있다. 읽기 슬레이브가 많을수록 더 많이 복제해야하므로 복제 지연이 더 커진다. 일부 시스템에서 마스터에 쓰기는 병렬로 처리하기 위해 여러 쓰레드를 생성할 수 있지만 읽기 전용 replica 서버는 단일 쓰레드를 사용한 순차적 쓰기만 지원한다. replication은 하드웨어에 복잡성을 증가 시킨다. ","date":"2021-05-03","objectID":"/database/:1:2","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Federation FederationFederation \" Federation 출처 - https://github.com/donnemartin/system-design-primer#database federation은 기능별로 데이터베이스를 분할한다. 예를들어 모노리틱 데이터베이스 대신에 forums, users, products 라는 3개의 데이터베이스를 가져서 데이터베이스에 대한 읽기 및 쓰기 트래픽을 줄이게 되고 이는 복제 지연이 줄어 들게 된다. 데이터베이스가 작을 수록 메모리에 들어갈 수 있는 더 많은 데이터가 생성되고 결과적으로 향상된 캐시 locality로 인해 더 많은 캐시 적중이 발생한다. 또한 서비스별로 나누게 되면 병렬로 쓰기가 가능해져 처리량이 증가한다. federation의 단점 스키마에 거대한 함수나 테이블이 필요한경우 효과적이지 않다. 서비스별로 나눠져 있기 때문에 어플리케이션 로직을 추가하여 읽고 쓸 데이터베이스를 결정해야한다. 두 데이터베이스의 데이터를 결합하는 것은 복잡하다. federation은 더 많은 하드웨어가 필요하고 복잡성이 증가된댜. ","date":"2021-05-03","objectID":"/database/:1:3","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Sharding shardingsharding \" sharding 출처 - https://github.com/donnemartin/system-design-primer#database sharding은 각 데이터베이스가 데이터의 하위 집합만 관리 할 수 있도록 서로 다른 데이터베이스에 데이터를 분산한다. 사용자 데이터베이스를 예로 들면 사용자 수가 증가하면 더 많은 샤드가 클러스터에 추가된다. federation의 이점과 마찬가지로 sharding은 읽기 및 쓰기 트래픽을 줄이고 복제를 줄이고 캐시 적중을 증가시킨다. 인덱스 크기도 줄어들어 일반적으로 빠른 쿼리를 사용할 수 있고 이는 성능이 향상된다. 하나의 샤드가 다운되더라도 다른 샤드는 여전히 작동하지만 데이터 손실을 방지하기 위해 어떤 형태의 replication을 추가하는게 좋다. federation과 마찬가지로 쓰기를 직렬화하는 단일 중앙 마스터가 없으므로 처리량을 늘리면서 병렬적으로 쓸 수 있다. 사용자 테이블을 분할하는 일반적인 방법은 사용자의 last name initial을 사용하거나 사용자의 geographic location을 사용하는 것이다. sharding의 단점 샤드 작업을 위한 어플리케이션에 로직을 추가해야하므로 복잡한 쿼리가 발생할 수 있다. 데이터 분산은 한 샤드에 치우칠 수 있다. 예를들어 파워 유저 집합은 다른 샤드에 비해 해당 샤드에 대한 부하를 증가시킬 수 있다. rebalancing은 추가적인 복잡성을 만든다. 일관된 해싱을 기반으로하는 샤딩 기능은 전송되는 데이터의 양을 줄일 수 있다. 여러 샤드의 데이터 결합은 복잡하다. 샤딩은 하드웨어와 복잡성을 증가시킨다. ","date":"2021-05-03","objectID":"/database/:1:4","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Denormalization 비정규화는 일부 쓰기 성능을 희생하면서 읽기 성능을 향상 시키려고한다. 데이터의 중복 복사본은 expensive join을 피하기 위해 여러 테이블에 기록된다. PostgresSQL 및 Ocacle과 같은 일부 RDBMS는 중복 정보를 저장하고 중복 복사본의 일관성을 유지하는 작업을 처리하는 구체화된 뷰를 지원한다. federation 및 sharding과 같은 기술로 데이터가 분산되면 데이터 센터에서 조인을 관리할 때 복잡성이 더욱 증가한다. 비정규화는 이러한 복잡한 조인의 필요성을 피할 수 있다. 대부분 시스템에서 읽기대 쓰기 비율을 100 : 1 또는 1000 : 1보다 훨씬 많을 수 있다. 복잡한 데이터베이스 조인을 초래하는 읽기는 비용이 많이 들고 디스크 작업에 상당한 시간이 소요 될 수 있다. Denormalization의 단점 데이터가 중복된다. 복사본이 동기화 상태를 유지해야하므로 데이터베이스 설계가 복잡해진다. 쓰기 로드가 많은 비정규화된 데이터베이스는 정규화된 데이터베이스보다 성능이 떨어질 수 있따. ","date":"2021-05-03","objectID":"/database/:1:5","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"SQL tuning sql튜닝은 광범위한 주제이다. 병목현상을 시뮬레이션하고 발견하기 위해 벤치마킹하고 프로파일링 하는 것이 중요하다. 벤치마크 - ab와 같은 도구를 사용하여 고부하 상황을 시뮬레이션한다. 프로파일링 - 성능 문제를 추척하는데 도움이 되는 느린 쿼리 로그와 같은 도구를 활성화 한다. 벤치마킹 및 프로파일링은 다음과같은 최적화를 할 수 있다. MySQL dumps to disk in contiguous blocks for fast access. Use CHAR instead of VARCHAR for fixed-length fields. CHAR effectively allows for fast, random access, whereas with VARCHAR, you must find the end of a string before moving onto the next one. Use TEXT for large blocks of text such as blog posts. TEXT also allows for boolean searches. Using a TEXT field results in storing a pointer on disk that is used to locate the text block. Use INT for larger numbers up to 2^32 or 4 billion. Use DECIMAL for currency to avoid floating point representation errors. Avoid storing large BLOBS, store the location of where to get the object instead. VARCHAR(255) is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS. Set the NOT NULL constraint where applicable to improve search performance. 좋은 인덱스 쓰기 쿼리시 (SELECT, GROUP BY,ORDER BY, JOIN)은 더 빠를 수 있다. 인덱스는 일반적으로 데이터를 정렬하고 시간에 따른 검색, 순차 액세스, 삽입 및 삭제를 허용하는 self-balancing b-tree로 표시된다. 인덱스를 배치하면 데이터를 메모리에 보관할 수 있으므로 더 많은 공간이 필요하다. 인덱스도 업데이트해야하므로 쓰기 속도도 느려질 수 있다. 많은 양의 데이터를 로드하는 경우 인덱스를 비활성화 하고 데이터를 로드한다음 인덱스를 다시 작성하는 것이 더 빠를 수 있다. ","date":"2021-05-03","objectID":"/database/:1:6","tags":["system-design"],"title":"Database","uri":"/database/"},{"categories":["system-design"],"content":"Application Layer","date":"2021-03-02","objectID":"/applicationlayer/","tags":["system-design"],"title":"Application Layer","uri":"/applicationlayer/"},{"categories":["system-design"],"content":"application layerapplication layer \" application layer 사실 참고하고 있는 페이지에서 주제는 application layer라고는 하지만 내용은 microservice에 대한 내용이 강한데.. 일단 있으니.. application layer에서 web layer를 분리하면 두 계층을 독립적으로 확장하고 구성할 수 있다. 새 API를 추가하면 web server를 추가하지 않아도 application server가 추가된다. single responsibility principle은 함께 작동하는 소규모 및 자율 서비스를 advocates 한다. 작은 단위의 서비스를 갖추고 작은 팀은 빠른 성장을 위해 보다 적극적으로 계획 할 수 있다. 참고 https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server ","date":"2021-03-02","objectID":"/applicationlayer/:0:0","tags":["system-design"],"title":"Application Layer","uri":"/applicationlayer/"},{"categories":["system-design"],"content":"Https","date":"2021-02-27","objectID":"/https/","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"HTTP(Hypertext Transfer Protocal)는 Hypertext인 HTML을 전송하기 위한 통신 규약을 의미한다. HTTPS는 보안이 강화된 HTTP라는 것은 누구나 알고 있을 것이다. ","date":"2021-02-27","objectID":"/https/:0:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"SSL SSL은 Secure Socket Layer의 약자로 보안 소켓 레이어이다. Netscape사에서 웹서버와 웹브라우저 간의 보안을 위해 만들어졌고 공개키/개인키 대칭키 기반으로 사용한다. ","date":"2021-02-27","objectID":"/https/:1:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"SSL에서 사용하는 암호화의 종류 ","date":"2021-02-27","objectID":"/https/:2:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"대칭키 대칭키는 동일한 키로 암호화와 복호화를 같이 할 수 있는 방식의 암호화 기법을 의미한다. ","date":"2021-02-27","objectID":"/https/:2:1","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"공개키 대칭키 방식은 암호를 주고 받는 사람들 사이에 대칭키를 전달하는게 어렵다. 이 대칭키가 유출되면 암호의 내용을 복호화 할 수 있기 떄문이다. 이러한 단점 떄문에 나온게 공개키 방식이다. 공개키 방식은 2개의 키를 가진다. public key, private key라고 부른다. private key는 자신만이 가지고 있고 public key를 타인에게 제공한다. public key를 제공받는 사용자는 이 키를 이용해서 암호화된 정보를 복호화 한다. public key가 유출되더라도 private key를 모르면 정보를 복호화 할 수 없기 때문에 안전하다. (public key로는 암호화 밖에 못함) 이러한 방식으로 응용가능한게 있는데, private key의 소유자는 private key를 이용해 정보를 암호화 한 후 public key와 같이 암호화된 정보를 전송한다. 정보와 public key를 획득한 사람은 public key를 이용해 암호화 된 정보를 복호화 한다. 이 과정에서 public key가 유출되면 공격자에 의해 데이터가 복호화 될 위험이 있는데, 이것은 데이터를 보호하려는 목적이 아니라 암호화된 데이터를 public key를 가지고 복호화 할 수 있다는 것은 그 데이터가 public key와 쌍을 이루는 private key에 의해서 암호화 되었다는 것을 의미한다. (데이터를 전송해준 사람이 알맞은 private key를 가지고 있다고 인증 되는 것 즉 신원을 보장해주는 것) 이러한 것을 전자 서명이라고 부른다. ","date":"2021-02-27","objectID":"/https/:2:2","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"SSL 인증서 SSL 인증서는 클라이언트와 서버간의 통신을 제3자가 보증해주는 전자화된 문서다. 클라이언트가 서버에 접속한 직후에는 서버가 클라이언트에게 이 인증서 정보를 전달한다. 클라이언트가 접속한 서버가 신뢰 할 수 있는 서버임을 보장한다. SSL 통신에 사용할 공개키를 클라이언트에게 제공한다. ","date":"2021-02-27","objectID":"/https/:3:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"CA 인증서의 역할을 클라이언트가 접속한 서버가 클라이언트가 의도한 서버가 맞는지 보장하는 역할을 하는데, 이러한 역할을 해주는 기업이 CA(Certificate authority) 라고 부른다. SSL을 통해서 암호화된 통신을 제공하려는 서비스는 CA를 통해서 인증서를 구입해야한다. 물론 개인적으로 인증서를 만들어서 사용할 수도 있지만 브라우저에서 경고가 발생할 것이다. SSL 인증서에는 다음과 같은 정보가 포홤되어 있다. 서비스의 정보(인증서를 발급한 CA, 서비스의 도메인 등) 서버 측 공개키 (공개키의 내용, 공개키의 암호화 방법) CA의 리스트는 브라우저에서 알고 있기 때문에 이 CA리스트에 포함되어야지만 공인된 CA가 될 수 있다. ","date":"2021-02-27","objectID":"/https/:3:1","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"SSL 인증서가 서비스를 보증하는 방법 웹 브라우저가 서버에 접속할 때 서버는 제일 먼저 인증서를 제공한다. 브라우저는 이 인증서를 발급한 CA가 자신이 가지고 있는 list에 있는지 확인한다. 자신이 가지고 있는 CA 리스트에 포함되어 있다면 해당 CA의 공개키를 이용해서 인증서를 복호화 한다. 위에서 말했듯 CA의 공개키를 이용해서 인증서를 복호화 할 수 있다는 것은 이 인증서가 CA의 private key에 의해서 암호화 된 것을 인증해주는 것이다. 4번에서 인증해주는 것은 해당 서비스가 신뢰할 수 있다는 것이다. ","date":"2021-02-27","objectID":"/https/:4:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"SSL의 동작방법 SSL은 암호화된 데이터를 전송하기 위해서 공개키와 대칭키를 혼합해서 사용한다. 클라이언트와 서버가 주고 받는 실제 정보는 대칭키 방식으로 암호화 하고, 대칭키는 공개키 방식으로 암호화 해서 클라이언트와 서버가 주고 받는다. 공개키 방식의 암호화는 안전하지만 컴퓨터 자원을 많이 사용한다. 반면에 대칭키는 송신측과 수신층이 동일한 키를 공유해야하기 떄문에 보안상 문제가 발생할 수 있다. SSL은 이 두개의 장점을 혼합한 방법을 사용한다. ","date":"2021-02-27","objectID":"/https/:5:0","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"Hadnshake 브라우저와 서버도 handshake를 하는데 이 때 SSL 인증서를 주고 받는다. 클라이언트가 서버에 접속 (Client Hello) 클라이언트에서 생성한 랜덤 데이터 클라이언트가 지원하는 암호화 방식: 서버와 지원하는 암호화 방식에 클라이언트에서 사용하는 암호화 방식이 없을 수 있기 떄문에 서버측에 리스트를 제공한다. 세션 아이디: handshake 이후 세션을 이용하기 위해 세션 식별자를 서버로 전송 서버는 Client Hello에 대한 응답으로 Server Hello를 한다. 서버 측에서 생성한 랜덤 데이터 서버가 선택한 클라이언트의 암호화 방식: 클라이언트에서 전달해준 암호화 리스트를 확인해 서버쪽에서도 사용할 수 있는 암호화 방식을 선택해서 클라이언트로 전달 인증서 클라이언트는 서버의 인증서가 CA에 의해서 발급된 거인지 확인 하기 위해 브라우저에 내장된 CA리스트 확인, 없다면 경고 메세지출력. 브라우저의 CA의 공개키를 이용해서 인증서를 복호화 한다. 복호화에 성공했다면 인증서는 CA의 private key로 암호화 된 것이라고 보증된 것이기 때문에 서버를 신뢰할 수 있게 된다. 클라이언트는 1번과 2번에서 말했던 클라이언트 랜덤데이터와 서버 랜덤데이터를 조합해서 pre master secret이라는 키를 생성한다. 이 키는 뒤에 세션단계에서 데이터를 주고 받을 때 암호화를 하기 위해 사용될 것이다. 이 pre master secret key는 대칭키 이기 때문에 노출되서는 안된다. 이 생성한 pre master secret key는 아까 받은 서버의 public key로 암호화 해서 서버로 전송하면 서버에서는 private key로 복호화 할 수 있다. 여기서 사용할 public key는 서버로 부터 받은 인증서에 들어있다. 서버는 클라이언트가 전송한 pre master secret key 값을 private key로 복호화 한다. 이로써 클라이언트, 서버가 pre master secret key를 공유 할 수 있게 되었고 master secret 으로 변경한뒤 session key를 생성한다. 이 session key를 이용해 서버와 클라이언트는 대칭키방식으로 암호화 한 후에 데이터를 주고 받는다. handshake 종료 ","date":"2021-02-27","objectID":"/https/:5:1","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["system-design"],"content":"session 세션은 클라이언트와 서버가 데이터를 주고 받는데 위에서 생성한 session key를 이용해 대칭키 방식으로 암호화 하여 데이터를 전송한다. 서로 키를 가지고 있기 때문에 복호화도 서로 할 수 있다. 데이터의 전송이 끝나면 SSL 통신이 끝났음을 서로에게 알려주고 session key를 폐기 한다. 참고 https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server ","date":"2021-02-27","objectID":"/https/:5:2","tags":["http","ssl"],"title":"Https","uri":"/https/"},{"categories":["go"],"content":"Goroutines vs Threads","date":"2021-02-26","objectID":"/goroutinesvsthreads/","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"go routine과 thread를 비교해보자 ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:0:0","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"goroutine thread의 차이점 ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:1:0","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"메모리 소비 go routine은 생성하는데 많은 메모리가 필요하지 않다. 오직 2KB의 스택 공간만 필요하다. go routine을 할당하고 필요에 의해 힙 저장 공간을 확보하여 사용한다. 반면에 thread는 thread의 메모리와 다른 thread간의 보호 역할을 하는 Guard page라고 불리는 메모리 영역(1MB)과 함께 시작한다. 따라서 go routine은 작은 자원으로 생성할 수 있지만 Thread는 많이 생성하게 될 시 OutOfMemoryError가 발생할 수 있다. ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:1:1","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"생성 및 삭제 비용 thread는 생성과 삭제 비용을 가진다. thread는 OS로 부터 리소스를 요청해야 하고 작업이 끝나면 리소스를 돌려 줘야하기 때문에.. ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:1:2","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"Context Switching 비용 thread가 blocking된다면 다른 thread가 작업을 가져와서 처리해야한다. 여기서 thread가 변경시 스케줄러는 모든 레지스터들을 save / restore해야한다. 16정도의 레지스터가 있음.. 하지만 go routine은 스케줄링될 때 오직 3개의 레지스터만 save / restore 하면 된다. 개수만 봐도 비용이 훨씬 적게 든다. thread 보다 go routine을 많이 생성해서 그만큼 교체 비용이 발생할 수 있지 않냐라고 생각할 수 있겠지만 요즘 스케줄러들은 O(1)의 복잡도를 가진다고 한다. 따라서 교체시간이 go routine의 갯수에 영향을 받지 않다고 볼 수 있다. ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:1:3","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"런타임 스케줄러와 GMP 이제 차이점을 알았으니,, 어떻게 OS에서 제공하는 thread보다 더 적은 비용으로 go routine을 관리하는지 알아보자. go routine은 GMP라고 부르는 구조에 의해 관리된다. G(go routine) : go routine의 구현체이며, 런타임에서 go routine을 관리하기 위해서 사용한다. blocking syscall등으로 go routine이 blocking될 경우 런타임은 다음 go routine을 실행하는데 이 과정에서 blocking이 최소화 된다. M(Machine) : OS의 thread를 의미한다. P(Processor) : process 자원을 의미하며, go를 실행할 때 GOMAXPROCS로 개수를 설정할 수 있다. default값은 시스템 코어 갯수. 정확히는 스케줄링에 대한 context를 지니고 있다. ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:2:0","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"스케줄러 작동 원리 먼저 프로그램이 실행되면 P가 할당되고 각각의 P에는 실행할 G가 배치된다. 보통 사용하는 go func() 같은 형태로 호출하면 새로운 go routine이 P의 큐에 등록된다. syscall이 발생했을 때(blocking) 만약 실행중 syscall이 발생하게 되면 해당 syscall을 인터럽트 하고 P로 부터 thread를 분리해두고 syscall이 재개 될 때 해당 작업을 다시 삽업하는 형태로 효율적으로 처린된다. (재개 되기 전까지 다른 thread로 넘겨(hadn off) 모든 go routine이 정상적으로 작동할 수 있도록 보장한다.) 참고 이게 M에 P가 붙는 이유이다. 실행중인 M이 blocking되었을 때 다른 M으로 현재 상태를 그대로 넘겨야하기 때문에 P를 포함하고, 이는 GOMAXPROCS값이 1이더라도 go가 multi thread로 동작하는 이유이다. work-stealing scheduling M은 P로부터 G를 가져와 실행하지만 P에 G가 없는 경우 무작위로 다른 P의 G의 절반을 훔치려고 하는데 이런 스케줄링 전략을 work-stealing scheduling이라고 불린다. 이를 통해 하나의 P에 대기열이 몰려있는걸 완화한다. 따라서 각 thread들은 놀지 않고 많은 작업을 더 효율적으로 동시에 처리 할 수 있게 된다. ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:2:1","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["go"],"content":"결론 syscall의 빈도를 줄여 유저 스페이스에서 동작하도록 하여 context switching을 최소화한다. thread 생성과 삭제의 빈도를 줄이고 기존 thread 최대한 재활용하여 OS 스레드 생성에 대한 비용을 줄인다. blocking syscall등으로 go routine이 blocking될 경우 Go 런타임은 다음 go routine을 실행하는데 이 과정에서 블로킹이 최소화되어 동시성 프로그래밍의 효율이 증가한다. 참고 https://blog.nindalf.com/posts/how-goroutines-work/ https://d2.naver.com/helloworld/0814313 https://tech.ssut.me/goroutine-vs-threads/ ","date":"2021-02-26","objectID":"/goroutinesvsthreads/:3:0","tags":["go","thread"],"title":"Goroutines vs Threads","uri":"/goroutinesvsthreads/"},{"categories":["system-design"],"content":"Proxy","date":"2021-02-25","objectID":"/reverseproxy/","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"프록시에는 두 가지 종류가 있는데, 포워드 프록시와 리버스 프록시에 대해 알아보자. ","date":"2021-02-25","objectID":"/reverseproxy/:0:0","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"Forward Proxy 일반적으로 프록시라고 하면 포워드 프록시를 말하는데, 클라이언트가 서버로 요청할 때 직접 요청하지 않고 먼저 프록시 서버를 통해 요청하는 방식이다. 내부 인트라넷 -\u003e 인터넷 서버 요청시 또는 단말 -\u003e 인터넷 서버 내부에서 인터넷의 서버로 요청할 때 먼저 프록시 서버를 호출하게 되서 서버에게 클라이언트가 누구인지 감추는 역할을 해준다. 단말에서는 미리 프록시 설정을하여 목적지로 데이터를 전송하면 중간에 프록시를 통해서 전송하게 된다. 서버가 응답받은 IP는 당연 프록시 IP이므로 클라이언트가 누군지 알 수 없다. ","date":"2021-02-25","objectID":"/reverseproxy/:1:0","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"Reverse Proxy 리버스 프록시는 포워드 프록시와 반대로 생각하면 된다. 다른 서버의 정보를 프록시를 통해서 데이터를 받아오는 것.. 클라이언트가 서버를 호출할 때 리버스 프록시를 호출하게 되고 프록시 서버가 서버를 요청하여 받은 응답을 클라이언트에게 전달하는 방식 인터넷 -\u003e 내부 인트라넷 서버 리버스 프록시는 서버가 뭔지 감추는 역할을 해준다. 대표적으로 nginx, apache 웹 서버 등이 있다. ","date":"2021-02-25","objectID":"/reverseproxy/:2:0","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"Reverse Proxy additional benefits include 보안 강화 백엔드 서버 감추는 역할 블랙리스트 IP관리 클라이언트 연결 제한 확장성 및 유연성 향상 클라이언트는 리버스 프록시 IP나 도메인만 보고 있으면 되므로 내부적으로는 서버를 확장하거나 구성을 변경하기 편함. SSL termination 들어오는 요청을 암호화하여 백엔드 서버의 비용을 감소할 수 있음. 각 서버의 X509 인증서를 설치할 필요가 없다. 서버 응답 압축 캐시하여 요청에 대한 응답 반환 직접 정적 콘텐츠 제공 (html, photo 등등) ","date":"2021-02-25","objectID":"/reverseproxy/:2:1","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"Load balancer vs reverse proxy 로드 밸런서를 배포하는 것은 여러 서버가 있는 경우에 유용하다. 보통 같은 기능을 제공하는 서버를 세트로 묶어서 트래픽을 라우팅한다. 리버스 프록시는 웹 서버나 어플리케이션 서버가 하나 뿐인 경우에도 사용하기 좋다. nginx 같은 것들은 7계층 리버스 프록시 및 부하 분산을 지원한다. ","date":"2021-02-25","objectID":"/reverseproxy/:2:2","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"단점 리버스 프록시를 도입하면 복잡성이 증가.. 리버스 프록시를 단일 서버로 두면 단일 장애 지점으로 맛탱이 가면 곤란해진다. 장애 조치할 뭔가를 구성하면 복잡성이 더욱 증가한다. 참고 https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server ","date":"2021-02-25","objectID":"/reverseproxy/:2:3","tags":["system-design"],"title":"Proxy","uri":"/reverseproxy/"},{"categories":["system-design"],"content":"Load balancer","date":"2021-02-18","objectID":"/loadbalancer/","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"로드 밸런서는 클라이언트 요청을 애플리케이션 서버 및 데이터베이스와 같은 곳에 리소스를 분배한다. 로드 밸런서는 다음 3개에 대해서 효과적이다. Preventing requests from going to unhealthy servers Preventing overloading resources Helping to eliminate a single point of failure 로드밸런서는 하드웨어 또는 HAProxy와 같은 소프트웨어로 구성할 수 있다. 또 다른 기능으로는 SSL termination 들어오는 요청을 해독하고, 서버 응답을 암호화 하여 백엔드 서버가 cost가 많이 드는 작업을 수행 할 필요가 없다. 각 서버에 X.509 인증서를 설치할 필요가 없다. Session persistence Issue cookies and route a specific client’s requests to same instance if the web apps do not keep track of sessions 장애로 부터 보호하기 위해 active-passive or active-active 의 여러 로드밸런서를 설정하는게 일반적이다. 로드 밸런서는 다음과 같은 다양한 지표를 기반으로 트래픽을 라우팅 할 수 있다. Random Least loaded Session/cookies round robin or weighted round robin Layer 4 Layer 7 ","date":"2021-02-18","objectID":"/loadbalancer/:0:0","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"Layer 4 load balancing Layer 4 로드 밸런서는 transport layer에서 어떻게 요청을 분산할지 정한다. 일반적으로 여기서는 헤더의 소스, 대상 IP 주소 및 port가 포함되지만 패킷 내용은 포함 되지 않는다. Layer 4 로드 밸런서는 NAT를 수행하여 업스트림 서버와 네트워크 패킷을 전달한다. ","date":"2021-02-18","objectID":"/loadbalancer/:0:1","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"Layer 7 load balancing Layer 7 로드 밸런서는 application layer에서 어떻게 요청을 분산할지 정한다. 여기에서는 헤더, 메시지 및 쿠키의 내용이 포함 될 수 있다. 또한 네트워크 트래픽을 terminate 하고 메시지를 읽고 로드 밸런싱 결정을 내린 다음 선택한 서버에 연걸한다. 예를 들어 비디오를 호스팅하는 서버로 트래픽을 보낸다고 하면 민감한 사용자 청구 트래픽 같은 것을 보안이 강화된 서버로 보낼 수 있다. ","date":"2021-02-18","objectID":"/loadbalancer/:0:2","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"Horizontal scaling 로드 밸런서는 Horizontal scaling에 도움이 되어 성능과 가용성을 향상 시킬수 있다. 반대로 Vertical Scaling(수직 확장)은 단일 서버의 머신을 좋은 성능으로 올리는 것. 에 비해 비용이 효율적이고 가용성이 높다. 단점 수평적으로 확장하면 복잡성이 발생하고 서버 복제를 해야함. 서버의 상태를 저장하지 않아야함. 사용자 관련 데이터를 포함하면 안된다. 세션은 DB(SQL,NoSQL) 또는 Redis와 같은 중앙 집중식 데이터 저장소에 저장될 수 있다. 캐시 및 데이터베이스와 같은 다운 스트림 서버는 업스트림 서버가 확장됨에 따라 더 많은 동시 연결을 처리해아한다. ","date":"2021-02-18","objectID":"/loadbalancer/:0:3","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"로드 밸런서 단점 리소스가 충분하지 않거나 제대로 구성이 안되어 있으면 성능 병목 현상이 일어난다. Introducing a load balancer to help eliminate a single point of failure results in increased complexity. 단일 로드밸런서는 단일 장애지점이라서 여러 로드 밸런서를 구성하면 복잡성이 더욱 증가한다. 참고 https://github.com/donnemartin/system-design-primer#load-balancer ","date":"2021-02-18","objectID":"/loadbalancer/:0:4","tags":["system-design"],"title":"Load balancer","uri":"/loadbalancer/"},{"categories":["system-design"],"content":"Content delivery network","date":"2021-02-16","objectID":"/cdn/","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"CDN(content delivery network)은 전 세계적으로 분산 된 프록시 서버 네트워크로, 사용자와 가까운 위치에서 콘텐츠를 제공한다. ","date":"2021-02-16","objectID":"/cdn/:0:0","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"CDN 일반적으로 정적 파일을 제공한다. 예를 들어 HTML, CSS, JS, 사진 및 비디오 같은 것들. 하지만 AWS의 CloudFront와 같은 일부 CDN은 다이나믹 콘텐츠를 지원한다. DNS은 클라이언트에게 연결할 서버를 알려준다. CDN에서 콘텐츠를 제공하면 다음과 같은 이점이 있다. 사용자는 가까운 데이터 센터에서 콘텐츠를 받음. 서버는 CDN이 수행하는 요청을 처리 할 필요가 없음. 만약 1대의 CDN서버가 장애가 나도 다른 CDN서버로 재연결 하기 때문에 안정성 또한 보장 ","date":"2021-02-16","objectID":"/cdn/:1:0","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"Push CDNs 서버에서 변경사항이 발생할 때마다 새 콘텐트를 CDN이 수신한다. CDN에 업로드 및 CDN의 URL에 관리의 모든 책임은 사용자에게 있다. 콘텐츠가 만료되는 시기와 업데이트시기를 구성할 수 있다. 새로운 콘텐츠나 변경된 경우에만 업로드 되어 트래픽을 최소화 하지만 스토리지는 최대화 한다. 트래픽이 적은 사이트 또는 자주 업데이트 되지 않는 콘텐츠가 있는 사이트는 CDN에 콘텐츠 업로드가 잘된다. 콘텐츠를 정기적으로 가져오는 대신 한번만 가져온다. ","date":"2021-02-16","objectID":"/cdn/:1:1","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"Pull CDNs 처음 이용하는 사용자는 콘텐츠를 요청할 때 서버에서 새 콘텐츠를 가져온다. 콘텐츠를 서버에 남겨두고 CDN의 URL을 rewrite한다. 이로 인해 콘텐츠가 CDN에 캐시 될 때 까지 요청 속도가 느려진다. TTL(time to live)는 콘텐츠가 캐시되는 기간을 결정함. Pull 이벤트는 CDN의 저장공간을 최소화 하지만 파일이 실제로 변경되기 전에 파일이 만료되고 Pull되면 중복 트래픽을 생성 할 수 있다. 트래픽이 많은 사이트는 최근 요청된 콘텐츠만 CDN에 남아있는 상태에서 트래픽이 더 균등하게 분산되기 때문에 Pull하는 방식이 잘 맞다. ","date":"2021-02-16","objectID":"/cdn/:1:2","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"CDN의 단점 CDN 비용은 트래픽에 따라 많이 발생할 수 있지만 CDN을 사용하지 않아도 추가 비용이 나올 수 있다. TTL(time to live)이 만료되기 전에 업데이트 되면 콘텐츠가 오래 되었을 수 있다. 정적 콘텐츠가 CDN을 가리 키도록 URL을 변경해야 한다. ","date":"2021-02-16","objectID":"/cdn/:1:3","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"GSLB 이런 것들을 가능하게 하는 기술은 GSLB라고 한다. 간단하게는 물리적으로 가깝거나 여유 트래픽이 남아 있는 곳으로 접속을 유도하는 기술. GSLB(Global Server Load Balancing)은 이름에서 보이는 것 같이 로드 밸런서의 종류, 발전된 것이라고 생각할 수 있지만 DNS의 발전한 상태이다. DNS는 이전 글에서 봤듯이 도메인 이름을 IP주소로 변환하는 일을 하는 서비스이다. DNS는 클라이언트가 질의를 할 경우 IP 목록을 확인해서 그 중 하나만 반환한다. 네트워크 지연, 성능등은 전혀 고려하지 않는다. 이러한 문제를 해결한게 GSLB이다. ","date":"2021-02-16","objectID":"/cdn/:2:0","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"GSLB vs DNS 재해 복구 DNS는 서버의 상태를 알 수 없어서 서비스를 실패하는 유저가 생길 수 있다. GSLB는 서버의 상태를 모니터링 하고 싶래한 서버의 IP는 응답에서 제외 시키므로, 유저는 서비스를 이용할 수 있다. 로드 밸런싱 DNS는 Round Robin을 이용하여 정교한 로드밸런싱이 힘들다. GSLB는 서버의 로드를 모니터링하기 때문에 로드가 적은 서버의 IP를 반환하여 정교하게 로드밸런싱할 수 있다. latency 기반 서비스 DNS는 Round Robin방식이므로 네트워크상에서 멀리 떨어진 위치의 서버로 연결 할 수 있다. GSLB는 지역별로 latency를 가지고 있기 때문에 유저가 접근하면 낮은 latency를 가지고 있는 서버로 연결한다. 위치 기반 서비스 DNS에서 유저는 역시 ROund Robin하게 서버 연결 GSLB는 유저의 지역을 기반으로 해당 지역을 서비스하는 서버로 연결할 수 있다. 참고 https://goddaehee.tistory.com/173 https://github.com/donnemartin/system-design-primer ","date":"2021-02-16","objectID":"/cdn/:2:1","tags":["system-design"],"title":"Content delivery network","uri":"/cdn/"},{"categories":["system-design"],"content":"Domain name system","date":"2021-01-13","objectID":"/doamin/","tags":["system-design"],"title":"Domain name system","uri":"/doamin/"},{"categories":["system-design"],"content":"DNS (Domain Name System)은 www.example.com과 같은 도메인 이름을 IP 주소로 변환한다. DNS는 계층적이다. top level에 권한 있는 서버가 있다. 라우터 또는 ISP는 lookup을 할 때 연결할 DNS 서버에 대한 정보를 제공한다. lower level DNS서버는 전파 지연으로 인해 문제가 생기는걸 방지해 매핑을 캐싱한다. DNS결과는 TTL(Time to Live)에 의해 결정된 특정 기간 동안 브라우저 또는 OS에 의해 캐싱될 수 있다. NS 레코드 (name server) domain/subdomain에 대한 DNS 서버를 지정한다. MX 레코드 (maiil exchange) 메시지를 수락하기위한 메일 서버를 지정한다. 레코드 (address) 이름이 IP주소를 가리킨다. CNAME (표준[canocinal]) 이름을 다른 이름이나 CNAME(example.com에서 www.example.com으로) A 레코드를 가리킨다. CloudFlare 및 AWS Route 53과 같은 서비스는 관리형 DNS 서비스를 제공한다. 참고 Route 53 및 DNS의 자세한 내용은 https://jaejin1.github.io/route53/ 참고 일부 DNS서비스는 다양한 알고리즘으로 트래픽을 라우팅 할 수있다. Weighted round robin Prevent traffic from going to servers under maintenance Balance between varying cluster sizes A/B testing Latency-based Geolocation-based DNS 단점 DNS서버에 액세스 하면 위에서 말한 것과 같이 캐싱을함에도 불구하고 약간의 지연이 발생 할 수 있다. DNS서버 관리는 복잡 할 수 있다. 일반적으로 정부, ISP 및 대기업에서 관리 DNS서비스는 최근 DDoS공격을 받아 사용자가 Twitter의 IP주소를 알지 못하는 상태에서 Twitter와 같은 웹 사이트에 액세스하지 못하게 되었다. ","date":"2021-01-13","objectID":"/doamin/:0:0","tags":["system-design"],"title":"Domain name system","uri":"/doamin/"},{"categories":["system-design"],"content":"Availability vs consistency","date":"2021-01-12","objectID":"/cap/","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"일관성과 가용성에 대해서 알아보자. ","date":"2021-01-12","objectID":"/cap/:0:0","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"availability vs consistency ","date":"2021-01-12","objectID":"/cap/:1:0","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"CAP theorem 분산 컴퓨터 시스템에서는 다음 중 2가지만 지원할 수 있다. Consistency 일관성 모든 노드들은 같은 시간에 동일한 항목에 대하여 같은 내용의 데이터를 사용자에게 보여준다. 모든 요청은 최신 데이터 또는 에러를 응답받는다. Every read receives the most recent write or an error Availability 가용성 모든 사용자들이 읽기 및 쓰기가 가능해야 하며, 몇몇 노드의 장애 시에도 다른 노드에 영향을 미치면 안된다. 모든 요청은 정상 응답을 받는다. Every request receives a response, without guarantee that it contains the most recent version of the information Partition Tolerance 분할내성 노드간에 메시지 전달이 실패하거나 시스템 일부가 망가져도 시스템이 계속 동작할 수 있어야 한다. The system continues to operate despite arbitrary partitioning due to network failures 하지만 네트워크는 신뢰할 수 없으므로 Partition Tolerance를 지원해야한다. 따라서 Consistency과 Availability 사이에 소프트웨어 절충안을 만들어야 한다. CP - consistency and partition tolerance 파티션된 노드의 응답을 기다리면 timeout 에러가 발생할 수 있다. CP는 비즈니스 요구에 아주 작은 단위적으로 읽기 및 쓰기가 필요할 경우 좋은 선택이다. AP - availability and partition tolerance response로 모든 노드에서 가장 쉽게 사용 가능한 데이터를 반환하며 이는 최신 버전의 데이터가 아닐 수 있다. 만약 쓰기를 하고 모든 노드에 전파되는데 시간이 걸릴 수 있다. AP는 비즈니스에서 궁극적인 Consistency를 허용? 하거나 외부 오류에도 불구하고 시스템이 계속 작동해야 하는 경우 좋은 선택이다. ","date":"2021-01-12","objectID":"/cap/:1:1","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Consistency patterns 동일한 데이터의 multiple copies를 사용하면 클라이언트가 데이터를 일관되게 볼 수 있도록 동기화 하는 방법에 대한 옵션이 있다. CAP theorem에서 consistency 정의는 최신 데이터 또는 에러를 응답받는다는 것을 기억하자. ","date":"2021-01-12","objectID":"/cap/:2:0","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Weak consistency 쓰기 후 읽으면 데이터를 볼 수도 있고 못볼 수도 있다. 이것에 대한 최선의 접근 방식은 memcached(오픈 소스인 분산 메모리 캐싱 시스템)와 같은 시스템에서 볼 수 있다. memcached를 사용하면 memcached로 묶인 모든 서버는 동일한 가상 메모리 풀을 공유해서, 전체 클러스터에 대해서 동일한 위치에 저장되고 검색되어 진다. 또 VoIP, 화상채팅 및 실시간 멀티 플레이어 게임과 같은 실시간을 필요로 하는 서비스에서 잘 작동한다. 예를들어 전화 통화시 몇 초 동안 수신이 끊긴 경우 다시 연결되면 연결이 끊겼을 때 말한 내용을 들을 수 없는 것처럼.. ","date":"2021-01-12","objectID":"/cap/:2:1","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Eventual consistency 쓰기 작업의 대한 결과는 언젠가 확인 할 수 있다. 쓰기 작업을 하게 되면 읽기는 결국 보게 된다. (일반적으로 milliseconds 이내에) 데이터는 비동기적으로 복제된다. 이런 접근은 DNS 및 Email과 같은 시스템에서 볼 수 있다. Eventual consistency는 고 가용성 시스템에서 잘 작동한다. ","date":"2021-01-12","objectID":"/cap/:2:2","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Strong consistency 쓰기 작업 이후 데이터를 본다. 쓰기 작업 이후에는 읽기할 때 볼 수 있다. 데이터는 동기식으로 복제 이 접근은 RDBMS에서 볼 수 있다. Strong consistency는 Transaction이 필요한 시스템에서 잘 작동한다. ","date":"2021-01-12","objectID":"/cap/:2:3","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Availability patterns CAP theorem에서 Availability의 정의는 몇몇 노드의 장애 시에도 다른 노드에 영향을 미치면 안되고 모든 요청은 정상 응답을 받는다는 것을 다시 기억하고. ","date":"2021-01-12","objectID":"/cap/:3:0","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Fail-over (장애 조치) Active-passive active passive 장애 조치를 사용하면, Active 서버와 Passive 서버간에 하트비트가 전송된다. 하트 비트가 중단되면 Passive 서버가 Active 서버의 IP를 인수하고 서비스를 다시 시작한다. downtime의 길이는 Passive 서버가 이미 ‘hot’ 대기에서 실행 중인지 ‘cold’ 대기에서 시작하는지 여부에 따라 결정된다. 오직 Active 서버만 트래픽을 처리한다. 이는 master-slave 장애 조치라고도 한다. Active-active active-active 에서는 두 서버가 트래픽을 관리하여 부하를 분산 시킨다. 서버가 public facing인 경우 DNS는 public ip에 대해 알고 있어야한다. 만약 internal facing인 경우 어플리케이션 login은 두 서버에 대해 알고 있어야한다. active-active 장애 조치는 master-master 장애 조치라고도 한다. Disadvantage(s): failover Fail over는 많은 하드웨어와 복잡성이 있다. 활성중인 시스템이 장애가 발생하면, 새로운 시스템이 띄워질 때 데이터를 복제하는 시간이 발생하므로 그때 데이터가 손실 될 가능성이 있다. ","date":"2021-01-12","objectID":"/cap/:3:1","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Replication Master-slave replication master-master replication ","date":"2021-01-12","objectID":"/cap/:3:2","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":["system-design"],"content":"Availability in numbers Availability in parallel vs in sequence 서비스가 실패하기 쉬운 여러 conponent로 되어 있는 경우, 서비스의 전체 availability는 component 가 sequence 한지 parallel한지에 따라 달라진다. In sequence availability \u003c 100%인 두 component가 배치되면 전체 availability가 감소한다. Availability (Total) = Availability (Foo) * Availability (Bar) Foo와 Bar의 Availability가 각각 99.9% 인경우 총 Availability는 99.8%가 된다. In parallel availability \u003c 100% 인 두 component가 병렬로 연결 되면 전체 Availability는 증가한다. Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar)) Foo와 Bar의 Availability이 각각 99.%인 경우 총 Availability는 99.9999%가 된다. 참고 https://github.com/donnemartin/system-design-primer#availability-in-numbers ","date":"2021-01-12","objectID":"/cap/:3:3","tags":["system-design"],"title":"Availability vs consistency","uri":"/cap/"},{"categories":null,"content":"developer 👨‍💻 updated: Jan 09, 2021 ","date":"2021-01-09","objectID":"/about/:0:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Work History 🔥 Cloud Engineer @ DreamusCompany(FLO) 2020.2.17 ~ Present Junior Software Developer @ ICONLOOP 2018.10 ~ 2020.2.14 Intern @ ICONLOOP 2018.07 ~ 2018.09 Undergraduate Research Assistant @ UNClab 2016.06 ~ 2018.06 ","date":"2021-01-09","objectID":"/about/:1:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Education 🎓 2013-2019 Hongik University Computer Information Communications Engineering ","date":"2021-01-09","objectID":"/about/:2:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":null,"content":"Links 🔗 Github https://www.github.com/jaejin1 Blog https://jaejin1.github.io/ Linkedin https://www.linkedin.com/in/jaejin-lee-425161173/ Thanks for reading! 👋 ","date":"2021-01-09","objectID":"/about/:3:0","tags":null,"title":"About Jaejin","uri":"/about/"},{"categories":["os","linux"],"content":"파티션이란 연속된 저장 공간을 하나 이상의 연속되고 독립된 영역으로 나누어서 사용할 수 있도록 정의한 규약.","date":"2020-06-28","objectID":"/partition/","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파티션이란 연속된 저장 공간을 하나 이상의 연속되고 독립된 영역으로 나누어서 사용할 수 있도록 정의한 규약. partitionpartition \" partition 파티션을 나누기 위해서는 저장장치에 연속된 공간에 있어야한다. 하나의 하드디스크에는 여러 개의 파티션을 나눌 수 있지만, 두 개의 하드디스크를 가지고 하나의 파티션을 만들 순 없다. ","date":"2020-06-28","objectID":"/partition/:0:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"사용 용도 하나의 물리적인 디스크를 여러 논리 영역으로 나누어 관리를 용이하게 함. OS영역과 Data 영역으로 나누어 OS 영역만 따로 포맷 및 관리를 하기 위해 사용 여러 OS를 설치하기 위해 사용 하드 디스크의 물리적인 배드 섹터로 특정 영역을 잘라서 사용하기 위해 사용한다. ","date":"2020-06-28","objectID":"/partition/:1:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"partition vs volume 볼륨은 OS나 Application등에서 이용할 수 있는 저장공간, 즉 섹터의 집합이다. 연속된 공간이 아니여도 볼륨으로 볼 수 있다. 즉 2개의 하드디스크를 사용하는 경우 하나의 하드디스크처럼 인식하여 사용할 수 있다. 보통 partition에 FS를 설정해주면 volume으로 보기에 partition역시 volume으로 볼 수 있다. hard diskhard disk \" hard disk ","date":"2020-06-28","objectID":"/partition/:2:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"MBR Master Boot Record 각 boot record는 각 partition의 첫 번째 섹터에 위치하며, 주로 해당 partition의 설치된 OS를 부팅하는 역할을 하게 된다. 즉 OS 실행을 위한 boot loader를 호출하는 것이다. partition을 나누지 않은 경우라면 boot record는 MBR에 있을 것이다. 단일 partition을 사용하는 경우 (플로피) boot record는 1개만 있을 것이므로 MBR이 필요없다. MBRMBR \" MBR MBR 호출과정 MBR 호출과정MBR 호출과정 \" MBR 호출과정 ","date":"2020-06-28","objectID":"/partition/:3:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"linux partition Primary partition 주 영역 파티션 4개까지 생성가능 (1개 ~ 4개) Extend partition 확장 영역 파티션 1개 까지 생성가능 Ligical partition 논리 영역 파티션 SCSI 한 개의 총 partition 15개만 넘지 않게 사용가능 12개 이상은 좋지 않다고함 ","date":"2020-06-28","objectID":"/partition/:4:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"실습 ","date":"2020-06-28","objectID":"/partition/:5:0","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파티션 나누기 aws에 ec2하나 띄워놓고 파티션을 나눠보장 먼저 파티션할 volume을 하나 생성하고 해당 ec2와 연결한다. 16GB짜리 disk를 연결했다. diskdisk \" disk $ fdisk -l fdisk -lfdisk -l \" fdisk -l 파티션 생성을 위해 파티션 설정으로 들어가자. $ fdisk /dev/xvdd partition 설정partition 설정 \" partition 설정 Primary 2GB Extended 4GB Logical 2GB Logical 2GB 사용안함 10GB 로 설정해보자. 먼저 2G 짜리 Primary 파티션을 만들기 위해 n 입력 partition 설정partition 설정 \" partition 설정 다음 primary 파티션을 생성해야하므로 p를 입력 순서대로 partition 번호는 1, sector 시작은 default, primary를 2GB로 설정하기위해서 last sector는 +2G 로 설정했다. partition 설정partition 설정 \" partition 설정 설정이 완료되었고 다시 p를 입력해 확인해보면 생성된 것을 확인 할 수 있다. partition 설정partition 설정 \" partition 설정 primary를 생성 했으므로 extended 파티션을 생성해보자. 똑같이 n 명령으로 생성하자. partition 설정partition 설정 \" partition 설정 xvdd2 이름으로 4GB Extended 파티션이 생성된 것을 확인 할 수 있다. 이제 Extended 파티션에 2개의 logical 파티션을 생성해보자. Extended 파티션을 생성하고 n 명령어를 입력하면 extended 대신 logical이 보일 것이다. partition 설정partition 설정 \" partition 설정 이제 2개의 logical 파티션을 생성하자 partition 설정partition 설정 \" partition 설정 2개째 설정하는 단계에서 Value out of range 에러가 발생했는데 Extended 파티션을 4GB로 설정했지만 완벽히 4GB가 아닐꺼라 남은 용량이 2GB보다 적어서 발생하는 에러 일 것이므로 default로 설정해주었더니 잘 할달 받은 것을 볼 수 있다. partition 설정partition 설정 \" partition 설정 w 명령으로 저장한다. partition 설정partition 설정 \" partition 설정 혹시 모르니 partprobe 명령으로 os에게 변경사항을 알려준다. $ partprobe 다시 확인하면 잘 나눠진것을 볼 수 있다. partition 설정partition 설정 \" partition 설정 ","date":"2020-06-28","objectID":"/partition/:5:1","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"파일 시스템 설정 파티션 생성을 완료하면 물리적인 공간만 나눠놓았고 파일시스템을 지정해줘야한다. ext4 파일 시스템으로 /dev/xvdd1에 적용한다. $ mkfs.ext4 /dev/xvdd1 fs 설정fs 설정 \" fs 설정 xvdd5, xvdd6도 설정해주자. xvdd5는 ext4로 , xvdd6은 ext3으로 설정해봤다. fs 설정fs 설정 \" fs 설정 ","date":"2020-06-28","objectID":"/partition/:5:2","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"마운트 파일시스템 설정까지 해줬으니 마운트를 해줘서 파티션한 공간을 사용해보자. /dev/xvdd1, /dev/xvdd5, /dev/xvdd6 을 연결하기 위해 3개의 마운트 포인트가 필요하다. mount 설정mount 설정 \" mount 설정 mount 설정mount 설정 \" mount 설정 마운트가 잘 된것을 볼 수 있고 user를 할당해서 home 디렉토리로 설정해보장 user 설정user 설정 \" user 설정 $ tail /etc/passwd user 설정user 설정 \" user 설정 mount 확인mount 확인 \" mount 확인 ","date":"2020-06-28","objectID":"/partition/:5:3","tags":["os","linux","filesystem"],"title":"Partition","uri":"/partition/"},{"categories":["os","linux"],"content":"sector란 물리 디스크에 입출력을 요청하는 최소 단위이다.","date":"2020-06-28","objectID":"/sector/","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"sector란 물리 디스크에 입출력을 요청하는 최소 단위이다. 전통적인 하드 디스크는 512 Byte 크기의 sector들로 구성 되어 있다고 한다. 큰 데이터에 접근 하고자 할때 sector의 크기가 작으면 많은 입출력 요청이 발생해서 디스크 에서는 퍼포먼스를 올리기 위해 sector 자체 크기를 늘리고자 했다. 파일의 크기가 작아 하나의 sector를 채우지 못하는 파일들은 0으로 나머지 sector 부분을 보유하고 있다. 또한 한 sector에 동시에 2 종류의 정보는 들어 갈 수 없다. 2011년 1월에 모든 하드 디스크 제조 업체가 sector의 크기를 4096 KByte를 표준으로 하는 것에 합의 했다는데 밑의 사진만 봐도 512 bytes로 나온다. sector sizesector size \" sector size 그 이유가 물리적으로는 4096 Byte인 것으로 입출력을 하지만 하드 디스크에 내장된 컨트롤러가 논리적으로 sector의 크기가 512Byte인 것과 같이 advanced format의 디스크가 나왔다고 한다. 실제 물리 디스크에 입출력을 요청하는 단위를 물리 sector 라 부르고 4096 Byte, 디스크 상단에 에뮬레이션 되는 단위를 논리 sector 라 하고 512 Byte이다. ","date":"2020-06-28","objectID":"/sector/:0:0","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"디스크 구조 ","date":"2020-06-28","objectID":"/sector/:1:0","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"LBA방식 현재의 하드 디스크는 LBA방식으로 접근하는 것이 보통이다. 이것은 하드 디스크의 전체 sector에 0부터 sector번호를 붙여서 이 번호를 이용해 접근하는 방식이다. sector번호와 물리적 sector위치를 짝 지우는 일은 하드 디스크 컨트롤러에 내장된 펌웨어가 담당한다. sectorsector \" sector partition 포스트에서 나누었던 partition을 그대로 가져왔다. 각 파티션을 보면 시작점과 종료점이 sector 번호로 표시된다. /dev/xvdd1 은 2048 sector 부터 시작해 0~1047 sector는 파티션으로 사용되지 않음을 볼 수 있다. 디스크의 처음에 MBR이 있고, 그 뒤에 GRUB의 스테이지 1.5가 기록된다고 한다. ","date":"2020-06-28","objectID":"/sector/:1:1","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"4KB sector disk 아까 위에서 기존의 하드 디스크는 1sector의 사이즈가 512Byte로 정해져 있다고 했다. 또, 물리적으로는 4KB씩 읽고쓰는데 논리적으로 512Byte로 에뮬레이션 하는 방식으로 되어 있다고도 했다. 예로 /dev/xvdd1이 2048 sector부터 시작한다면 256번째 4KB sector 시점과 일치한다. 파티션의 sector 수도 8의 배수이므로 파티션의 종료 위치도 딱 4KB sector의 마지막 지점이 된다. 장래에 4KB sector를 채용한 하드가 늘어날 것을 고려해 기본적으로 파티션의 시작 지점과 사이즈는 논리 sector 수로 생각한 8의 배수로 준비하는게 좋다. 이 작업을 파티션 정렬(alignment) 라고 한다. ","date":"2020-06-28","objectID":"/sector/:1:2","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"파티션 정렬 파티션 정렬의 설명을 좀 더 추가하자면 사진의 0~62 sector들이 있고 그 뒤에 63번 sector부터 파티션이 시작되는 것을 볼 수 있는데, 논리적으로 봤을때는 데이터가 잘 위치한 것으로 보이지만 물리적 섹터의 경우 경계에 맞물려 데이터가 담긴 블록이 위치하게 된다. naver cloudnaver cloud \" naver cloud sector크기가 4096 Byte일 때 512 Byte 크기의 sector크기로 에뮬레이션 하나 실제로 4096 Byte 크기의 sector단위로 입출력 되기 때문에 물리 sector의 8번, 9번 sector에 각각 접근해야한다. naver cloudnaver cloud \" naver cloud 다음과 같이 8번 물리 sector의 남은 512Byte 만큼을 indent 한다면 그림과 같다. 1개의 논리 sector만큼 indent하여 물리 섹터의 경계에 정렬한다. 따라서 9번 물리 sector에만 입출력을 요청하면 된다. 이렇게 물리 sector 경계에 맞춰주는 것이 파티션의 정렬이다. 즉 파티션 교체 할때 파티션 정렬을 안하면 디스크 I/O 지연이 발생하겠다. ","date":"2020-06-28","objectID":"/sector/:2:0","tags":["os","linux","sector"],"title":"Sector","uri":"/sector/"},{"categories":["os","linux"],"content":"FS는 `저장장치 내에서 데이터를 읽고 쓰기 위해 미리 정해진 약속` 이라고 볼 수 있다.","date":"2020-06-21","objectID":"/filesystem/","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"FS는 저장장치 내에서 데이터를 읽고 쓰기 위해 미리 정해진 약속 이라고 볼 수 있다. 또한 컴퓨터에서 파일이나 자료를 쉽게 발견 및 접근할 수 있도록 보관 또는 조직하는 체제를 가리킨다. ","date":"2020-06-21","objectID":"/filesystem/:0:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"소개 파일 시스템은 일반적으로 크기가 일정한 블록들의 배열에 접근할 수 있는 자료 보관 장치 위에 생성되어 이러한 배열들을 조직함으로 파일이나 디렉터리를 만들며 어느 부분이 파일이고 어느 부분이 공백인지 구분하기 위하여 각 배열에 표시를 해둔다. 자료를 클러스터 또는 블록 이라고 불리는 일정한 단위에 새겨 넣는데 이것이 바로 파일 하나가 필요로 하는 디스크의 최소 공간이다. ","date":"2020-06-21","objectID":"/filesystem/:1:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"분류 일반적인 파일 시스템 일반적인 FS는 하드디스크와 같은 저장장치에서 주로 사용된다. 그 종류를 간단히 보면 다음과 같다. FAT (File Allocation Table) FAT은 어느 영역에 파일이 속해 있는지, 공간에 여유가 있는지, 또 어디에 각 파일이 디스크에 저장되어 있는지에 대한 정보를 중심으로 하는 테이블을 이용하는 것에서 비롯한다. 상대적으로 간단한 파일 시스템. 성능은 상대적으로 다른 파일 시스템보다 좋지 않다. 너무나도 단순한 자료구조를 사용하고 조그만 파일이 많으면 공간 활용률이 적어지기 때문이다. FAT12, FAT16, FAT32, exFAT등 종류가 더 있다. HPFS (High Performance FileSystem) NTFS (New Technology FileSystem) FAT32의 약점을 보완하기 위해 개발된 FS 드라이브 최대용량 256TB 파일 하나당 저장할 수 있는 최대 크기 16TB 윈도우에서는 최적화 되어 있지만, MAC, android, linux와 같은 기기는 제한되어있음. UFS (Unix FileSystem) ext ext, ext2, ext3, ext4의 종류들이 있다. 리눅스용 파일 시스템 가운데 하나 로 오늘날 많은 리눅스 배포판에서 주 파일 시스템으로 쓰이고 있다. APFS 애플 파일 시스템은 애플에서 macOS, iOS, watchOS, tvOS등에서 범용으로 사용하고자 만든 FS Flash FileSystem Network FileSystem 네트워크 파일시스템은 원격에 위치한 파일시스템을 로컬 파일시스템처럼 이용할 수 있도록 개발한 프로토콜이다. 단순히 파일 공유가 아니라 NFS도 파일 시스템임을 인지 해야 하기 때문에 원격 파일시스템이 mount되면 mount 지점 아래 위치한 파일에 접근을 하는 경우 NFS가 파일시스템 레벨에서 system call을 받아 직접 네트워크 파일을 수신해 쓰거나 실행할 수 있도록 한다. Virtual FileSystem OS차원에서 가상 파일시스템이라는 상위 레벨의 파일시스템 인터페이스가 존재하기 때문에 응용프로그램에서는 아무 구분 없이 OS의 system call을 호출하면 커널은 미리 등록 되어 있는 파일시스템 함수를 호출해 같은 경과를 얻을 수 있다. ","date":"2020-06-21","objectID":"/filesystem/:2:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"요소 ","date":"2020-06-21","objectID":"/filesystem/:3:0","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"클러스터 클러스터는 파일을 저장하는 단위로 1개 또는 복수의 섹터로 이루어진다. 여러 개의 클러스터를 사용할 때 반드시 연결되어 있지 않다. 여기저기 흩어져 있어도, 그 위치와 순서를 기록한 FAT등에 의하여 관리되므로 한 번에 파일의 전체 내용을 읽을 수 있다. OS가 파일시스템 생성 시 저장장치의 크기를 고려해 클러스터의 크기를 조절한다. 저장장치의 크기 및 사용 용도에 따라 달라져야한다. OS에 의해 데이터를 읽고 쓰는 과정에서 파일시스템은 미리 정해져 있는 클러스터의 크기를 기본단위로 하여 읽고 쓰는 과정에서 파일시스템은 미리 정해져있는 클러스터의 크기를 기본단위로 하여 입출력을 하게 된다. 클러스터의 크기가 4096Byte라면 1Byte를 읽더라도 4096Byte를 읽어야한다. processprocess \" process 크기가 작은 파일을 저장할 경우 낭비되는 영역이 생기는데 이 부분의 공간은 사용이 불가능 해진다. 낭비되어도 성능적인 측면에서 I/O의 비용이 커서 요즘의 대용량 하드라면 성능을 위해 무시할 정도다 ","date":"2020-06-21","objectID":"/filesystem/:3:1","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["os","linux"],"content":"파일 파일시스템은 결국 파일을 기록하기 위한 것이므로 파일을 이루는 구조와 관리할 수 있는 추가적인 방법을 제시한다. 파일은 속성을 기록하는 메타 데이터, 실제 데이터를 기록하는 데이터 영역으로 나뉜다. 파일 정보 요청 프로세스 파일정보요청 → Meta Data 로 파일 경로 요청을 보낸다. Meta Data → 요청 파일 경로 안내를 해준다. Meta Data로 부터 받은 경로로 실제 파일로 접근한다. 참고 https://blog.naver.com/bitnang/70183421214 ","date":"2020-06-21","objectID":"/filesystem/:3:2","tags":["os","linux","filesystem"],"title":"FileSystem","uri":"/filesystem/"},{"categories":["aws"],"content":"공격의 표면, Bastion을 관리 하는 운영상의 부담 및 발생하는 추가 비용을 더욱 줄이기 위해, AWS Systems Manager Session Manager를 사용하여, Bastion을 운영하거나 조작 할 필요없이 EC2 인스턴스에 안전하게 연결 할 수 있다. Bastion실행에 수반되는 불편함과 인스턴스의 inbound SSH port 개방으로 유발되는 위험을 피하고자 한다. 보안 액세스 수동으로 인스턴스에 사용자 계정, 암호 또는 SSH를 설정할 필요가 없으며 inbound port를 개발하지 않아도 된다. Session Manager는 SSM 에이전트를 사용하여 인스턴스에서 시작한 암호화된 채널을 통해 인스턴스와 통신하며 Bastion이 필요하지 않다. 액세스 제어 IAM 정책과 사용자를 사용하여 인스턴스에 대한 액세스를 제어하며 SSH key를 배포하지 않아도 된다. IAM의 날짜 조건 연산자를 사용하면 원하는 시간/유지 관리 기간으로 액세스를 제한 할 수 있다. 감사 용이성 명령과 응답은 Amazon CloudWatch 및 S3 버킷에 로깅 할 수 있다. 또한 세션이 시작될 때 SNS 알림을 수신할 수 있도록 설정할 수 있다. ","date":"2020-04-02","objectID":"/sessionmanager/:0:0","tags":["cloud","aws"],"title":"AWS Session Manager","uri":"/sessionmanager/"},{"categories":["aws"],"content":"Session Manager의 작동방식 ","date":"2020-04-02","objectID":"/sessionmanager/:1:0","tags":["cloud","aws"],"title":"AWS Session Manager","uri":"/sessionmanager/"},{"categories":["aws"],"content":"Session Manager 사용을 위한 Role Session Manager를 사용하여 EC2 인스턴스에 액세스 하려면 인스턴스에 SSM 에이전트가 실행 중이어야 한다. 또한 인스턴스에는 AmazonEC2RoleForSSM 정책을 부여해 주어야 한다. ","date":"2020-04-02","objectID":"/sessionmanager/:1:1","tags":["cloud","aws"],"title":"AWS Session Manager","uri":"/sessionmanager/"},{"categories":["aws"],"content":"SSM Role을 갖는 EC2 인스턴스를 Public Subnet, Private Subnet에 설치 기본적으로 Session Manager의 기능을 사용하기 위해서는 EC2 인스턴스가 인터넷을 통해 접속이 가능하여야 한다. (SSM 에이전트는 Session Manager의 public Endpoint에 연결할 수 있어야 하므로) 따라서 Public Sebnet에 설치하거나, Private Subnet에 설치할 경우 PrivateLink 연결을 설정해서 사용할 수 있다. Public Subnet에 설치할 경우 Instance에 공인 IP를 부여 받고, SSM 에이전트를 설치 하고, IAM정책만 부여해주면 System Manager에 Session Manager메뉴에 해당 EC2 인스턴스를 확인 할 수 있을 것이다. Private Subnet에 설치할 경우 Public과 동일하지만 인스턴스가 인터넷에 접속 가능해야 하기 때문에 PrivateLink 설정을 하고 Public Subnet 구성과 같이 하면 된다. Info Notice: VPC End pont를 생성할시 Service Name에서 다음 4가지를 추가 해줘야 한다. 1. \"com.amazonws.region-name.ec2\" 2. \"com.amazonaws.region-name.ec2messages\" 3. \"com.amazonaws.region-name.ssm\" 4. \"com.amazonaws.region-name.ssmmessages\" ","date":"2020-04-02","objectID":"/sessionmanager/:1:2","tags":["cloud","aws"],"title":"AWS Session Manager","uri":"/sessionmanager/"},{"categories":["aws"],"content":"AWS Session Manager과 Bastion 비교 Session Manager Bastion 비용 Session Manager는 별도의 요금 없음 AWS PrivateLink 요금 Instance 운영 비용 로그 Amazon CloudWatch 및 S3 버킷 제공 SNS 알림 log 파일 수집 -\u003e log 수집기 보내는 작업 필요 보안 수동으로 인스턴스에 사용자 계정, 암호 또는 SSH 키를 설정할 필요 없음. Session Manager Console에 접속하면 인스턴스 접속 가능 (계정 관리 따로 안해도 됨 IAM으로 user에 따른 권한 부여) Bastion 서버에서 instance접속 SSH key관리 필요 설정 인스턴스에 SSM에이전트 설치 인스턴스가 생성될때 AmazonEC2RoleforSSM IAM 정책 할당 Session Manager에 대한 권한 IAM정책 수립 필요 (참고) VPC에 PrivateLink 설정 필요 구성된 Bastion 서버에 맞게 설정 참고 https://aws.amazon.com/ko/privatelink/pricing/ https://docs.aws.amazon.com/ko_kr/systems-manager/latest/userguide/session-manager.html ","date":"2020-04-02","objectID":"/sessionmanager/:2:0","tags":["cloud","aws"],"title":"AWS Session Manager","uri":"/sessionmanager/"},{"categories":["java","spring"],"content":"spring security는 spring 기반의 애플리케이션의 보안(인증, 권한)을 담당하는 프레임워크이다. spring security는 filter기반으로 동작하기 때문에 spring mvc와 분리되어 관리 되고 동작된다. ","date":"2020-01-04","objectID":"/security/:0:0","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"Spring Security 구조 ","date":"2020-01-04","objectID":"/security/:1:0","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"인증 architecture securitysecurity \" security 1. Received the Http Request Spring security는 series/chain filter들을 가진다. request가 오면 인증 및 허가 목적으로 chain filter를 통하게 된다. 또한 인증 메커니즘/모델에 기반한 관련 인증 필터를 찾을때 까지 filter chain을 가치게 된다. HTTP 기본 인증은 BasicAuthenticationFilter에 도달할 때 까지 filter chain을 거친다. HTTP Digest 인증은 DigestAuthenticationFilter를 거친다. Login form으로 request가 오면 UsernamePasswordAuthenticationFilter를 거친다. X509 인증 request가 오면 X509에 도달할때 까지 filter chain을 거친다. X509AuthenticationFilter 다양한 filter들은 밑에, 종류들을 나열해놨다. 2. Creates AuthenticationToken based on user credentials 인증요청이 AuthenticationFilter에 의해 수신되면, 수신된 request로 부터 username, password를 추출한다. 추출된 사용자 자격 증명을 기반으로 인증 개체를 생성한다. 만약 credential이 username, password면 이를 통해 UsernamePasswordAuthenticationToken을 username, password로 생성한다. 3. Delegating created AuthenticationToken for AuthenticationManagager 이후 UsernamePasswordAuthenticationToken object를 Authenticationmanager의 인증방법을 호출하는데 사용된다. ProviderManager에는 사용자 요청을 인증하는 데 사용해야 하는 구성된 AuthenticationProvider 목록이 있다. ProviderManager는 제공된 각 AuthenticationProvider를 살펴보고 전달된 Authentication Object를 기반으로 사용자를 인증하려고 시도한다. 4. Trying to authenticate with list of AuthenticationProvider AuthenticationProvider는 인증 객체를 이용해 사용자를 인증하려고 시도한다. provider CasAuthenticationProvider JaasAuthenticationProvider DaoAuthenticationProvider OpenIDAuthenticationProvider RememberMeAuthenticationProvider LdapAuthenticationProvider 5. UserDetailsService Required 일부 AuthenticationProvider는 사용자 이름을 기반으로 사용자 세부 정보를 검색하기 위해 UserDetailsService를 사용할 수 있다. 6 \u0026 7. UserDetails or User Object UserDetailsService는 사용자 이름을 기반으로 UserDetails를 검색한다. 8. Authentication Object Or AuthenticationException 사용자가 성공적으로 인증되면 완전히 채워진 인증 객체가 반환된다. 9. Authentication is done AuthenticationManager는 완전히 채워진 Authentication 개체를 관련 Authentication Filter로 다시 반환한다. 10. Setting up Authentication Object in SecurityContext 그런 다음 관련 AuthenticationFilter는 획득한 인증 개체를 향후 필터 사용을 위해 SecurityContext에 저장한다. ","date":"2020-01-04","objectID":"/security/:1:1","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"security의 filter 아까 위에서 말한 filter들의 종류들을 보자. SecurityContextPersistenceFilter : SecurityContextRepository 에서 SecurityContext를 가져오거나 저장하는 역할. LogoutFilter : 설정된 로그아웃 URL로 오는 요청을 감시하고 로그아웃 처리를한다. BasicAuthenticationFilter : HTTP 기본 인증 헤더를 감시하여 처리한다. RequestCacheAwareFilter : 로그인 성공 후 원래 요청 정보를 재구성하기 위해 사용한다. UsernamePasswordAuthenticationFilter : username과 password를 사용하는 form 기반 인증에서 설정된 login URL로 오는 요청을 감시하며 유저 인증을 처리한다. DefaultLoginPageGeneratingFilter : 인증을 위한 login form URL을 감시한다. SecurityContextHolderAwareRequestFilter : HttpServletRequestWrapper를 상속한 SecurityContextHolderAwareRequestWapper 클래스로 HttpServletRequest 정보를 감싼다. AnonymousAuthenticationFilter : 이 필터가 호출되는 시점까지 사용자 정보가 인증 되지 않았다면, 인증토큰에 사용자가 익명 사용자로 나타난다. SessionManagementFilter : 인증된 사용자와 관련된 모든 세션을 추적한다. ExceptionTranslationFilter : 보호된 요청을 처리하는 중에 발생할 수 있는 예외를 위암하거나 전달한다. FilterSecurityInterceptor : AccessDecisionManager로 권한부여 처리를 위임해서 접근 제어 결정을 쉽게 해준다. ","date":"2020-01-04","objectID":"/security/:1:2","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"Authentication 모든 접근 주체는 Authentication을 생성한다. 이것은 SecurityContext에 보관되고 사용된다. security의 세션들은 내부 메모리(SecurityContextHolder)에 쌓고 꺼내 쓰는것이다. ","date":"2020-01-04","objectID":"/security/:1:3","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"AuthenticationManager User의 요청내에 담긴 Authentication을 AuthenticationManager에 넘겨주고 AuthenticationManager를 구현한 ProviderManager가 처리한다. public interface Authentication extends Principal, Serializable { Collection\u003c? extends GrantedAuthority\u003e getAuthorities(); // Authentication 저장소에 의해 인증된 사용자의 권한 목록 Object getCredentials(); // 주로 비밀번호 Object getDetails(); // 사용자 상세정보 Object getPrincipal(); // 주로 ID boolean isAuthenticated(); //인증 여부 void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException; } ","date":"2020-01-04","objectID":"/security/:1:4","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"설정 의존성 추가 security 설정 ","date":"2020-01-04","objectID":"/security/:2:0","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"의존성추가 gradle implementation 'org.springframework.boot:spring-boot-starter-security' ","date":"2020-01-04","objectID":"/security/:2:1","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"security 설정 @Configuration @EnableWebSecurity @AllArgsConstructor public class securityConfig extends WebSecurityConfigurerAdapter { private UserService userService; @Bean public PasswordEncoder passwordEncoder() { return new BCryptPasswordEncoder(); } @Override public void configure(WebSecurity web) throws Exception { web.ignoring().antMatchers(\"/css/\\*\\*\", \"/js/\\*\\*\", \"/img/\\*\\*\", \"/lib/\\*\\*\"); super.configure(web); } @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .antMatchers(\"/user/myinfo/**\").hasRole(\"USER\") .antMatchers(\"/admin/**\").hasRole(\"ADMIN\") .antMatchers(\"/**\").permitAll(); http.csrf().disable(); http.formLogin() .loginPage(\"/user/login\") .failureUrl(\"/login-error\") .defaultSuccessUrl(\"/user/login/result\", true) .usernameParameter(\"username\") .passwordParameter(\"password\"); http.logout() .logoutRequestMatcher(new AntPathRequestMatcher(\"/user/logout\")) .logoutSuccessUrl(\"/user/logout/result\") .invalidateHttpSession(true); http.exceptionHandling().accessDeniedPage(\"/user/denied\"); } @Override public void configure(AuthenticationManagerBuilder auth) throws Exception { auth.userDetailsService(userService).passwordEncoder(passwordEncoder()); } // 만약 default로 유저를 설정하고 싶다면 // // @Override // protected void configure(AuthenticationManagerBuilder auth) throws Exception { // auth.inMemoryAuthentication() // .withUser(\"user\") // .password(passwordEncoder().encode(\"1234\")) // .roles(\"USER\"); // } } @EnableWebSecurity @Configuration 클래스에 @EnableWebSecurity 어노테이션을 추가하여 spring security클래스라고 명시해준다. passwordEncoder() BCryptPasswordEncode는 spring security에서 제공하는 비밀번호 암호화 객체이다. Service에서 비밀번호를 암호화 할 수 있도록 Bean으로 등록한다. 다음으로 Configure() 메서드를 오버라이딩 하여 security설저을 해준다. configure(WebSecurity web) WebSecurity는 FilterChainProxy를 생성하는 필터이다. web.ignoring().antMatchers(\"/css/\", “/js/”, “/img/”, “/lib/\"); 와 같이 사용하고 해당 경로의 파일들은 spring security가 무시할 수 있도록 설정한다. configure(HttpSecurity http) HttpSecurity를 통해 HTTP요청에 대한 웹 기반 보안 설정을 할 수 있다. authorizeRequests() HttpServletRequest에 따라 접근을 제한한다. antMatchers() 메서드로 특정 경로를 지정하고 permitAll(), hasRole() 메서드로 역할에 따른 접근 설정을 잡아준다. formlogin() form기반으로 인증하도록 해준다. 로그인 정보는 기본적으로 HttpSession을 이용한다. 기본적으로 spring security에서 제공되는 login form을 사용할 수 있는데 /login으로 접근하면 된다. .loginPage(\"/user/login”) 기본으로 제공되는 form말고 커스텀 form을 사용하고 싶으면 loginPage() 메서드를 이용한다. 이 때 커스텀 로그인 form의 action 경로와 loginPage()의 파라미터 경로가 일치해야 인증을 처리할 수 있다. logout() 로그아웃을 지원하는 메서드. 기본적으로 /logout에 접근하면 HTTP 세션을 제거한다. .logoutRequestMatcher(new AntPathRequestMatcher(\"/user/logout\")) 기본 /logout이 아닌 다른 URL로 커스텀한다. .invalidateHttpSession(true); HTTP 세션을 초기화 한다. configure(AuthenticationManagerBuilder auth) spring security 에서 모든 인증은 AuthenticationManager를 통해 이뤄지며 AuthenticationManager를 생성하기 위해서는 AuthenticationManagerBuilder를 사용한다. 인증을 위해서 UserDetaulService를 통해 필요한 정보들을 가져온다. service 클래스에서는 UserDetailsService 인트퍼에시를 implements하여, loadUserByUsername() 메서드를 구현하면된다. ","date":"2020-01-04","objectID":"/security/:2:2","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["java","spring"],"content":"실습 예제 UserController @Controller @AllArgsConstructor public class UserController { private UserService userService; @GetMapping(value = \"/index\") public String home() { return \"/index\"; } // 회원가입 페이지 @GetMapping(\"/user/signup\") public String dispSignup() { return \"/signup\"; } // 회원가입 처리 @PostMapping(\"/user/signup\") public String execSignup(UserDto memberDto) { userService.joinUser(memberDto); return \"redirect:/user/login\"; } // 로그인 페이지 @GetMapping(\"/user/login\") public String dispLogin() { return \"/login\"; } // 로그인 결과 페이지 @GetMapping(\"/user/login/result\") public String dispLoginResult() { return \"/loginSuccess\"; } // 로그아웃 결과 페이지 @GetMapping(\"/user/logout/result\") public String dispLogout() { return \"/logout\"; } // 접근 거부 페이지 @GetMapping(\"/user/denied\") public String dispDenied() { return \"/denied\"; } // 내 정보 페이지 @GetMapping(\"/user/info\") public String dispMyInfo() { return \"/myinfo\"; } // 어드민 페이지 @GetMapping(\"/admin\") public String dispAdmin() { return \"/admin\"; } } UserService @Service @AllArgsConstructor public class UserService implements UserDetailsService { private UserRepository userRepository; @Transactional public UserEntity joinUser(UserDto userDto) { BCryptPasswordEncoder passwordEncoder = new BCryptPasswordEncoder(); userDto.setPassword(passwordEncoder.encode(userDto.getPassword())); return userRepository.save(userDto.toEntity()); } @Override public UserDetails loadUserByUsername(String userEmail) throws UsernameNotFoundException { Optional\u003cUserEntity\u003e userEntityWrapper = userRepository.findByEmail(userEmail); UserEntity userEntity = userEntityWrapper.get(); List\u003cGrantedAuthority\u003e authorities = new ArrayList\u003c\u003e(); if((\"admin@example.com\").equals(userEmail)) { authorities.add(new SimpleGrantedAuthority(Role.ADMIN.getValue())); } else { authorities.add(new SimpleGrantedAuthority(Role.USER.getValue())); } return new User(userEntity.getEmail(), userEntity.getPassword(), authorities); } } UserEntity @NoArgsConstructor(access = AccessLevel.PROTECTED) @Getter @Entity public class UserEntity extends BaseTimeEntity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Column(length = 20, nullable = false) private String email; @Column(length = 100, nullable = false) private String password; @Builder public UserEntity(Long id, String email, String password) { this.id = id; this.email = email; this.password = password; } } UserRepository public interface UserRepository extends JpaRepository\u003cUserEntity, Long\u003e { Optional\u003cUserEntity\u003e findByEmail(String userEmail); } Role @AllArgsConstructor @Getter public enum Role { ADMIN(\"ROLE_ADMIN\"), USER(\"ROLE_USER\"); private String value; } UserDto @Getter @Setter @ToString @NoArgsConstructor public class UserDto { private Long id; private String email; private String password; private LocalDateTime createdDate; private LocalDateTime modifiedDate; public UserEntity toEntity(){ return UserEntity.builder() .id(id) .email(email) .password(password) .build(); } @Builder public UserDto(Long id, String email, String password) { this.id = id; this.email = email; this.password = password; } } 참고 https://springbootdev.com/2017/08/23/spring-security-authentication-architecture/#more-54 https://sjh836.tistory.com/165 https://victorydntmd.tistory.com/328 ","date":"2020-01-04","objectID":"/security/:3:0","tags":["java","spring"],"title":"Spring Security","uri":"/security/"},{"categories":["aws"],"content":"Route53은 AWS에서 제공하는 DNS서비스 이다. DNS는 domain name(www.example.com) 을 ip주소로 바꿔 주는 일종의 dictionary 서비스 이다. DNS동작과정은 네트워크 통신을 하기 위해 IP를 찾아가는 과정이다. 이러한 mapping 정보를 저장해 놓는 파일을 DNS Zone file이라고 한다. ","date":"2019-12-26","objectID":"/route53/:0:0","tags":["cloud","aws"],"title":"Amazon Route53 DNS Service","uri":"/route53/"},{"categories":["aws"],"content":"레코드 DNS서버에 저장해놓은 파일을 기반으로 주소를 변환하는데, 여기서 정의되는 레코드들은 다음과 같다. A(Address Mapping records) 도메인을 IP로 Mapping 레코드 A는 주어진 호스트에 대한 IP주소 (IPv4)를 알려준다. AAAA(IP Version 6 Address records) 레코드 AAAA(quad-A)는 주어진 호스트에 대해 IPv6주소를 알려준다. 결국 A레코드와 같은 방식으로 작동. 차이는 IPv6이다. CNAME(Canonical Name) 도메인명을 다른 도메인과 Mapping할때 사용 (alias와 비슷) 즉, 도메인 이름의 별칭을 만드는데 사용. 경우에 따라 CNAME 레코드를 제거하고 A레코드로 대체하면 성능 오버헤드를 줄일 수 있다고한다..(아직 잘 모르겠네요) HINFO(Host Information) HINFO 레코드는 호스트에 대한 일반 정보를 얻는데 사용. CPU 및 OS유형을 알려준다. 따라서 두 호스트가 통신하기를 원할 때 OS특정 프로토콜을 사용할 수 있는 가능성을 제공한다. 일반적으로 보안 때문에 공용 서버에서 사용되지 않는다. ISDN(Integrated Services Digital Network) ISDN주소를 알려준다. ISDN주소는 국가코드, 국가 별 대상코드, ISDN 가입자 번호 및 선택적으로 ISDN 하위 주소로 구성된 전화 번호 이다. MX(Mail exchanger) DNS도메인 이름에 대한 메일 교환 서버를 알려준다. 이 정보는 SMTP가 전자 메일을 적절한 호스트로 라우팅하는 데 사용한다. 일반적으로 DNS 도메인에 대해 둘 이상의 메일 교환 서버가 있으며 각 도메인에 우선 순위가 설정 된다. NS(Name Server) DNS서버가 참조하는 다른 DNS서버이다. DNS 서버 자신에서 domain name에 대한 주소를 알아 내지 못할때, 이 NS레코드에 정의도니 서버로 가서 주소를 알아 온다. PTR(Reverse-lookup Pointer records) 정방향 DNS확인 (A, AAAA)레코드와 달리 PTR레코드는 IP주소를 기반으로 도메인 이름을 찾는데 사용된다. SOA(Start of Authority) 기본 이름 서버, 도메인 관리자의 전자 메일, 도메인 일련 번호 및 영역 새로 고침과 관련된 여러 타이머를 포함하여 DNS 영역에 대한 핵심 정보를 지정. TXT(Text) 형식이 지정되지 않은 임의의 텍스트 문자열을 저장할 수 있다. ","date":"2019-12-26","objectID":"/route53/:1:0","tags":["cloud","aws"],"title":"Amazon Route53 DNS Service","uri":"/route53/"},{"categories":["aws"],"content":"캐싱 DNS 서버의 특성중에 중요한 특성은 캐싱이다. 보통 DNS서버는 클라이언트가 사용하는 로컬 네트워크에 있는 DNS를 사용하게 된다. 이 DNS 서버들은 look up을 요청한 목적 서비스 서버에 대한 ip 주소를 다른 클라이언트가 요청할 때 응답을 빠르게 하기 위해 자체적으로 캐싱 하고 있다. 예를들어서 www.example.com 으로 접속할 수 있는 서비스가 있다고 하면 이 것을 운영하고 있는 회사의 DNS서버에 주소가 정의되어 있을 것이다. 만약 나의 스마트 폰으로 이 www.example.com 으로 접근하면, 내가 쓰는 통신사의 DNS서버를 이용해 주소를 look up할 것이고, 이 DNS서버는 위의 www.example.com 을 운영하고 있는 회사의 DNS서버에 주소를 물어보고 다음 서비스를 위해 캐시를 업데이트 한다. 이 캐시가 업데이트 되는 시간이 TTL이다. ","date":"2019-12-26","objectID":"/route53/:2:0","tags":["cloud","aws"],"title":"Amazon Route53 DNS Service","uri":"/route53/"},{"categories":["aws"],"content":"aws route53 route53에서 네임서버 등록하는 것을 알아보자. route53사이트에서 Create Hosted Zone을 눌러서 생성하면 4개의 NS레코더가 생성된다. 예를들어 다음과 같이 생성된다. 이를 도메인 등록 대행 기관 사이트에 가서 route53에서 지정된 NS정보를 입력한다. ns-1.awsdns-50.com ns-6.awsdns-20.net ns-3.awsdns-30.co.uk ns-8.awsdns-33.org route53은 2가지 host zone이 있다. Public Host zone 일반 네임서버로 동작 Private Host zone AWS내에서만 동작 route53은 ALIAS가 있다 도메인에 별칭을 줄 수 있어서 만약 example.com 도메인에 ALIAS Target을 www로 줘서 www.example.com 과 같은 IP를 응답주게 가능하다. Route 기능 route53은 DNS + 모니터링 + L4 + GSLB기능을 제공한다. Route53은 특정 포트에 대해 모니터링이 가능하다. L4기능, Failover기능이 있다. GSLB은 Global Server Load Balancing으로 지역에 상관없이 부하를 분산해주는 기능을 제공한다. 참고 https://win100.tistory.com/360 https://bcho.tistory.com/795 https://brunch.co.kr/@topasvga/49 ","date":"2019-12-26","objectID":"/route53/:3:0","tags":["cloud","aws"],"title":"Amazon Route53 DNS Service","uri":"/route53/"},{"categories":["java","spring"],"content":"아직 Spring \u0026 MyBatis를 사용한다고 하지만 이젠 생산성이 좋은 JPA를 많이 쓴다고 하니 해보자. ","date":"2019-12-17","objectID":"/jpa/:0:0","tags":["java","spring","jpa"],"title":"Spring Data JPA","uri":"/jpa/"},{"categories":["java","spring"],"content":"Entity @NoArgsConstructor @Getter @Entity public class Students extends BaseTimeEntity { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String studentId; private String password; private String name; private String birthDay; private String email; private String phone; @Builder public Students(String studentId, String password, String name, String birthDay, String email, String phone){ this.studentId = studentId; this.password = password; this.name = name; this.birthDay = birthDay; this.email = email; this.phone = phone; } public void updateStudent(UpdateStudentReq dto) { this.password = dto.getPassword(); this.birthDay = dto.getBirthDay(); this.phone = dto.getPhone(); this.email = dto.getEmail(); } } 여기서 Student 클래스는 실제 DB의 테이블과 Mapping될 클래스이고 Entity 클래스라고 한다. @Entity 테이블과 연결될 클래스이다. _으로 이름을 매칭한다. (ex. JaejinTest -\u003e Jaejin_Test) @Id 해당 테이블의 PK필드를 나타낸다. @GeneratedValue PK의 생성 규칙 default는 AUTO이고 auto_increment와 같이 자동 증가하는 정수 값이 된다. spring2.x버전 부터는 조심 ! 참고 blog @Column 테이블의 컬럼 굳이 사용안해도 모두 컬럼이 된다. 다음과 같이 모두 그냥 써도 컬럼으로 인식한다. private String studentId; private String password; private String name; 기본값 외에 추가가 필요한 옵션이 있으면 사용한다. JPA는 lombok 라이브러리하고 같이 사용하니 알아두도록하자. lombok은 단순히 의존성 추가 뿐만아니라 IDE에 맞게 셋팅이 필요하다. ","date":"2019-12-17","objectID":"/jpa/:1:0","tags":["java","spring","jpa"],"title":"Spring Data JPA","uri":"/jpa/"},{"categories":["java","spring"],"content":"Repository 이제 Entity를 Repository와 연결 시켜 보자. @Repository public interface StudentsRepository extends JpaRepository\u003cStudents, Long\u003e { } JpaRepository를 상속 하고 있는데 이렇게 되면 CRUD를 따로 작성 할 필요없이 사용가능하다. private final StudentsRepository studentsRepository; public Students create(StudentsDto.RegistStudentReq dto) { return studentsRepository.save(dto.toEntity()); } 이렇게 JpaRepository를 상속받은 Repository객체를 이용해서 save 메소드를 바로 호출 할 수 있다. ","date":"2019-12-17","objectID":"/jpa/:2:0","tags":["java","spring","jpa"],"title":"Spring Data JPA","uri":"/jpa/"},{"categories":["java","spring"],"content":"Queries CRUD는 대충 알것 같은데 내가 원하는 Query를 작성하고 싶다면 어떻게 해야할까? @Repository public interface StudentsRepository extends JpaRepository\u003cStudents, Long\u003e { List\u003cStudent\u003e findByEmailAddressAndName(String emailAddress, String name); } 이렇게 한 추가 해보면 다음과 같이 Query로 인식한다. select s from Students s where s.emailAddress = ?1 and s.name = ?2 더 많은 Sample을 확인하고 싶다면 JPA 문서 Table 2.2. Supported keywords inside method names 를 확인하자. 물론 @Query로 직접 Query를 작성할 수도 있다. @Repository public interface StudentsRepository extends JpaRepository\u003cStudents, Long\u003e { @Query(\"select s from Students s where s.emailAddress = ?1 and s.name = ?2\") List\u003cStudent\u003e findByEmailAddressAndName(String emailAddress, String name); } 참고로 Update를 해야하는 Query에는 @Modifying을 추가해야한다. @Modifying @Query(\"UPDATE Posts p SET p.author = :author, p.content = :content, p.title = :title, p.modifiedDate = :modifiedDate WHERE p.id = :id\") void editPost(@Param(\"author\") String author, @Param(\"content\") String content, @Param(\"title\") String title, @Param(\"modifiedDate\") LocalDateTime modifiedDate, @Param(\"id\") Long id); 참고 - https://jojoldu.tistory.com/295 ","date":"2019-12-17","objectID":"/jpa/:3:0","tags":["java","spring","jpa"],"title":"Spring Data JPA","uri":"/jpa/"},{"categories":["blockchain"],"content":"Hyperledger Fabric에 대해 알아 보았따. 이제 직접 개발 환경은 올려보자. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:0:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"개발을 위한 환경 docker go virtualBox ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:1:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Docker VM환경 생성 물론 default환경을 사용해도 좋지만 나중에 여러개의 node를 올려보기 위해서 미리 분리를 하도록 하자. $ docker-machine create --driver virtualbox blockchain 다음으로 생성된 VM의 환경변수를 확인하자 $ docker-machine env blockchain 이 환경을 그대로 적용 시키자 $ eval $(docker-machine env blockchain) 잘 적용되었는지 확인하려면 ps나 images 명령을 날려보자. $ docker ps or $ docker images docker machine을 만들기 전과 결과가 다르다면 성공이다. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:2:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Hyperledger Fabric Docker image받기 docker pull hyperledger/fabric-baseimage:x86_64-0.2.2 docker pull hyperledger/fabric-membersrvc:x86_64-0.6.1-preview docker pull hyperledger/fabric-peer:x86_64-0.6.1-preview baseimage의 경우 TAG를 latest로 해야한다. docker tag hyperledger/fabric-baseimage:x86_64-0.2.2 hyperledger/fabric-baseimgae:latest docker images 이제 image들을 받았으니 docker-compose 파일을 하나 작성하자. membersrvc:image:hyperledger/fabric-membersrvc:x86_64-0.6.1-previewports:- \"7054:7054\"command:membersrvcvp0:image:hyperledger/fabric-peer:x86_64-0.6.1-previewports:- \"7050:7050\"- \"7051:7051\"- \"7053:7053\"environment:- CORE_PEER_ADDRESSAUTODETECT=true- CORE_LOGGING_LEVEL=DEBUG- CORE_PEER_ID=vp0- CORE_PEER_PKI_ECA_PADDR=membersrvc:7054- CORE_PEER_PKI_TCA_PADDR=membersrvc:7054- CORE_PEER_PKI_TLSCA_PADDR=membersrvc:7054- CORE_SECURITY_ENABLED=true- CORE_SECURITY_ENROLLID=test_vp0- CORE_SECURITY_ENROLLSECRET=MwYpmSRjupbTlinks:- membersrvccommand:sh -c \"sleep 5; peer node start --peer-chaincodedev\" $ docker-compose up 에러 없이 정상 작동하면 Blockchain server가 실행된 것이다. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:3:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Fabric 소스 준비 chaincode는 go로 되어있는 example을 이용하자. $ mkdir -p $GOPATH/src/github.com/hyperledger $ cd $GOPATH/src/github.com/hyperledger $ git clone https://github.com/hyperledger/fabric.git $ cd fabric $ git checkout v0.6.1-preview ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:4:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"chaincode 빌드 및 실행 소스를 받았으면 빌드를 한다. $ cd $GOPATH/src/github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 $ go build build되었으면 chaincode_example02라는 실행파일이 생성되어 있을 것이다. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:5:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"개발모드 vs 운영모드 개발모드에서는 chaincode가 blockchain 런타임과 별도의 프로세스로 실행된다. 운영모드에서는 chaincode를 deploy하면 blockchain network에 있는 모든 peer들에게 복제가 되고 모든 peer들이 동일한 chaincode로 동작한다. 개발모드는 단일 peer에 chaincode가 붙어서 동작한다. 이제 실제로 블록체인 런타임에 chaincode를 실행하자. 이전에 생성했던 docker-compose.yaml파일을 실행한다. $ docker-compose up -d chaincode 실행 전 docker VM의 IP를 확인한다. docker-machine ls URL에 tcp://192.168.99.100:2376 이라고 (아마 다를 수도 있습니다) 나올 텐데 이를 CORE_PEER_ADDRESS에 넣고 실행시키면된다. $ cd $GOPATH/src/github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 $ CORE_CHAINCODE_ID_NAME=mycc CORE_PEER_ADDRESS=192.168.99.100:7051 ./chaincode_example02 요런식으로. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:5:1","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"테스트 REST API를 통해 테스트 해보자. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:6:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"사용자 등록 및 로그인 먼저 사용자 등록 및 로그인을 해보자. ID, PW는 사전에 정의 되어 있어서 동일 하게 테스트 한다. POST /registrar { \"enrollId\": \"admin\", \"enrollSecret\": \"Xurw3yU9zI0l\" } response로 { \"OK\": \"Login successful for user 'admin'.\" } 이라고 응답이 오면 성공한 것이다. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:6:1","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"chaincode deploy POST /chaincode { \"jsonrpc\": \"2.0\", \"method\": \"deploy\", \"params\": { \"type\": 1, \"chaincodeID\":{ \"name\": \"mycc\" }, \"ctorMsg\": { \"args\":[\"init\", \"a\", \"100\", \"b\", \"200\"] }, \"secureContext\": \"admin\" }, \"id\": 1 } 이에 대한 response는 { \"jsonrpc\": \"2.0\", \"result\": { \"status\": \"OK\", message\": \"mycc\" }, \"id\": 1 } 이렇게 돌아오면 성공적이다. 만약 개발 모드에서 chaincode를 수정했을시 코드 빌드 -\u003e chaincode 실행 -\u003e 로그인 API호출 -\u003e chaincode deploy 호출 단계로 진행해야 기본 환경 준비가 된다. ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:6:2","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Invoke POST /chaincode { \"jsonrpc\": \"2.0\", \"method\": \"invoke\", \"params\": { \"type\": 1, \"chaincodeID\":{ \"name\":\"mycc\" }, \"ctorMsg\": { \"args\":[\"invoke\", \"a\", \"b\", \"10\"] }, \"secureContext\": \"admin\" }, \"id\": 3 } response { \"jsonrpc\": \"2.0\", \"result\": { \"status\": \"OK\", \"message\": \"~~~~\" }, \"id\": 3 } ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:6:3","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Query invoke가 정상적으로 호출 되었다면 Query로 값을 확인하자. POST /chaincode { \"jsonrpc\": \"2.0\", \"method\": \"query\", \"params\": { \"type\": 1, \"chaincodeID\":{ \"name\":\"mycc\" }, \"ctorMsg\": { \"args\":[\"query\", \"a\"] }, \"secureContext\": \"admin\" }, \"id\": 5 } response { \"jsonrpc\": \"2.0\", \"result\": { \"status\": \"OK\", \"message\": \"~~~\" }, \"id\": 3 } 참고 - IBM Developer Blog ","date":"2019-12-16","objectID":"/hyperledger-fabric2/:6:4","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric 개발 환경 구성 및 chaincode 배포","uri":"/hyperledger-fabric2/"},{"categories":["blockchain"],"content":"Hyperledger Fabric에 대해 알아봅시다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:0:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Hyperledger 하위 프로젝트 Hyperledger에는 여러 하위 프로젝트가 있다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:1:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Blockchain Explorer Blockchain Explorer는 Blockchain 런티암에 대한 다양한 정보를 보여주기 위한 웹 기반 어플리케이션을 만드는 프로젝트이다. ICON에서 Tracker 처럼 Block, Tx 정보를 보여주거나, Chaincode 및 원장의 정보들을 모니터링 하기 위한 기능을 포함하고 있다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:1:1","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Fabric Fabric은 Blockchain 기술을 구현한 프로젝트이다. 즉 엔진을 만드는 프로젝트. Chaincode라고 부르는 SmartContract 코드를 통해 적용 시킬 수 있다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:1:2","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Iroha Iraha는 시스템에 분산원장의 기능을 쉽게 통합하고자 하기 위한 목적으로 개발 되고 있는 프로젝트 ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:1:3","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Sawtooth Lake ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:1:4","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"Hyperledger Fabric ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:2:0","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"용어 Transactor : transaction을 일으키는 Entity Transaction : Tx Ledger : Tx와 현재 상태를 포함하는 일련의 암호 학적으로 링크 된 Block. 이전 거래의 데이터 외에 원장에는 현재 실행중인 ChainCode의 데이터가 포함되어 있다. World state : Tx에 의해서 Chaincode가 호출될 때 상태 및 데이터 저장을 위한 Key-value 데이터베이스 Chaincode : 다양한 Tx유형을 구현한 Blockchain에 임베드 되는 로직. SmartContract라고 보면 될거 같다. Chaincode는 Tx 를 일으키고 유효성이 확인되면 공유원장에 추가하고 World state를 수정한다. Validating peer(VP) : Blockchain network에서 원장을 관리 유지하기 위해 Tx의 유효성을 검증하는 합의 프로토콜을 실행하는 노드이다. 합의에 실패하면 Block에서 제거되므로 장부에 기록되지 않는다. Validating peer는 Chaincode를 deploy, invoke, query할 권한을 가진다. Non-validating peer(NVP) : Transactor가 Validating peer에 접속 할 수 있도록 프록시 역할을 하는 노드. NVP는 호출된 요청을 Validating peer로 전달하며, 이벤트 스트림, REST를 담당하는 노드이다. Consensus : Blockchain network의 Tx(deploy, invoke) 순서를 유지하는 프로토콜, Validating 노드들은 합의 프로토콜을 구현하여 Tx를 승인하기 위해 동작한다. Permissioned network : 각 노드는 Blockchain network에서 접근 권한을 관리해아 하는 노드, 각 노드는 권한이 있는 사용자만 접근할 수 있다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:2:1","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"아키텍쳐 architecturearchitecture \" architecture Membership Services Blockchain network에서 인증 서비스를 제공한다. Non-permissioned blockchain의 경우 사용자 인증이 필요없고, 모든 노드는 동등하게 Tx 처리가 가능하고 Block에 Tx 정보를 입력할 수 있다. Membership Services는 PKI(Public Key Infrastructure)와 분산화/합의 컴포넌트를 non-permissioned Blockchain에서 permissioned Blockchain으로 변환시킨다. Permissioned Blockchain에서는 Entity가 장기적인 인증서(enrollment certificates)를 획득하기 위해 등록절차를 거치게 되며, Entity 유형에 따라 구별될 수 있다. 사용자의 경우 TCA (Transaction Certifacate authority)가 인증서를 발급 할 수 있다. 여기서 얻은 인증서는 Tx를 발생시킬때 인증에 사용된다. Blockchain Services Blockchain Services는 HTTP/2를 기반으로 P2P 프로토콜을 통해 분산원장을 관리한다. 데이터 구조는 Hash 알고리즘을 통해 World state를 복제하는 등 관리 하는데 가장 효율적으로 관리 할 수 있도록 최적화되어 있다. 필요에 따라 합의 알고리즘 플러그인(PBET,PoW, PoS..)을 연결하고 구성 할 수 있다. Chaincode Services Chaincode Services는 Validating node에서 안전하고 가벼운 방법으로 Chaincode가 실행되도록 보장한다. 환경은 보안 OS 및 Chaincode 언어, Go, Java 및 Node.js등 사용할 수 있다. 그외 4.1. Events Validating peers와 chaincode는 Blockchain network에서 Event를 발생할 수 있다. Event 발생을 대기하고 있는 어플리케이션에 필요한 노티를 보낸다. 또한 Web hooks나 Kafka 등을 이용하여 Event 수신도 가능하다. 4.2. Application Programming Interface (API) Fabric에 대한 기본 Interface REST API이다. API 호출을 통해 사용자 등록, Blockchain에 대한 쿼리, Tx발생 들을 할 수 있다. 특히 Chaincode와 상호작용하기 위한 API를 통해 Tx를 관리 할 수 있다. 4.3. Command Line Interface (CLI) REST API의 일부 기능을 지원하는 CLI기능을 통해 Chaincode의 deploy 및 Tx 처리 등을 빠르게 진행할 수 있도록 한다. CLI는 Go로 제작이 되어 있으며 다양한 OS를 지원한다. ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:2:2","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["blockchain"],"content":"토폴로지 Fabric을 통한 Blockchain network는 하나의 멤버쉽 서비스와 다수의 Validating peer와 non-validating peer 들로 이루어 질 수 있다. 이 모든 컴포넌트를 통해 하나 또는 다수의 체인을 운영 할 수 있다. Single Validating Peers 기능적으로 non-validating peer는 validating peer의 서브셋이다. 그러므로 모든 non-validating peer의 기능은 validating peer로 사용 가능하다. 그래서 가장 간단한 Blockchain의 network는 하나의 validating peer로 구성된 Blockchain network이다. Multiple Validating Peers 운영환경이거나 개발 환경일 경우 다양한 validating peer와 non-validating peer를 이용하여 Blockchain network를 구성해야 합니다. 이 구성에서 non-validating peer는 Event 처리 및 REST API 서비스 관리등의 역할을 하게 되는 노드이다. multipleValidatingPeermultipleValidatingPeer \" multipleValidatingPeer Validating peers들은 Blockchain network 상에서 일어나는 모든 Event, Tx등의 데이터를 공유하게 된다. Multichain Blockchain network는 validating peer와 non-validating peer로 이루어져 있다. 다양한 목적에 따라서 이와 같은 조합으로 다양한 Blockchain을 구성 할 수 있다. 참고 - IBM Developer Blog ","date":"2019-12-15","objectID":"/hyperledger-fabric1/:2:3","tags":["blockchain","hyperledger"],"title":"Hyperledger Fabric","uri":"/hyperledger-fabric1/"},{"categories":["java","spring"],"content":"Spring MVC를 이용해 웹 어플리케이션을 개발하는 방법을 살펴본다. Spring boot를 공부하기전 비교?를 하기 위해서 살짝 보자. Spring을 결국엔 하게 되었다. Spring boot를 많이 사용하지만 MVC의 내용은 알아야 하므로 살펴봅시다. ","date":"2019-12-11","objectID":"/springmvcbasic/:0:0","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"Spring MVC의 구성 및 흐름 다른 MVC기반의 프레임 워크와 마찬가지로 Spring MVC도 Controller를 사용하여 Client의 요청을 처리한다. 이 Controller의 역할을 하는 것이 DispatcherServlet이다. 구성요소 설명 DespatcherServlet Client의 요청을 받음 Controller에게 Client의 요청을 전달하고 Contoller의 return값을 받아 View에 전달 HandlerMapping Client의 요청 URL을 어떤 Controller가 처리할지 결정 Controller Client의 요청 처리, 결과를 DispatcherServlet에 알려줌 ViewResolver Commander의 처리 결과를 보여줄 View를 결정 View Commander의 처리 결과를 보여줄 응답을 생성 흐름은 다음과 같다. Client의 요청이 DispatcherServlet에 전달 DispatcherServlet은 HandlerMapping을 사용해 Client의 요청이 전달될 Controller 객체 검색 DispatcherServlet은 Contoller 객체의 handleRequest() 메소드를 호출해 Client의 요청 처리 Contoller.handleRequest() 메소드는 ModelAndView 객체 return DispatcherServlet은 ViewResolver로 부터 처리 결과를 보여줄 View 검색 View는 Client에 전송할 응답 생성. ","date":"2019-12-11","objectID":"/springmvcbasic/:1:0","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"Spring MVC 웹 개발 Client의 요청을 받을 DispatcherServlet을 web.xml 파일에 설정. 요청 URL과 Controller의 매핑 방식 설정. Controller의 처리 결과를 어떤 View로 보여줄지 결정하는 ViewResolver설정 Controller를 작성 View 코드 작성 ","date":"2019-12-11","objectID":"/springmvcbasic/:2:0","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"DispatcherServlet 설정 및 Spring context 설정 Spring MVC를 사용하기 위해서 web.xml 파일에 DispatcherServlet을 설정 해줘야한다. DispatcherServlet은 Servlet 클래스로서 다음과 같이 설정해주면 된당. \u003c?xml version=\"1.0\" ?\u003e \u003cweb-app version=\"2.4\" xmlns=\"http://java.sun.com/xml/ns/j2ee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd\"\u003e \u003cservlet\u003e \u003cservlet-name\u003eexample\u003c/servlet-name\u003e \u003cservlet-class\u003eorg.springframework.web.servlet.DispatcherServlet\u003c/servlet-class\u003e \u003cload-on-startup\u003e1\u003c/load-on-startup\u003e \u003c/servlet\u003e \u003cservlet-mapping\u003e \u003cservlet-name\u003eexample\u003c/servlet-name\u003e \u003curl-pattern\u003e*.do\u003c/url-pattern\u003e \u003c/servlet-mapping\u003e \u003c/web-app\u003e 위의 설정은 *.do로 들어오는 요청을 DispatcherServlet이 처리하도록 하고 있다. 위의 경우 \u003cservlet-name\u003eexample\u003c/servlet-name\u003e 로서 이름이 example 이므로 사용되는 Spring설정 파일은 WEB-INF/example-servlet.xml 이다. 만약 다른 이름의 파일을 사용하고 싶다면 \u003cinit-param\u003e을 추가 해주면 된다. \u003cservlet\u003e \u003cservlet-name\u003eexample\u003c/servlet-name\u003e \u003cservlet-class\u003eorg.springframework.web.servlet.DispatcherServlet\u003c/servlet-class\u003e \u003cinit-param\u003e \u003cparam-name\u003econtextConfigLocation\u003c/param-name\u003e \u003cparam-value\u003e /WEB-INF/example1.xml, /WEB-INF/example2.xml \u003c/param-value\u003e \u003c/init-param\u003e \u003cload-on-startup\u003e3\u003c/load-on-startup\u003e \u003c/servlet\u003e 여기서 만약 동일한 이름의 Bean을 지정했다면 뒤에 위치한 설정 파일에 명시된 Bean이 우선순위를 가진다. ","date":"2019-12-11","objectID":"/springmvcbasic/:2:1","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"HandlerMapping 설정 클라이언트 요청을 Sping의 DispatcherServlet이 처리 하도록 설정했다면 다음은 HandlerMapping을 사용할지의 여부이다. HandlerMapping은 클라이언트의 요청을 어떤 Commender가 수행할 지의 여부를 결정한다. 구현체 설명 BeanNameUrlHandlerMapping 요청 URI와 동일한 이름을 가진 Controller Bean을 Mapping SimpleUrlHandlerMapping 경로 Mapping 방식을 사용하여 URI와 Controller Bean을 Mapping BeanNameUrlHandlerMapping BeanNameUrlHandlerMapping은 요청 URI와 동일한 이름을 갖는 Controller Bean으로 하여금 클라이언트 요청을 처리한다. HandelrMapipng Bean을 따로 생성하지 않은 경우 DispatcherServlet은 기본적으로 HandlerMapping의 구현체로 BeanNameUrlHandlerMapping을 사용한다. SimpleUrlHandlerMapping SimpleUrlHandlerMapping은 Ant 스타일의 Mapping 방식을 사용하여 Controller에 매칭한다. ? - 한글자에 매칭 * - 0개 이상의 글자에 매칭 ** - 0개 이상의 디렉토리에 매칭 ","date":"2019-12-11","objectID":"/springmvcbasic/:2:2","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"ViewResolver 설정 HandlerMapping을 했다면 이제 ViewResolver를 설정한다. internalResourceViewResolver - JSP를 사용하여 View 생성 VelocityViewResolver - Velocity 템플릿 엔진을 사용하여 View 생성 internalResourceViewResolver 다음과 같이 두 개의 프로퍼티를 입력받는다. prefix - Controller가 리턴한 뷰 이름 앞에 붙을 접두어 suffix - Controller가 리턴한 뷰 이름 뒤에 붙을 확장자 \u003cproperty name=\"prefix\" value=\"/view/\" /\u003e \u003cproperty name=\"suffix\" value=\".jsp\" /\u003e ExampleController가 example을 return했다고 하면 View는 /view/example.jsp가 된다. VelocityViewResolver Velocity 템플릿을 사용한다. ","date":"2019-12-11","objectID":"/springmvcbasic/:2:3","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"Controller 구현 Controller를 구현하는 방법응ㄴ Controller인터페이스를 implements하는 것이지만 몇가지 기능을 구현하고 있는 클래스만 상속 받아서 구현하는게 일반적이다. Controller interface org.springframework.web.servlet.mvc.Controller public interface Controller { ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception } Controller를 구현하는 방법은 handleRequest() 메소드에 맞게 구현하는 것이다. ","date":"2019-12-11","objectID":"/springmvcbasic/:2:4","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"View의 구현 View에서는 ModelAndView 객체에 저장한 객체를 사용할 수 있다. 다음과 같이 ModelAndView에 데이터를 저장했다고 보자. protected ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object command, BindException errors) throws Exception { Example example = (Example) examples; ModelAndView mv = new ModelAndView(\"hello\"); mv.addObject(\"hello\", example); return mv; } 이 경우 JSP에서 다음과 같이 사용할 수 있다. example: ${example.hello} \u003c% Example cmd = (Example) request.getAttribute(\"hello\"); %\u003e 뭐 워낙 예전 방식이라 전자정부프레임워크 쓸때 이런식으로 했던 것으로 기억하는데… Spring도 업데이트 되면서 많은 것이 바뀌었으므로 천천히 업데이트 해야겠다. ","date":"2019-12-11","objectID":"/springmvcbasic/:2:5","tags":["java","spring"],"title":"Spring MVC basic","uri":"/springmvcbasic/"},{"categories":["java","spring"],"content":"이전글에서 잠깐 보았던 것을 전체적으로 봐보자. Spring MVCSpring MVC \" Spring MVC 출처 : https://lazymankook.tistory.com/69 ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:0:0","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"DispatcherServlet등록 Dispatcher Servlet이 클라이언트의 모든 요청을 받는다. DispatcherServlet을 사용하기 위해 어딘가에 객체로 등록되어 있을 것이다. 요즘엔 사용안하는 web.xml파일에 정보들을 적어서 사용했었다. 여기에 DispatcherServlet을 url패턴과 함께 작성하여 DispatcherServlet은 FontController의 역할을 하기 때문에 모든 url을 DispatcherServlet을 거치도록 설정 해놓을 것이다. Servlet 3.0 이상 부터 WebApplicationInitializer구현 또는 AbstractAnnotationConfigDispatcherServletInitializer 상속으로 web.xml에서 하던 설정을 java config로써 코드로 작성할 수 있다. java config를 통해 설정해 classpath내에 두면, SpringServletContainerInitializer가 서블릿을 초기화 할 때 감지한다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:1:0","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"DispatcherServlet 초기화 DispatcherServlet이 생성 후 초기화 될때, initStrategies(ApplicationContext context) 메소드를 실행해 DeispatcherServlet에 관련된 Bean들을 초기화한다. initStrategiesinitStrategies \" initStrategies 초기화시 Bean이 존재하는지 찾고 없으면 default 설정에 따라 Bean을 생성하거나 아무 작업을 안 시킬 수도 있다. 이제 DispatcherServlet이 가지는 전략 인터페이스를 보자. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:0","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"MultipartResolver multipart 파일 업로드 요청에 대해 해선하여 변환하는 전략 인터페이스. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:1","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"LocaleResolver 웹 기반의 locale 결정 전략 인터페이스. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:2","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"ThemeResolver 웹 기반의 Theme 결정 전략 인터페이스. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:3","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"HandlerMapping Request와 핸들러 객체를 Mapping을 정의하는 인터페이스이다. 일반적으로 @RequestMapping을 이용한 핸들러 Mapping 전략이다. default로 선언되는 Bean은 BeanNameUrlHandlerMapping, DefaultAnnotationHandlerMapping 이다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:4","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"HandlerAdapter Spring MVC의 Service Provider Interface. Controller의 구현 전략 인터페이스 이다. HandlerMapping이 핸들러를 찾아주면, HandlerAdapter가 해당 컨트롤러의 핸들러에 전달하기 위한 Adapter라고 보면 됨. HandlerAdapter 인터페이스를 보면 modelAndView를 리턴하는 handle이라는 메소드가 있다. ModelAndViewModelAndView \" ModelAndView Spring의 Controller 종류는 4가지가 있다. Servlet, Controller, HttpRequestHandler, @Controller 어노테이션. 각 Controller를 DispatcherServlet에 연결시켜주는 Adapter가 하나씩 있어야 하므로 handlerAdapter도 4개가 있어야한다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:5","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"HandlerExceptionResolver Handler Mapping이나 Controller 실행 도중 발생한 예외를 다루는 인터페이스 이다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:6","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"RequestToViewNameTranslator Controller에서 View를 지정하지 않았을때, Request를 참고하여 내부적으로 View 이름을 생성해준다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:7","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"ViewResolver Controller에서 Request를 처리하고 생성하는 결과물에 대한 View 처리 전략 인터페이스. Controller가 return \"index.jsp\"; 와 같이 이름만 남겨줘도 ViewResolver가 ModelAndView객체로 변경해서 돌려준다. prefix와 suffix를 좋바하는 로직도 이 안에 포함되어 있다. ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:8","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["java","spring"],"content":"FlashmapManager FlashMap객체를 retrieve \u0026 save 하는 전략 인터페이스 이다. 위의 모든 전략 인터페이스의 처리 과정은 Bean에 이뤄진다. 또한 커스터마이징이 필요한 부분은 인터페이스를 구현해 Spring Bean으로 등록하면 된다. 등록하면 DispatcherServlet이 초기화 될 때 Bean이 주입된다. Default Bean은 @EnableMvc를 사용할 때 생성된다. default Bean 설정은 spring-mvc 프로젝트내 DispatcherServlet.properties에 선언되어 있다. 요청을 받았을 때 순서대로 보면 Handler Mapping DispatcherServlet이 요청을 받으면 처음 거치는 곳인데 요청받은 URL에 해당하는 Controller를 찾아준다. HandlerMapping을 따로 설정하지 않으면 RequestMappingHandler(@RequestMapping)와 SimpleUrlHandlerMapping(URI 경로 패턴)이 기본 구현으로 된다. Handler Adapter HandlerMapping으로 결정된 Controller의 메서드를 호출 하는 역할을 한다. DispatcherServlet이 Handler가 어떤 방식으로 구현되어 있는지 신경 쓰지 않게 하기 위해서다. Controller, Service, Repository DB의 Data Access를 통해 나온 데이터를 Model에 저장하고 View를 정하여 View Resolver에 전달한다. View Resolver 어떤 종류의 view를 사용할지 선택한다. 선택된 view는 데이터를 이용해 페이지를 완성하고 클라이언트에 전달한다. 참고 https://lazymankook.tistory.com/69 ","date":"2019-12-11","objectID":"/springmvcdispatcherservlet/:2:9","tags":["java","spring"],"title":"Spring MVC와 DespatcherServlet","uri":"/springmvcdispatcherservlet/"},{"categories":["algorithm"],"content":"python3에서 Linked List에 대한 여러 연산들을 정리 해놓자. 안하면 자꾸 까먹어서.. ","date":"2019-11-21","objectID":"/linkedlist/:0:0","tags":["algorithm"],"title":"Python3에서 Linked List에 관한 여러 연산들","uri":"/linkedlist/"},{"categories":["algorithm"],"content":"Linked List 절반 구하기 class ListNode: def __init__(self, x): self.val = x self.next = None class LinkedList: def halfOfLinkedList(self, head: ListNode) -\u003e ListNode: fast = slow = head # slow is half of head while fast and fast.next: fast = fast.next.next slow = slow.next return slow 예시 head ListNode{val: 1, next: ListNode{val: 2, next: ListNode{val: 2, next: ListNode{val: 1, next: None}}}} slow ListNode{val: 2, next: ListNode{val: 1, next: None}} ","date":"2019-11-21","objectID":"/linkedlist/:1:0","tags":["algorithm"],"title":"Python3에서 Linked List에 관한 여러 연산들","uri":"/linkedlist/"},{"categories":["algorithm"],"content":"Linked List 역순 class ListNode: def __init__(self, x): self.val = x self.next = None class LinkedList: def ReverseOfLinkedList(self, head: ListNode) -\u003e ListNode: node = None while head: nxt = head.next head.next = node node = head head = nxt return node 예시 head ListNode{val: 2, next: ListNode{val: 1, next: None}} node ListNode{val: 1, next: ListNode{val: 2, next: None}} ","date":"2019-11-21","objectID":"/linkedlist/:2:0","tags":["algorithm"],"title":"Python3에서 Linked List에 관한 여러 연산들","uri":"/linkedlist/"},{"categories":["algorithm"],"content":"Class Memory 공유 class ListNode: def __init__(self, x): self.val = x self.next = None class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -\u003e ListNode: dummy = cur = ListNode(0) print(id(dummy)) # 140375281581984 print(id(cur)) # 140375281581984 # 주소값 동일 따라서 dummy변수에 대해서 연산을 안해도 데이터가 들어가 있는 걸 볼 수 있음. while l1 and l2: if l1.val \u003c l2.val: cur.next = ListNode(l1.val) # cur.next = l1 l1 = l1.next else: cur.next = ListNode(l2.val) # cur.next = l2 l2 = l2.next cur = cur.next # 남은거 정리 cur.next = l1 or l2 return dummy.next ","date":"2019-11-21","objectID":"/linkedlist/:3:0","tags":["algorithm"],"title":"Python3에서 Linked List에 관한 여러 연산들","uri":"/linkedlist/"},{"categories":["os"],"content":"Application memory 구조의 Heap과 Stack을 살펴보자","date":"2019-11-20","objectID":"/memory/","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"Application memory 구조의 Heap과 Stack을 살펴보자. memory 이전글에서 Process/Thread를 얘기할때 Memory에 대한 언급을 잠깐 했다. 여기서 사용하는 Memory는 Virtual Memory라고.. 모든 프로세스가 실제의 메모리를 사용하게 되면 용량 문제가 발생하기 때문에 페이징 기법과 가상메모리를 사용한다. 흔히 코드에서 출력하는 주소값들은 가상주소이다. ","date":"2019-11-20","objectID":"/memory/:0:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"1 페이징기법 계산기의 GUI가 있다고하자. +, -, *, % 등의 연산을 할때 이는 페이징 파일에 저장된다. 페이징 파일에 아직 동작하지 않은 주소를 저장 시켜서 잠시 가지고 있는것.. 이와같이 페이징 파일을 가지고 있기에 모든 프로세스가 더 적게 메모리를 사용하는 것이다. ","date":"2019-11-20","objectID":"/memory/:1:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2 가상메모리 프로그램을 동작하면 가상 메모리 공간이 생성된다. 그 메모리 공간은 상위, 하위 메모리로 나눠진다. ","date":"2019-11-20","objectID":"/memory/:2:0","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2.2 상위 메모리 Stack의 메모리 공간이 할당된다. 지역변수, 리턴값등을 저장.. 값이 싸다. 이유는 할당과 해제는 CPU 명령어 2개로 끝난다고 한다. (할당, 해제) 코드로 예를 들면 함수가 종료되면 함수에 있던 모든 변수가 Stack에서 Pop된다고 생각하면 된다. 함수가 사라지면 외부에서 참조 못하는 것처럼 여기서 학부시절에 그렇게 많이 들은 Stack Overflow, Underflow등이 일어나게 되는 것이다. ","date":"2019-11-20","objectID":"/memory/:2:1","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"2.3 하위 메모리 Heap이 생성된다. 프로그램이 동작할 때 필요한 데이터 정보를 임시로 저장. heap할당은 비싸다. C에서 보면 malloc(), free()등으로 조작가능. Go, Python같은 언어들은 자동으로 GC가 작동하기 때문에 개발자가 신경 안써도 되는 경우도 있다. 하지만 코드를 작성할때 신경써야하는 부분이 있을 수도 있는데 Go를 예로 들어보면 2.3.1 Go에서 heap Go에서 포인터를 사용하면 대부분 heap에 할당 된다고 한다. 가능한 안쓰는게 좋겠지..? 포인터를 사용하지 않고 값을 복사하는 것이 memory를 작게써 CPU 캐시 적중률이 오른다고한다. 또한 포인터를 포함하지 않는 메모리 영역은 GC가 생략할 수 있다. 반대로 포인터가 있으면 GC가 스캔 할 필요가 있다. 이렇게 GC가 돌면 메모리 상에 흩어진 영역을 계속 탐색하기 때문에 무거워 지기 때문에 개발자는 조금이라도 가볍게 개발하기 위해서는 알아야 하는 개념이다. ","date":"2019-11-20","objectID":"/memory/:2:2","tags":["os","memory"],"title":"메모리 영역 Heap, Stack 살펴보기","uri":"/memory/"},{"categories":["os"],"content":"갑자기 컴퓨터 구조에 대해서 정리해보고 싶어서 머리속에 있는것을 더해 끄적여본다.","date":"2019-11-06","objectID":"/computerstructure/","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"갑자기 컴퓨터 구조에 대해서 정리해보고 싶어서 머리속에 있는것을 더해 끄적여본다. 대학교에서 전공과목으로 컴퓨터 구조를 배웠지만 생각이 나지 않고 뭘배웠는지 기억을 못한다… 그나마 남아있는것을 정리하려고한다. ㅜㅜ.. 컴퓨터는 가장 크게 H/W, S/W로 나눌 수 있고 이를 좀더 자세히 나눠서 보자 ","date":"2019-11-06","objectID":"/computerstructure/:0:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"H/W 말그대로 하드웨어 물리적인 컴퓨터 자원을 말한다. CPU, GPU, RAM등등 본체에 꼽혀있는것들.. ","date":"2019-11-06","objectID":"/computerstructure/:1:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"S/W 소프트웨어는 물리적 반대로 논리적(Logical)하다 라고 많이 불리고 이는 IT쪽에서 Virtual과 비슷한 의미를 가진다. 더 작게 나눠보자 ","date":"2019-11-06","objectID":"/computerstructure/:2:0","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"Kernel 커널은 운영체제의 핵심 부분으로서, 운영체제의 다른 부분 및 응용 프로그램 수행에 필요한 여러가지 서비스를 제공한다. 라고 위키백과에 나와있다. 간단히 말하면 이 커널 영역에서 흔히 하드웨어 장치에 필요한 Driver가 있어야 하고, 그에 따른 구성요소가 포함된다고 보면 될 것 같다. 예를들어 Disk라는 물리적인 저장공간을 가지고있고 이에 따른 Driver가 있고 이 Driver는 File System(NTFS) 구성요소와 연결 되고, File System은 우리가 흔히 쓰는 File과 연결되어 처리가 된다. 우리가 컴퓨터로 파일을 옮기고 삭제하는등의 행동을 취했을때 내부적으로는 File -\u003e File System -\u003e Driver -\u003e Disk 또는 SSD등 이렇게 이뤄지는 것이당 File system부터 Disk까지의 행동을 커널 영역에서 다룬다. 음.. 다른 예를 하나더 들어보자. 제목에 socket을 넣어서 잠깐 다뤄보면 우리가 흔히 쓰는 Socket도 결국 위에서 말한 File을 이용한다. 이게 무슨? 소리징하고 처음엔 생각했다. 그 이유는 유닉스의 리소스 구성은 파일로 이뤄져있다. 파일, 하드웨어, 파이프, 소켓등등 밑의 사진을 보면 하드웨어들이 file로 저장되어 있는것을 볼 수 있다. HwfileHwfile \" Hwfile 다시 Socket으로 돌아와서 커널 영역에서 뭘하는지 보면, Socket(file) -\u003e Protocol(TCP/IP) -\u003e Driver -\u003e NIC 이렇게 될듯 하다. NIC는 하드웨어로 랜카드이다. TMI로 NIC -\u003e L2 -\u003e R -\u003e internet이 되겠지요? Kernel 영역에서 하는 일은 위의 예시들로 알 수 있을 것이다. 백신 프로그램을 잠깐 보면 어떻게 동작하는지 짐작 할 수 있을 것 같은데 아까 위에서 File -\u003e 구성요소 -\u003e Driver이렇게 흘러간다고 얘기를 많이 했다. 그 중간에 Filter라는 것이 존재하는데 이 Filter는 구성요서 앞/뒤 쯤 올 수 있다. 이 Filter를 백신 업체에서 제공 해주면 Filtering이 되겠거니 싶다. 물론 Filter를 이용하다 보니 속도는 느려 질 수 있을 것이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:1","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"User User영역은 아까 많이 언급한 File이 속한다. 또한 백신 얘기할때 anti virus가 속할 수 있고 이 anti virus는 아까 Filter와 주거니 받거니 할 것이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:2","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os"],"content":"Process/Thread 가장 많이 접한 Process와 Thread이다. 이 둘을 간단히 비교해보면 Process안에 Thread가 있고 Thread는 프로세스 내에서 실행되는 여러 흐름의 단위라고 구글에 검색하면 많이 나올 것이다. 엄청 간단히 예시를 들면 집(house)가 Process 그 안의 구성원 엄마, 아빠 등이 Thread라고 생각하면 될 것 같다. Process끼리는 독립된 Memory를 가져서 서로 데이터 공유가 안된다. 물론 Thread는 Process 내에서 동작하기 때문에 Process내의 Thread끼리는 같은 메모리 공간을 이용한다. 따라서 동기화가 중요 하다 ! 실생활의 예를 들어보면 집(process)에 내가(thread) 화장실을 사용하고 있을때 문을 잠그고 이용하게 되면 다른 가족구성원(thread)는 화장실을 사용하지 못한다. 만약, 문을 안잠그고 사용하다가 큰일을 보는 중에 문을 열어버린다던지 그러면 안되는 것 처럼 이해 하면 될 것 같다. 여기서 화장실 문을 잠그는 행위는 Lock을 거는 행위이고 이는 mutex를 이용해 Lock, UnLock을 하는 것과 같다고 보면 될 듯 하다. 여기서 말한 Memory는 아 ~~ 까 말한 S/W는 Virtual == Logical 을 잠깐 인용하자면 Process에 할당된 Memory는 virtual memory이다. ","date":"2019-11-06","objectID":"/computerstructure/:2:3","tags":["os","linux","socket"],"title":"Computer structure and socket","uri":"/computerstructure/"},{"categories":["os","linux"],"content":"linux에서 container가 내부적으로 어떻게 동작하는지 알아보자.","date":"2019-10-27","objectID":"/namespace/","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"linux에서 container가 내부적으로 어떻게 동작하는지 알아보자. namespace는 k8s에서 먼저 많이 접해봤는데, 각각 별개의 독립된 공간인 것처럼 환경을 제공하는 가상화 기술이다. container기술이 이를 통해 만들어 졌다. Linux namespace는 6가지 종류가 있다. UTS IPC PID NS (FS) NET USER ","date":"2019-10-27","objectID":"/namespace/:0:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"UTS 독립적인 Hostname할당 현재 hostname은 다음과 같이 알 수 있다. uname -n unshare로 namespace를 만드는데 -u를 주면 UTS가 부모 프로세스와 공유가 안된 상태로 생성된다. unshare -u /bin/bash ","date":"2019-10-27","objectID":"/namespace/:1:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"IPC inter process communication IPC는 프로세스간 서로 데이터를 주고 받는 경로를 뜻한다. IPC는 Signal, Socket, pipe, semaphore, file locking, mutex등이 있다. 또한 프로세스간 데이터 교환 및 프로세스와 쓰레드간의 작업을 동기화 하는기능을 제공한다. ","date":"2019-10-27","objectID":"/namespace/:2:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"PID Linux에서 실행되는 모든 process들은 각각에 고유한 PID가 부여된다. kernel에서는 이 process들을 Tree 형태로 관리한다. pstreepstree \" pstree 최상위 process는 init process라 하고 PID를 “1\"을 가진다. PID namespace에서 가지는 특징이 이 PID 1프로세스를 독립적으로 추가 할당해 주는 것이다. PID를 1받았다고 해서 init process가 되는건 아니고 init process와 비슷하게 수행된다. 이는 OS에서 systemd 프로세스 뿐만 아니라 PID 충돌없이 실행 가능하게 해준다. ","date":"2019-10-27","objectID":"/namespace/:3:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"NS (FS) process Tree의 PID 1프로세스를 분기 시켜도 FS는 그대로 공유하고 있을것이다. 이유는 /proc를 아직 공유하고 있기 때문이다. 새로운 폴더를 만들고 /proc폴더를 mount시켜주면 독립적으로 FS를 관리할 수 있을 것이다. ","date":"2019-10-27","objectID":"/namespace/:4:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"NET namespace간 Network 충돌 방지한다. $ ip link list 명령어로 현재 서버의 interface 정보를 확인 할 수 있다. $ ip netns add {namespace} 새로운 namespace를 생성하고 다음과 같이 확인 할 수있다. $ ip netns 독립된 namespace간 내부 통신도 가능한데 자세한건 다음 글에서 작성한다. ","date":"2019-10-27","objectID":"/namespace/:5:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["os","linux"],"content":"USER 프로세스가 namespace 내부와 default namespace간에 각기 다른 사용자 및 그룹 ID를 가질 수 있게한다. ","date":"2019-10-27","objectID":"/namespace/:6:0","tags":["os","linux","namespace"],"title":"Introduction to Linux Namespaces","uri":"/namespace/"},{"categories":["grpc"],"content":"이전글에서 작성 한 go server를 가지고 python 에서 client로 접속해보자. ","date":"2019-09-09","objectID":"/grpc2/:0:0","tags":["grpc"],"title":"Building microservices in Go and Python using gRPC","uri":"/grpc2/"},{"categories":["grpc"],"content":"코드 ","date":"2019-09-09","objectID":"/grpc2/:1:0","tags":["grpc"],"title":"Building microservices in Go and Python using gRPC","uri":"/grpc2/"},{"categories":["grpc"],"content":"server 이전글에서 작성한 go server 그대로 이용한다. https://github.com/jaejin1/gRPC/blob/master/go/server/main.go ","date":"2019-09-09","objectID":"/grpc2/:1:1","tags":["grpc"],"title":"Building microservices in Go and Python using gRPC","uri":"/grpc2/"},{"categories":["grpc"],"content":"client client이기 때문에 간단하게 python으로 작성해보자. 먼저 proto file을 python 코드로 변환 한다. 물론 전에 사용한 proto file을 그대로 사용한다. $ python -m grpc_tools.protoc -I./ --python_out=. --grpc_python_out=. ./hi.proto proto file을 compile하게 되면 hi_pb2_grpc.py, hi_pb2.py 2개의 파일이 생성된다. 이 2파일을 다음 작성할 python코드에 import 시킨 뒤 사용한다. main.py \"\"\"The Python implementation of the gRPC route guide client.\"\"\" from __future__ import print_function import random import logging import grpc import hi_pb2 import hi_pb2_grpc def run(): with grpc.insecure_channel('localhost:50051') as channel: stub = hi_pb2_grpc.HiStub(channel) ## JaejinHi response = stub.JaejinHi(hi_pb2.HiRequest(name='test test test test !!')) print(\"Greeter client received: \" + response.message) if __name__ == '__main__': logging.basicConfig() run() ","date":"2019-09-09","objectID":"/grpc2/:1:2","tags":["grpc"],"title":"Building microservices in Go and Python using gRPC","uri":"/grpc2/"},{"categories":["grpc"],"content":"실행 먼저 Server를 실행한다. $ go run main.go 2019/09/08 22:51:48 grpc server start at port :50051 python으로 작성한 client를 실행해보자. $ python main.py Greeter client received: Hi jaejin test 다시 server를 확인하면 데이터가 온 것을 볼 수 있다. $ go run main.go 2019/09/08 22:51:48 grpc server start at port :50051 2019/09/08 22:52:22 Received: Hi jaejin test test test test !! 2019/09/08 22:52:22 Send back to client: Hi jaejin test ","date":"2019-09-09","objectID":"/grpc2/:2:0","tags":["grpc"],"title":"Building microservices in Go and Python using gRPC","uri":"/grpc2/"},{"categories":["grpc"],"content":"gRPC는 원경의 Client가 Server 단의 함수를 로컬 함수를 호출하듯 부를 수 있게 해준다. 이때 메시지를 보내고 받는데는 protocol buffers를 사용하고 HTTP/2 기반의 Streaming을 지원하며 REST 대비 빠른 성능을 지원한다. gRPC의 특징 중 하나는 Java, Ruby, Node, Python, Go등 과 같은 프로그래밍 언어를 지원하고 MSA (마이크로 서비스 아키텍쳐)와 조합이 매력적이다. ","date":"2019-09-08","objectID":"/grpc1/:0:0","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"Google protocol buffers 구글에서 개발하고 오픈소스로 공개한 직렬화 데이터 구조이다. 다양한 언어를 지원하고 직렬화 속도가 빠르고 파일의 크기가 작다고 한다. ","date":"2019-09-08","objectID":"/grpc1/:1:0","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"사용하기 protocol buffers는 proto file이라는 형태로 정의하는데, protocol buffers는 한가지 언어가 아닌 다양한 언어를 지원하기 때문에 특정언어에 종속성이 없는 형태로 데이터 타입을 정의하는데 이 파일이 proto file이다. proto file을 컴파일 하면 각 언어에 맞게 클래스 파일을 생성해준다. 예시는 밑의 예제에서 살펴보자. ","date":"2019-09-08","objectID":"/grpc1/:1:1","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"설치 golang 1.6 이상 install gRPC $ go get -u google.golang.org/grpc install Protocol Buffers v3 install protoc plugin for go $ go get -u github.com/golang/protobuf/protoc-gen-go $ export PATH=$PATH:$GOPATH/bin ","date":"2019-09-08","objectID":"/grpc1/:2:0","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"Smaple ","date":"2019-09-08","objectID":"/grpc1/:3:0","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"참고 사이트 https://grpc.io/docs/quickstart/go/ https://jusths.tistory.com/127 ","date":"2019-09-08","objectID":"/grpc1/:3:1","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"sample code https://github.com/jaejin1/gRPC/tree/master/go ","date":"2019-09-08","objectID":"/grpc1/:3:2","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"proto file hi.proto syntax = \"proto3\";package higrpc;service Hi { rpc SayHi (HiRequest) returns (HiReply) {} rpc CountHi (HiRequest) returns (HiReply) {} rpc JaejinHi (HiRequest) returns (HiReply) {}}message HiRequest { string name = 1;}message HiReply { string message = 1;} compile hi.proto $ protoc hi.proto --go_out=plugins=grpc:. compile을 하게 되면 hi.pb.go 파일이 생성될 것이다. 이 파일을 server, client 에서 import 시킨다. ","date":"2019-09-08","objectID":"/grpc1/:3:3","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"Server package main import ( \"context\" \"log\" \"net\" \"strconv\" pb \"github.com/jaejin1/grpc/pb\" \"google.golang.org/grpc\" ) const ( port = \":50051\" ) type server struct{} func (s *server) SayHi(ctx context.Context, in *pb.HiRequest) (*pb.HiReply, error) { log.Printf(\"Received: %v\", in.Name) replyMsg := \"Hi \" + in.Name log.Printf(\"Send back to client: %s\", replyMsg) return \u0026pb.HiReply{Message: replyMsg}, nil } func (s *server) CountHi(ctx context.Context, in *pb.HiRequest) (*pb.HiReply, error) { log.Printf(\"Received: %v, length: %d\", in.Name, len(in.Name)) replyMsg := \"Count \" + in.Name + \" length: \" + strconv.Itoa(len(in.Name)) log.Printf(\"Send back to client: %s\", replyMsg) return \u0026pb.HiReply{Message: replyMsg}, nil } func (s *server) JaejinHi(ctx context.Context, in *pb.HiRequest) (*pb.HiReply, error) { log.Printf(\"Received: Hi jaejin %v\", in.Name) replyMsg := \"Hi jaejin test\" log.Printf(\"Send back to client: %s\", replyMsg) return \u0026pb.HiReply{Message: replyMsg}, nil } func main() { log.Printf(\"grpc server start at port %s\", port) l, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterHiServer(s, \u0026server{}) if err := s.Serve(l); err != nil { log.Fatalf(\"fail to serve: %v\", err) } } ","date":"2019-09-08","objectID":"/grpc1/:3:4","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"client package main import ( \"context\" \"flag\" \"log\" \"time\" pb \"github.com/jaejin1/grpc/pb\" \"google.golang.org/grpc\" ) const ( serverAddr = \"localhost:50051\" defaultName = \"world\" ) func main() { conn, err := grpc.Dial(serverAddr, grpc.WithInsecure()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := pb.NewHiClient(conn) name := defaultName flag.Parse() if flag.NArg() \u003e 0 { name = flag.Arg(0) } ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHi(ctx, \u0026pb.HiRequest{Name: name}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } log.Printf(\"Greeting from the Server: %s\", r.Message) r, err = c.CountHi(ctx, \u0026pb.HiRequest{Name: name}) if err != nil { log.Fatalf(\"could not count: %v\", err) } log.Printf(\"Count from the Server: %s\", r.Message) r, err = c.JaejinHi(ctx, \u0026pb.HiRequest{Name: name}) if err != nil { log.Fatalf(\"Error !!: %v\", err) } log.Printf(\"Hi jaejin: %s\", r.Message) } ","date":"2019-09-08","objectID":"/grpc1/:3:5","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"Test ","date":"2019-09-08","objectID":"/grpc1/:4:0","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"server 실행 $ go run server/main.go 2019/09/08 15:06:17 grpc server start at port :50051 ","date":"2019-09-08","objectID":"/grpc1/:4:1","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"client 실행 $ go run client/main.go 2019/09/08 15:06:44 Greeting from the Server: Hi world 2019/09/08 15:06:44 Count from the Server: Count world length: 5 2019/09/08 15:06:44 Hi jaejin: Hi jaejin test ","date":"2019-09-08","objectID":"/grpc1/:4:2","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["grpc"],"content":"server 확인 $ go run server/main.go 2019/09/08 15:06:17 grpc server start at port :50051 2019/09/08 15:06:44 Received: world 2019/09/08 15:06:44 Send back to client: Hi world 2019/09/08 15:06:44 Received: world, length: 5 2019/09/08 15:06:44 Send back to client: Count world length: 5 2019/09/08 15:06:44 Received: Hi jaejin world 2019/09/08 15:06:44 Send back to client: Hi jaejin test 다음은 gRPC를 가지고 MSA 특성을 살려보기 위해 go server를 띄워놓고 python client로 접속을 해보려고 한다. ","date":"2019-09-08","objectID":"/grpc1/:4:3","tags":["grpc"],"title":"Introduction to gRPC","uri":"/grpc1/"},{"categories":["graphql"],"content":"서버리스 GraphQL워크샵 내용을 참조했습니다. 앞의 글은 개념만 다뤘다면 이번 글에서는 실질적으로 사용을 함에 있어 필요한 내용들을 다룬다. ","date":"2019-09-07","objectID":"/graphql2/:0:0","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"소개 GraphQL은 페이스북에서 만든 어플리케이션 레이어 쿼리 언어이다. 기존의 웹 혹은 모바일의 어플리케이션의 API를 구현 할 때는 보통 REST API를 사용한다. 기존의 REST API를 사용하게 되면, 클라이언트에서 기능이 필요할때마다 그때 그때 필요한 API를 만들어 줘야 했다. 예를들어 어플리케이션에 Account라는 모델이 있고, /accounts라는 endpoint가 있따고 하면 GET /accounts GET /accounts/{id} … { \"account\": { \"id\": \"1\", \"email\": \"jaejin@gmail.com\", \"friends\": [ \"2\", \"3\" ], \"first_name\": \"Jaejin\", \"last_name\": \"Lee\" } } 이렇게 가져올텐데 만일 friends목록을 가져오려면 또 다른 API를 만들어야 할 것이다. GET /accounts/1/?include_friend=username or GET /include_friend/1 이렇게 계속 API를 만들다 보면 수 많은 endpoint가 생길 것이다. ","date":"2019-09-07","objectID":"/graphql2/:1:0","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"쿼리 query { account(id: \"1\") { username email firstName lastName friends { firstName username } } } ","date":"2019-09-07","objectID":"/graphql2/:1:1","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"결과 { \"data\": { \"account\": { \"username\": \"velopert\", \"email\": \"public.velopert@gmail.com\", \"firstName\": \"Minjun\", \"lastName\": \"Kim\", \"friends\": [ { \"firstName\": \"Jayna\", \"username\": \"jn4kim\" }, { \"firstName\": \"Abet\", \"username\": \"abet\" } ] } } } GraphQL은 다음과 같이 query로 만들어 서버에 전달하면 원하는 데이터를 가져올 수 있는 것이다. ","date":"2019-09-07","objectID":"/graphql2/:1:2","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"문제? GraphQL은 REST API의 새로운 관점의 스펙이다. 따라서 어느 언어를 사용하든 GraphQL의 스펙에 맞는 코드를 작성해야한다. javascript go python … 기존 REST로 구현된 API는 GET /accounts, POST /accounts, DELETE /accounts/:accountsID등 과 같은 방식으로 개발이 된다. 이 구조를 GraphQL로 바꿀 수 있지만 GraphQL은 단일 endpoint를 권장한다. 따라서 모든 요청을 /graghql에서 처리 해야한다. 왜냐하면 GraphQL은 url이 아니라 Query를 통해 표현하기 때문에. 기존 REST API와 같이 사용하려면 기존 REST API들은 놔두고 새로운 endpoint /graphql을 작성하면 된다. ","date":"2019-09-07","objectID":"/graphql2/:2:0","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"Content-Type: “application/graphql” GraphQL은 기본적으로 Content-Type: \"application/json\"을 사용한다. 물론 Content-Type: \"application/graphql\"도 사용가능하다. 하지만 두 요청은 차이점이 있다. Content-Type: \"application/json\" request body는 다음 형식을 따라야한다. { \"query\": \"...\", \"operationName\": \"...\", \"variables\": { \"myVariable\": \"someValue\", ... } } Content-Type: \"application/graphql\" Query요청을 위해 축약형 body를 사용할 수 있게 해준다. Content-Type: “application/json” 의 query필드 내부에 있는 값을 그대로 사용하는 것이다. { account(id: \"1\") { id email ... } } 이러한 형식 차이 때문에 Content-Type: \"application/graphql\" 을 사용하기 보다는 Content-Type: \"application/json\"로 통일 하여 사용 할 것이다. ","date":"2019-09-07","objectID":"/graphql2/:2:1","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"Example go로 간단한 GraphQL 서버를 만들어보자. ","date":"2019-09-07","objectID":"/graphql2/:3:0","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"Code main.go package main import ( \"encoding/json\" \"fmt\" \"io/ioutil\" \"net/http\" \"github.com/graphql-go/graphql\" ) type user struct { ID string `json:\"id\"` Name string `json:\"name\"` } var data map[string]user /* Create User object type with fields \"id\" and \"name\" by using GraphQLObjectTypeConfig: - Name: name of object type - Fields: a map of fields by using GraphQLFields Setup type of field use GraphQLFieldConfig */ var userType = graphql.NewObject( graphql.ObjectConfig{ Name: \"User\", Fields: graphql.Fields{ \"id\": \u0026graphql.Field{ Type: graphql.String, }, \"name\": \u0026graphql.Field{ Type: graphql.String, }, }, }, ) /* Create Query object type with fields \"user\" has type [userType] by using GraphQLObjectTypeConfig: - Name: name of object type - Fields: a map of fields by using GraphQLFields Setup type of field use GraphQLFieldConfig to define: - Type: type of field - Args: arguments to query with current field - Resolve: function to query data using params from [Args] and return value with current type */ var queryType = graphql.NewObject( graphql.ObjectConfig{ Name: \"Query\", Fields: graphql.Fields{ \"user\": \u0026graphql.Field{ Type: userType, Args: graphql.FieldConfigArgument{ \"id\": \u0026graphql.ArgumentConfig{ Type: graphql.String, }, }, Resolve: func(p graphql.ResolveParams) (interface{}, error) { idQuery, isOK := p.Args[\"id\"].(string) if isOK { return data[idQuery], nil } return nil, nil }, }, }, }) var schema, _ = graphql.NewSchema( graphql.SchemaConfig{ Query: queryType, }, ) func executeQuery(query string, schema graphql.Schema) *graphql.Result { result := graphql.Do(graphql.Params{ Schema: schema, RequestString: query, }) if len(result.Errors) \u003e 0 { fmt.Printf(\"wrong result, unexpected errors: %v\", result.Errors) } return result } func main() { _ = importJSONDataFromFile(\"data.json\", \u0026data) http.HandleFunc(\"/graphql\", func(w http.ResponseWriter, r *http.Request) { result := executeQuery(r.URL.Query().Get(\"query\"), schema) json.NewEncoder(w).Encode(result) }) fmt.Println(\"Now server is running on port 8080\") fmt.Println(\"Test with Get : curl -g 'http://localhost:8080/graphql?query={user(id:\\\"1\\\"){name}}'\") http.ListenAndServe(\":8080\", nil) } //Helper function to import json from file to map func importJSONDataFromFile(fileName string, result interface{}) (isOK bool) { isOK = true content, err := ioutil.ReadFile(fileName) if err != nil { fmt.Print(\"Error:\", err) isOK = false } err = json.Unmarshal(content, result) if err != nil { isOK = false fmt.Print(\"Error:\", err) } return } data.json { \"1\": { \"id\": \"1\", \"name\": \"Dan\" }, \"2\": { \"id\": \"2\", \"name\": \"Lee\" }, \"3\": { \"id\": \"3\", \"name\": \"Nick\" } } ","date":"2019-09-07","objectID":"/graphql2/:3:1","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"Run $ go run main.go Now server is running on port 8080 Test with Get : curl -g 'http://localhost:8080/graphql?query={user(id:\"1\"){name}}' $ curl -g 'http://localhost:8080/graphql?query={user(id:\"1\"){name}}' {\"data\":{\"user\":{\"name\":\"Dan\"}}} ","date":"2019-09-07","objectID":"/graphql2/:3:2","tags":["graphql"],"title":"Explore GraphQL2","uri":"/graphql2/"},{"categories":["graphql"],"content":"서버리스 GraphQL워크샵 내용을 참조했습니다. tonyfromundefined/serverless-graphql-workshop ","date":"2019-09-04","objectID":"/graphql1/:0:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"GraphQL이란? GraphQL은 API 설계와 요청을 구조화하는 일련의 약속이다. GraphQL을 통해 우리는 데이터에 기반해 API를 디자인 할 수 있고, 클라이언트에서는 정해진 쿼리 언어를 통해 API를 체계적으로 사용할 수 있다. 클라이언트는 서버에 필요한 자원만 요청. 클라이언트는 서버가 가진 많은 자원을 단 한 번의 요청으로 가져 올 수 있다. 타입 시스템을 통해 개발 생산성을 비약적으로 향상 시킬 수 있다. 제공되는 기본 개발자 도구를 통해 API를 쉽게 문서화하고 검색할 수 있다. 버전 관리 없이 API를 점진적으로 진화 시킬 수 있다. ","date":"2019-09-04","objectID":"/graphql1/:1:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"GraphQL 타입 시스템과 쿼리, 뮤테이션 타입 기본 타입은 다음 5가지가 존재한다. Int: 부호가 있는 32비트 정수. Float: 부호가 있는 부동소수점 값. String : UTF-8 문자열. Boolean : true 또는 false ID : ID 스칼라 타입은 객체를 다시 요청하거나 캐시의 키로써 자주 사용되는 고유 식별자를 나타낸다. ID 타입은 String과 같은 방법으로 직렬화되지만, ID로 정의하는 것은 사람이 읽을 수 있도록 하는 의도가 아니라는 것을 의미한다. 살펴보기 User 와 Post 라는 타입을 스키마에 선언 type User { id: ID! username: String! posts: [Post!]! } type Post { id: ID! title: String! content: String! author: User! } 참고: ! 는 데이터 값에 null 이 포함 될 수 없음을 나타낸다. 기본적으로 ! 는 모두 붙여준다고 생각하자. API 설계 단에서 다음과 같이 API에 필요한 타입들을 정의할 수 있다. 이제 해당 타입을 가지는 데이터를 가져올 수 있도록 해보자. ","date":"2019-09-04","objectID":"/graphql1/:2:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"쿼리와 뮤테이션 쿼리와 뮤테이션은 기본적으로 선언되어야 한다. 쿼리는 데이터를 가져오는데 사용 ID 속성을 통해 Client 내부에서 Cache를 구현할 수 있다. 뮤테이션은 데이터를 조작하는데 사용. 생성 수정 삭제 등등.. 이 두가지 타입을 통해 데이터를 가져오고, 수정할 수 있도록 Query , Mutation 타입 내에 속성들을 선언하자. 살펴보기 type User { id: ID! username: String! posts: [Post!]! } type Post { id: ID! title: String! content: String! author: User! } type Query { user(id: ID!): User! post(id: ID!): Post! } type Mutation { createUser(username: String!): User! updateUser(id: ID!, username: String): User! deleteUser(id: ID!): User! createPost(title: String!, content: String!, authorId: String!): Post! updatePost(id: ID!, title: String, content: String): Post! deletePost(id: ID!): Post! } 다음과 같이 Query 와 Mutation 에 속성 값으로 필요한 쿼리, 뮤테이션 들을 만들었다. ","date":"2019-09-04","objectID":"/graphql1/:2:1","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"리졸버 이렇게 선언한 GraphQL 스키마를 어떻게 구동 시킬까? 바로 리졸버가 그 역할을 해준다. 리졸버는 타입 내 속성과 1:1로 일치시켜 구현이 필요한다. 리졸버는 parent , args 를 기본 argument로 하는 함수의 형태이다. parent 는 상위 리졸버에서 return 한 값을 나타낸다. args 는 쿼리문 내에서 넣어준 매개변수를 나타낸다. context 는 요청 하나를 타고 공유되는 전역 상태이다. ( 로그인 세션 정보등 저장 ) 살펴보기 const User = { id: (parent, args, context) =\u003e { return parent.id }, username: (parent, args, context) =\u003e { return parent.username }, posts: (parent, args, context) =\u003e { /* parent.id를 통해 DB에서 Post를 가져옵니다 */ return posts }, } const Post = { id: (parent, args, context) =\u003e { return parent.id }, title: (parent, args, context) =\u003e { return parent.title }, content: (parent, args, context) =\u003e { return parent.content }, author: (parent, args, context) =\u003e { /* parent.authorId를 통해 DB에서 해당 User를 가져옵니다 */ return user }, } const Query = { user: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 해당 User를 가져옵니다 */ return user }, post: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 해당 Post를 가져옵니다 */ return post }, } const Mutation = { createUser: (parent, args, context) =\u003e { /* 새 유저를 생성해서 DB에 삽입합니다 */ return user }, updateUser: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 기존 유저를 불러와 값을 수정 한 뒤 DB에 삽입합니다 */ return user }, deleteUser: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 기존 유저를 삭제합니다 */ return user }, createPost: (parent, args, context) =\u003e { /* 새 게시물을 생성해서 DB에 삽입합니다 */ return post }, updatePost: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 기존 게시물을 불러와 값을 수정 한 뒤 DB에 삽입합니다 */ return post }, deletePost: (parent, args, context) =\u003e { /* args.id를 통해 DB에서 기존 게시물을 삭제합니다 */ return post }, } ","date":"2019-09-04","objectID":"/graphql1/:2:2","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"요청 클라이언트가 다음과 같은 문법으로 서버에 요청을 날리면 query { user(id: \"ea9f5eac-1449-5f03-a1c9-6521622de815\") { id username } } 이 요청에 응답하기 위해 서버에서 Query.user 리졸버 함수 실행 데이터베이스에서 User를 가져와서 해당 값을 리졸버 함수 내에서 return return 값을 [User.id](http://user.id) , User.username 리졸버에 parent argument로 전달 [User.id](http://user.id) 리졸버 함수는 parent argument를 사용해, id 값을 return User.username 리졸버 함수는 parent argument를 사용해, username값을 return 순으로 리졸버 함수가 실행된다. 그 후 결과를 종합해 다음과 같이 JSON으로 응답 해준다. { \"user\": { \"id\": \"ea9f5eac-1449-5f03-a1c9-6521622de815\", \"username\": \"tonyfromundefined\" } } 이런 개발 방식을 GraphQL에서 Schema-First (SDL-First) 개발 방식이라고 한다. 이런 방식은 처음 GraphQL구현체가 등장했을 때 많이 사용되었다. 하지만 Schema-First 개발 방식은 다음과 같은 한계점이 있다. 스키마 정의와 리졸버 간의 불일치 문제 GraphQL 스키마 분리 문제 스키마 정의의 중복 (코드 재사용 문제) IDE 지원 부족으로 인한 낮은 개발 경험 Schema 작성 문제 따라서 이러한 한계점을 효과적으로 해결 하기 위해 Code-First 개발 방식이 등장했다. ","date":"2019-09-04","objectID":"/graphql1/:2:3","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"Nexus로 시작하는 Code-First GraphQL 개발 앞에서 말한 Code-First 개발 방법에 대한 구현체로 Nexus라는 오픈소스 라이브러리를 Prisma에서 내놓았다. Node.js TypeScript Webpack Nexus 를 기반으로 하는 실제 GraphQL 프로젝트를 살펴보자. 폴더 구조 https://github.com/jaejin1/serverless-graphql / .env.development , .env.production 환경 변수 설정파일 package.json , yarn.lock 현재 프로젝트가 의존하고 있는 라이브러리와 버전 정보 yarn 을 이용해 라이브러리를 설치 할 수 있다. tsconfig.json TypeScript 관련 설정파일 tslint.json TSLint 관련 설정파일 webpack.config.dev.js 개발 모드의 Webpack 빌드 설정 yarn dev 명령에서 해당 설정으로 Webpack이 작동 webpack.config.prod.js Production 모드의 Webpack 빌드 설정 yarn build 명령에서 해당 설정으로 Webpack이 작동. serverless.yml 배포를 위한 Serverless Framework 설정 /src/resolvers/ index.ts 엔트리 파일 Query.ts 와 Mutation.ts 가 내보낸 항목을 다시 내보내는 역할을 한다. Query.ts 기본 Query 타입을 선언한다. Mutation.ts 기본 Mutation 타입을 선언한다. /src/generated schema.graphql Nexus가 자동 생성한 Schema이다. Nexus가 자동 생성한 TypeScript Typing이다. ","date":"2019-09-04","objectID":"/graphql1/:3:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"시작하기 /starters/server/ 폴더로 이동 $ cd ./starters/server 프로젝트에 필요한 라이브러리를 설치 # 기존에 yarn이 설치되어 있지 않다면, $ npm i -g yarn # 라이브러리 설치하기 $ yarn 개발 서버 시작 $ yarn dev Nexus 기반으로 작성된 Query 와 Mutation 살펴보기 Query.ts import { queryType } from 'nexus' export const Query = queryType({ definition(t) { t.string('stage', { resolve: (_parent, _args, _context) =\u003e { return process.env.STAGE as string }, }) }, }) Mutation.ts import { mutationType } from 'nexus' export const Mutation = mutationType({ definition(t) { t.string('ping', { resolve: (_parent, _args, _context) =\u003e { return 'pong' }, }) }, }) 다음과 같이 Nexus를 통해서 코드를 작성하면 Nexus가 해당 코드를 이용해 schema.graphql 를 자동 생성해준다. 따라서 Schema-First에서 존재 했던 문제점인 스키마 정의와 리졸버 간 불일치 문제와 Schema 작성문제를 해결 할 수 있다. 추가적으로 Nexus가 nexus.ts 에 TypeScript 타이핑을 자동으로 생성해주기 때문에 개발 편의성을 느낄수 있다. schema.graphql type Query { stage: String! } type Mutation { ping: String! } ","date":"2019-09-04","objectID":"/graphql1/:3:1","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"GraphQL Playground API를 작성했다면 해당 API가 정상적으로 동작하는지 테스트 해야하는데 개발 서버를 띄워논 상태에서 [http://localhost:3000/graphql](http://localhost:3000/graphql) 로 접속하면 GraphQL 문서화 도구인 GraphQL Playground를 볼 수 있다. GraphQL Playground를 통해, GraphQL API를 검색하고 구현된 API를 테스트 할 수 있다. query { stage } \u003e\u003e\u003e { \"data\": { \"stage\": \"development\" } } mutation { ping } \u003e\u003e\u003e { \"data\": { \"ping\": \"pong\" } } ","date":"2019-09-04","objectID":"/graphql1/:4:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["graphql"],"content":"Task 타입과 쿼리, 뮤테이션 만들기 resolvers 폴더에 task 폴더를 생성해서 index.ts ( Task 타입 정의 및 Query, Mutation을 받아서 export ) Query.ts ( Query 타입을 확장 ) Mutation.ts ( Mutation 타입을 확장 ) 파일을 생성한다. /src/resolvers/task/index.ts import { objectType } from 'nexus' interface ITask { id: string content: string isDone: boolean } // 가상의 Database export const TASKS: ITask[] = [] export const Task = objectType({ name: 'Task', definition(t) { t.id('id', { description: 'Task 생성 시 자동 생성되는 Unique ID', }) t.string('content', { description: 'Task 내용', }) t.boolean('isDone', { description: 'Task 완료 여부', }) }, }) export * from './Query' export * from './Mutation' /src/resolvers/task/Query.ts import { extendType, idArg } from 'nexus' import { TASKS } from './' export const TaskQueries = extendType({ type: 'Query', definition(t) { t.field('task', { type: 'Task', args: { id: idArg({ required: true, }), }, resolve: (_parent, args) =\u003e { const task = TASKS.find((task) =\u003e task.id === args.id) if (task) { return task } else { throw new Error(`${args.id}를 가진 Task를 찾을 수 없습니다`) } }, }) t.list.field('tasks', { type: 'Task', resolve: () =\u003e { return TASKS }, }) }, }) /src/resolvers/task/Mutation.ts import { booleanArg, extendType, idArg, stringArg } from 'nexus' import short from 'short-uuid' import { TASKS } from './' export const TaskMutations = extendType({ type: 'Mutation', definition(t) { t.field('createTask', { type: 'Task', args: { content: stringArg({ required: true, }), }, resolve: (_parent, args) =\u003e { const task = { id: short.generate(), content: args.content, isDone: false, } TASKS.push(task) return task }, }) t.field('updateTask', { type: 'Task', args: { id: idArg({ required: true, }), content: stringArg(), isDone: booleanArg(), }, resolve: async (_parent, args) =\u003e { const taskIndex = TASKS.findIndex((task) =\u003e task.id === args.id) if (taskIndex \u003e -1) { if (args.content) { TASKS[taskIndex].content = args.content } if (args.isDone) { TASKS[taskIndex].isDone = args.isDone } return TASKS[taskIndex] } else { throw new Error(`${args.id}라는 ID를 가진 Task를 찾을 수 없습니다`) } }, }) t.field('deleteTask', { type: 'Task', args: { id: idArg({ required: true, }), }, resolve: async (_parent, args) =\u003e { const taskIndex = TASKS.findIndex((task) =\u003e task.id === args.id) if (taskIndex \u003e -1) { const task = TASKS[taskIndex] TASKS.splice(taskIndex, 1) return task } else { throw new Error(`${args.id}라는 ID를 가진 Task를 찾을 수 없습니다`) } }, }) }, }) 기존 schema 엔트리 파일을 수정해 Task 엔트리 파일을 내보낸다. /src/resolvers/index.ts export * from './Query' export * from './Mutation' export * from './task' 개발 서버를 띄워 놨다면, Nexus가 자동으로 Schema를 생성한다. ### This file was autogenerated by GraphQL Nexus ### Do not make changes to this file directly type Mutation { createTask(content: String!): Task! deleteTask(id: ID!): Task! ping: String! updateTask(content: String, id: ID!, isDone: Boolean): Task! } type Query { stage: String! task(id: ID): Task! tasks: [Task!]! } type Task { \"\"\"Task 내용\"\"\" content: String! \"\"\"Task 생성 시 자동 생성되는 Unique ID\"\"\" id: ID! \"\"\"Task 완료 여부\"\"\" isDone: Boolean! } GraphQL Playground에서 테스트 해보자. mutation { createTask(content: \"Hello, World\") { id } } \u003e\u003e\u003e { \"data\": { \"createTask\": { \"id\": \"3fZAgMTo7Va7B2DHuZRXTN\" } } } query { tasks { id content } } \u003e\u003e\u003e { \"data\": { \"tasks\": [ { \"id\": \"3fZAgMTo7Va7B2DHuZRXTN\", \"content\": \"Hello, World\" } ] } } ","date":"2019-09-04","objectID":"/graphql1/:5:0","tags":["graphql"],"title":"Explore GraphQL","uri":"/graphql1/"},{"categories":["algorithm"],"content":"Leetcode - 146. LRU Cache ","date":"2019-08-17","objectID":"/lrucache/:0:0","tags":["algorithm"],"title":"Leetcode - 146. LRU Cache","uri":"/lrucache/"},{"categories":["algorithm"],"content":"Least Recently Used Cache ","date":"2019-08-17","objectID":"/lrucache/:1:0","tags":["algorithm"],"title":"Leetcode - 146. LRU Cache","uri":"/lrucache/"},{"categories":["algorithm"],"content":"Cache? cache는 데이터나 값을 미리 복사해 놓는 임시 장소를 가리킨다. 접근 시간에 비해 원래 데이터를 접근하는 시간이 오래걸리는 경우, 다시 값을 계산하는시간을 절약 하고 싶을때 사용한다. 캐시에 미리 데이터를 복사 해놓으면 빠른 속도로 데이터접근이 가능하다. ","date":"2019-08-17","objectID":"/lrucache/:1:1","tags":["algorithm"],"title":"Leetcode - 146. LRU Cache","uri":"/lrucache/"},{"categories":["algorithm"],"content":"LRU Cache LRU는 OS의 페이지 교체 알고리즘중 하나이다. 가장 오랫동안 사용하지 않은 페이지를 교체하는 기법이다. ","date":"2019-08-17","objectID":"/lrucache/:1:2","tags":["algorithm"],"title":"Leetcode - 146. LRU Cache","uri":"/lrucache/"},{"categories":["algorithm"],"content":"LRU Cache 구현 LRU Cache는 head에 가까운 데이터일수록 최근에 사용한 데이터이다. tail은 가까울 수록 오랫동안 사용하지 않은 데이터이다. 또한 새로운 데이터를 삽입할때 가장 먼저 삭제되도록 한다. code type LRUCache struct { capacity int cache map[int]*node head, tail *node } type node struct { prev, next *node key, value int } LRUCache를 정의하고 node를 linked list로 묶기위해서 node를 정의한다. func Constructor(capacity int) LRUCache { return LRUCache{ capacity: capacity, cache: map[int]*node{}, head: nil, tail: nil, } } 초기화 시키는 함수 func (this *LRUCache) Get(key int) int { node, ok := this.cache[key] if !ok { return -1 } if this.head != nil \u0026\u0026 node.key == this.head.key { return node.value } if this.tail != nil \u0026\u0026 node.key == this.tail.key { this.pop() this.insert(node) this.cache[node.key] = node return node.value } if node.prev != nil \u0026\u0026 node.next != nil { node.prev.next = node.next node.next.prev = node.prev this.insert(node) } return node.value } Get 함수이다. 데이터를 가져오는데 Cache의 특성상 사용(Get)한 데이터는 head쪽에 업데이트를 해주기 위해서 pop하고 insert를 다시해주는걸 볼 수 있다. func (this *LRUCache) Put(key int, value int) { if this.Get(key) != -1 { this.cache[key].value = value return } n := \u0026node{key: key, value: value} this.cache[key] = n this.insert(n) if len(this.cache) \u003e this.capacity { this.pop() } return } Put 함수이다. 데이터를 넣는데 사용한다. cache크기를 넘어서면 pop으로 tail의 데이터를 제거해준다. func (this *LRUCache) insert(n *node) { if this.tail == nil { this.tail = n this.head = n n.next = this.tail return } this.head.prev = n n.next = this.head this.head = n } insert를 하면 head를 업데이트 해준다. func (this *LRUCache) pop() { delete(this.cache, this.tail.key) if this.tail.prev == nil { this.tail = nil return } this.tail.prev.next = nil this.tail = this.tail.prev } tail의 데이터를 꺼낸다. ","date":"2019-08-17","objectID":"/lrucache/:1:3","tags":["algorithm"],"title":"Leetcode - 146. LRU Cache","uri":"/lrucache/"},{"categories":["go"],"content":" channel := make(chan chan string) go에 대해 channel은 대략 알겠지만 chan chan 이란 또 무엇인가.. chan chan은 내가 준비되었다는 것을 알려줄테니, 내가 보낸 채널을 통해 보내줘 라고 한다. 이는 양반향 처리를 할 수 있다. 이해가 어려우니 코드를보자 ! package main import ( \"fmt\" \"time\" ) func main() { test := make(chan chan string) go test1(test) go test2(test) time.Sleep(time.Second) } func test1(channel chan chan string) { responsechan := make(chan string) channel \u003c- responsechan response := \u003c-responsechan fmt.Printf(\"Response: %v\\n\", response) } func test2(channel chan chan string) { responsechan := \u003c-channel responsechan \u003c- \"ang !\" } 아ㅏ아ㅏ… 코드를 보는게 더 어렵다. 보통 channel에는 type과 맞는 값을 전송하거나 받는데, channel안에 channel을 전송하고 받는다고 생각하면 될 것 같다. 이를 통해 간단하게 user별로 여러개의 node를 생성한다고 가정하고 간단하게 작성해보자. package main import ( \"fmt\" \"time\" ) func main() { User := make(chan chan string) User2 := make(chan chan string) go makenode(User) go input(User, \"user1\") go makenode(User2) go input(User2, \"user2\") go input(User2, \"user2\") time.Sleep(time.Second) } func makenode(user chan chan string) { makechannel := make(chan string) for { select { case user \u003c- makechannel: case channelname := \u003c-makechannel: fmt.Printf(\"channelname: %v\\n\", channelname) } } } func input(user chan chan string, username string) { responsechan := \u003c-user responsechan \u003c- \"data \" + username } User 별로 chan chan을 만들고, makenode go 루틴을 user 별로 실행. data를 input하는 go 루틴 실행. 2번 실행했을때 결과는 다음과 같다. channelname: data user2 channelname: data user2 channelname: data user1 --- channelname: data user1 channelname: data user2 channelname: data user2 chan chan을 보고 바로 작성한거라 특성에 맞지 않고 이상할 수 있겠지만 계속 해서 사용하면 괜찮지 않을까 싶다.. go의 chan chan을 비동기 처리에 사용해도 좋겠다는 생각을 가진다. 보통 mq를 사용하는데 간단하게 go는 chan chan을 사용해 비동기 처리에 이용할 수 있다. ","date":"2019-07-10","objectID":"/chanchan/:0:0","tags":["go","gochannel"],"title":"Golang chanchan","uri":"/chanchan/"},{"categories":["kubernetes"],"content":"K8s를 사용하면서 사용자별로 Role을 정해줄 수 있다.","date":"2019-06-27","objectID":"/serviceaccount/","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["kubernetes"],"content":"K8s를 사용하면서 사용자별로 Role을 정해줄 수 있다. ServiceAccount 생성 Role 생성 RoleBinding 생성 ServiceAccount의 token을 얻어서 kubectl에 사용해보자. ","date":"2019-06-27","objectID":"/serviceaccount/:0:0","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["kubernetes"],"content":"ServiceAccount, Role, RoleBinding apiVersion:v1kind:ServiceAccountmetadata:namespace:testname:jaejin---apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:testname:pod-readerrules:- apiGroups:[\"\"]resources:[\"pods\"]verbs:[\"get\",\"watch\",\"list\"]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:read-podsnamespace:testsubjects:- kind:ServiceAccountname:jaejinnamespace:testroleRef:kind:Rolename:pod-readerapiGroup:rbac.authorization.k8s.io 위의 코드를 보고 설명하면, 먼저 계정? 으로 쓰일 ServiceAccount를 생성한다. test namespace에 jaejin이란 이름으로 생성된다. Role은 pods의 정보를 가져온다는 것을 의미한다. RoleBinding은 ServiceAccount와 Role을 연결한다. Note 물론 test란 namespace는 미리 생성했다. $ kubectl get sa -n test NAME SECRETS AGE default 1 16s jaejin 1 13s jaejin 이란 ServiceAccount가 생성되었다. Role과 RoleBinding도 확인하자. $ kubectl get role -n test NAME AGE pod-reader 51s $ kubectl get rolebinding -n test NAME AGE read-pods 77s 이제 이것들을 가지고 token을 뽑아 내자. $ export serviceaccount=jaejin $ export namespace=test $ export server=$(kubectl config view | grep server | cut -f 2- -d \":\" | tr -d \" \") $ export name=$(kubectl get secret -n $namespace | grep $serviceaccount-token | cut -f -1 -d \" \") $ export ca=$(kubectl get secret/$name -n $namespace -o jsonpath='{.data.ca\\.crt}') $ export token=$(kubectl get secret/$name -n $namespace -o jsonpath='{.data.token}' | base64 -D) 위의 명령을 실행하면 token, ca 등의 정보를 가져올 수 있다. kubectl의 config 파일을 만들어보자. echo \" apiVersion: v1 kind: Config clusters: - name: default-cluster cluster: certificate-authority-data: ${ca}server: ${server}contexts: - name: default-context context: cluster: default-cluster namespace: ${namespace}user: default-user current-context: default-context users: - name: default-user user: token: ${token}\" \u003e config config 파일이 만들어지고 이를 ~/.kube/config의 경로로 저장한다. 이제 kubectl 명령을 날려보자. test namespace의 pods의 정보만 가져 올 수 있을 것이다. serviceaccountserviceaccount \" serviceaccount Role을 이용해 사용자별로 권한을 다르게 줄 수 있다 !! ","date":"2019-06-27","objectID":"/serviceaccount/:1:0","tags":["kubernetes","serviceaccount"],"title":"Configuring kubectl by using serviceAccount token","uri":"/serviceaccount/"},{"categories":["terraform"],"content":"Terraform을 공부하면서 AWS의 VPC를 생성해보고, peering까지 해보자. us-east-1, us-east-2 region을 연결 해보고자 한다. terraform { required_version = \"\u003e= 0.10.3\" } provider \"aws\" { region = \"us-east-1\" } provider \"aws\" { region = \"us-east-2\" alias = \"us-east-2\" } 2개의 region을 이용하기 위해 provider를 정의 해준다. resource \"aws_vpc\" \"create_vpc\" { cidr_block = \"10.10.0.0/16\" enable_dns_hostnames = true enable_dns_support = true instance_tenancy = \"default\" tags { Name = \"jaejin test\" } } resource \"aws_vpc\" \"create_vpc_2\" { provider = \"aws.us-east-2\" cidr_block = \"172.10.0.0/16\" enable_dns_hostnames = true enable_dns_support = true instance_tenancy = \"default\" tags { Name = \"jaejin test222\" } } 연결 시킬 VPC를 region 마다 생성한다. resource \"aws_default_route_table\" \"create_vpc\" { default_route_table_id = \"${aws_vpc.create_vpc.default_route_table_id}\" tags { Name = \"default\" } } resource \"aws_default_route_table\" \"create_vpc_2\" { provider = \"aws.us-east-2\" default_route_table_id = \"${aws_vpc.create_vpc_2.default_route_table_id}\" tags { Name = \"default\" } } 생성한 VPC의 라우팅 테이블을 생성해준다. 추후에 VPC들을 바라보게 만들고, 외부에서 ssh 접속을 위해 외부 접속도 받게 한다. resource \"aws_subnet\" \"subnet_1\" { vpc_id = \"${aws_vpc.create_vpc.id}\" cidr_block = \"10.10.1.0/24\" map_public_ip_on_launch = true availability_zone = \"${data.aws_availability_zones.available.names[0]}\" tags = { Name = \"public-az-1\" } } resource \"aws_subnet\" \"subnet_2\" { provider = \"aws.us-east-2\" vpc_id = \"${aws_vpc.create_vpc_2.id}\" cidr_block = \"172.10.1.0/24\" map_public_ip_on_launch = true availability_zone = \"${data.aws_availability_zones.available_2.names[0]}\" tags = { Name = \"public-az-2\" } } data \"aws_availability_zones\" \"available\" {} data \"aws_availability_zones\" \"available_2\" { provider = \"aws.us-east-2\" } 라우팅 테이블에 다른 region의 VPC를 등록하기 위해 subnet을 생성한다. subnet을 등록할때 availability zone을 불러와서 등록 하면된다. list로 받아오기 때문에 0번째 인덱스만 사용하도록 설정해 놓았다. 즉 여러 availability zone을 이용할 수 있다. resource \"aws_internet_gateway\" \"igw_1\" { vpc_id = \"${aws_vpc.create_vpc.id}\" tags { Name = \"internet-gateway\" } } resource \"aws_internet_gateway\" \"igw_2\" { provider = \"aws.us-east-2\" vpc_id = \"${aws_vpc.create_vpc_2.id}\" tags { Name = \"internet-gateway\" } } 외부 접속을 위해 internet gateway도 생성해준다. # vpc끼리 통신하기 위해서 라우팅 테이블 수정 resource \"aws_route\" \"internet_access_vpc\" { route_table_id = \"${aws_vpc.create_vpc.main_route_table_id}\" destination_cidr_block = \"${aws_vpc.create_vpc_2.cidr_block}\" vpc_peering_connection_id = \"${aws_vpc_peering_connection.default.id}\" }## 외부에서 ssh 접속하기 위해서 internet gateway 연결 resource \"aws_route\" \"internet_access\" { route_table_id = \"${aws_vpc.create_vpc.main_route_table_id}\" destination_cidr_block = \"0.0.0.0/0\" gateway_id = \"${aws_internet_gateway.igw_1.id}\" }# vpc끼리 통신하기 위해서 라우팅 테이블 수정 resource \"aws_route\" \"internet_access_vpc_2\" { provider = \"aws.us-east-2\" route_table_id = \"${aws_vpc.create_vpc_2.main_route_table_id}\" destination_cidr_block = \"${aws_vpc.vpc_test.cidr_block}\" vpc_peering_connection_id = \"${aws_vpc_peering_connection.default.id}\" }## 외부에서 ssh 접속하기 위해서 internet gateway 연결 resource \"aws_route\" \"internet_access_2\" { provider = \"aws.us-east-2\" route_table_id = \"${aws_vpc.create_vpc_2.main_route_table_id}\" destination_cidr_block = \"0.0.0.0/0\" gateway_id = \"${aws_internet_gateway.igw_2.id}\" } 라우팅 테이블에 vpc의 cidr_block을 등록 해주고, 외부 접속을 위해 0.0.0.0/0 도 등록해준다. 마지막으로 vpc peering을 하자. resource \"aws_vpc_peering_connection\" \"default\" { peer_owner_id = \"871229912567\" peer_vpc_id = \"${aws_vpc.create_vpc_2.id}\" vpc_id = \"${aws_vpc.vpc_test.id}\" peer_region = \"us-east-2\" auto_accept = false tags = { Side = \"Requester\" } } resource \"aws_vpc_peering_connection_accepter\" \"default\" { provider = \"aws.us-east-2\" vpc_peering_connection_id = \"${aws_vpc_peering_connection.default.id}\" auto_accept = true tags = { Side = \"Accepter\" } } peer_vpc_id 에 연결할 region의 vpc를 등록해주고 vpc_id 에 현재 region의 vpc를 등록해준다. resource에 provider를 명시 안해줬으므로 default region은 us-east-1로 잡혀 있으므로 peer_region에는 연결 시킬 us-east-2를 입력 해주었다. 전체 코드 https://github.com/jaejin1/VPC-Peering/blob/master/main.tf ","date":"2019-03-17","objectID":"/terraform1/:0:0","tags":["terraform","aws","devops"],"title":"aws vpc peering using terraform","uri":"/terraform1/"},{"categories":["kubernetes"],"content":"GCP, AWS와 같은 Cloud 환경이 아닌 물리 서버에 kubernetes cluster 구성해보자.","date":"2019-03-10","objectID":"/kubeadm/","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"GCP, AWS와 같은 Cloud 환경이 아닌 물리 서버에 kubernetes cluster 구성해보자. Creating a single master cluster with kubeadm ","date":"2019-03-10","objectID":"/kubeadm/:0:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Maser node setting $ sudo kubeadm --kubernetes-version=1.13.2 init init을 실행하면 worker node 에서 master node로 접속할 수 있는 token hash 값을 내준다. 이것을 이용해 master node에 접속한다. ex) kubeadm join \u003cmaster-ip\u003e:\u003cmaster-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e Note 만약 [ERROR Swap]: running with swap on is not supported. Please disable swap 에러가 발생한다면 swapoff를 해주면된다. $ sudo swapoff -a To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2019-03-10","objectID":"/kubeadm/:1:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Installing a pod network add-on 문서에 보면 여러가지 network를 설정할 수 있는 것들이 존재하는데 Canal을 사용하였다. $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/rbac.yaml $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/canal/canal.yaml 잘 작동하고 있는지 pod을 확인하자. $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE canal-vb5fw 0/3 ContainerCreating 0 27s coredns-86c58d9df4-mf68s 0/1 Pending 0 2s coredns-86c58d9df4-zbxnp 0/1 Pending 0 2s etcd-eta 1/1 Running 0 26s kube-apiserver-eta 1/1 Running 0 45s kube-controller-manager-eta 1/1 Running 0 33s kube-proxy-tww4r 1/1 Running 0 82s kube-scheduler-eta 1/1 Running 0 24s Note 만약 coredns가 fail이 나거나 error가 발생하면 다음 설정으로 해결할 수 있다. $ kubectl -n kube-system edit configmap coredns loop 삭제 $ kubectl -n kube-system delete pod -l k8s-app=kube-dns ","date":"2019-03-10","objectID":"/kubeadm/:2:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"Joining your nodes 아까 master node에서 init할 때 나온 token hash값을 이용해서 node들을 join 시켜보자. worker node에서는 init 할 필요없이 바로 join 명령어만 실행 하면된다. $ kubeadm join \u003cmaster-ip\u003e:\u003cmaster-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e master에서 확인해보면, $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE canal-6gg89 3/3 Running 0 24m canal-7dhgw 3/3 Running 0 24m canal-vb5fw 3/3 Running 0 24m coredns-86c58d9df4-mf68s 1/1 Running 0 24m coredns-86c58d9df4-zbxnp 1/1 Running 0 24m etcd-eta 1/1 Running 0 24m kube-apiserver-eta 1/1 Running 0 25m kube-controller-manager-eta 1/1 Running 0 24m kube-proxy-57qh5 1/1 Running 0 24m kube-proxy-tww4r 1/1 Running 0 25m kube-proxy-x6rng 1/1 Running 0 24m kube-scheduler-eta 1/1 Running 0 24m node 개수 만큼 canal, proxy등이 추가 된것을 확인 할 수 있다. $ kubectl get nodes NAME STATUS ROLES AGE VERSION chi Ready \u003cnone\u003e 14m v1.13.2 delta Ready \u003cnone\u003e 14m v1.13.2 eta Ready master 16m v1.13.2 ","date":"2019-03-10","objectID":"/kubeadm/:3:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["kubernetes"],"content":"pod test하기 조대협님 블로그 node.js 코드 참조 Deployment를 올리고 apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:jaejin-node-testspec:replicas:3template:metadata:labels:app:myappspec:containers:- name:stupid-serverimage:opiximeo/jaejintest:v1imagePullPolicy:Alwaysports:- containerPort:8080 외부에서 접속하기 위해 service를 올린다. apiVersion:v1kind:Servicemetadata:name:hello-node-svcspec:selector:app:myappports:- port:5920protocol:TCPtargetPort:8080externalIPs:- {your server ip} cloud에서 service에 type: LoadBalancer를 설정해주면 자동으로 외부 IP를 가져오지만 cloud에서 올린게 아니라서 자동으로 가져오지는 못한다. 따라서 externalIPs 옵션을 줘서 접속할 수 있는 IP를 명시해주면 된다. http://{your server ip}:5920 ","date":"2019-03-10","objectID":"/kubeadm/:4:0","tags":["kubernetes","kubeadm"],"title":"Creating kubernetes clusters using kubeadm","uri":"/kubeadm/"},{"categories":["elasticsearch"],"content":"요번엔 Elasticsearch / kibana에 Search Guard를 연동시켜서 login 기능을 구현해보자. AWS Cloudformation으로 ICON Devnet을 AWS Marketplace에 등록하는 도중 Elasticsearch / kibana를 포함하려면 login 기능이 있어야 한다고한다.. 그에 대한 작업 중 일부를 기록하려고 한다. 일단 HTTPS를 이용하기 때문에 인증서를 직접 만들어도 되고, Search Guard에서 sample로 지원해주는 인증서가 있다. 후자를 선택했다. 전자의 경우 예~~전에 작업해놓은 것이 있는데 https://github.com/jaejin1/docker-EK를 참조 해보길 바란다. 먼저 docker-compose.yml 파일을 다음과 같이 만들었다. version:'3'services:elasticsearch:image:docker.elastic.co/elasticsearch/elasticsearch:6.6.0container_name:elasticsearchvolumes:- /home/ec2-user/elasticsearch/config:/usr/share/elasticsearch/config- /home/ec2-user/elasticsearch/data:/var/data/elasticsearch- /home/ec2-user/elasticsearch/log:/var/log/elasticsearchports:- \"9200:9200\"- \"9300:9300\"environment:ES_JAVA_OPTS:\"-Xmx1024m -Xms1024m\"ulimits:nofile:soft:90000hard:90000networks:- ekkibana:image:docker.elastic.co/kibana/kibana:6.6.0container_name:kibanavolumes:- /home/ec2-user/kibana/config:/usr/share/kibana/configports:- \"5601:5601\"networks:- ekenvironment:NODE_OPTIONS:\"--max-old-space-size=8192\"depends_on:- elasticsearchnetworks:ek:driver:bridge docker-compose.yml 파일의 volumes의 conf 폴더에는 elasticsearch.yml 파일과 cloudformation에서 이상하게 6.x 버전은 ingest-geoip이라는 plugin이 필요해서 그 폴더들을 넣어주었다. kibana의 volumes도 마찬가지로 conf폴더엔 kibana.yml 파일이 존재한다. 단순히 test 용이라면 따로 설정파일을 생성해놓을 필요는 없을 것이다. 이제 elasticsearch, kibana를 실행시켜보자. $ docker-compose up -d 지금까지는 elasticsearch, kibana를 실행한 것 뿐이다. http://localhost:5601 로 접속해보면 kibana가 잘 뜰 것이다. 이제, Search Guard를 실행 시켜보자. 이 글에서는 docker-compose로 docker에서 실행 시켰기 때문에 docker-compose exec 명령으로 실행 시킨다. docker를 쓰지 않는다면 docker-compose exec -T ~~ 뒤 부터 실행하면 된다. elasticsearch $ docker-compose exec -T elasticsearch elasticsearch-plugin install --batch com.floragunn:search-guard-6:6.6.0-24.1 $ docker-compose exec -T elasticsearch bash plugins/search-guard-6/tools/install_demo_configuration.sh -y $ docker-compose restart elasticsearch $ docker-compose exec -T elasticsearch ./sgadmin_demo.sh Search Guard plugin을 설치한다. 버전 정보는 https://docs.search-guard.com/latest/search-guard-versions 을 참조하길 바란다. demo_configuration 을 실행해 demo 설정을 한다. ( elasticsearch.yml 파일에 search guard 설정이 자동으로 추가된다. ) elasticsearch.yml 파일을 적용하기 위해 docker를 restart 한다. sgadmin_demo.sh 를 실행해 Search Guard를 실행한다. 이후 부터는 http가 아닌 https로 접속을 해야 한다. kibana $ docker-compose exec -T kibana kibana-plugin install https://repo1.maven.org/maven2/com/floragunn/search-guard-kibana-plugin/6.6.0-18/search-guard-kibana-plugin-6.6.0-18.zip $ docker-compose exec -T kibana bash -c 'echo searchguard.cookie.password: \\\"123567818187654rwrwfsfshdhdhtegdhfzftdhncn\\\" \u003e\u003e /usr/share/kibana/config/kibana.yml' $ docker-compose restart kibana kibana plugin을 설치한다. 버전은 위의 elasticsearch 부분을 참조하길 바란다. kibana.yml파일에 설정을 추가한다. kibana.yml 파일을 적용하기 위해 kibana를 restart 한다. 여기까지 완료 했다면, localhost:5601 or {serverIP}:5601 로 kibana에 접속해보면 loginlogin \" login 다음과 같이 login 화면이 나올 것이다. admin / admin으로 접속하면 된다. ID / Password 는 따로 권한을 줄 수 있는데 여기서 다루지는 않겠다. ID마다 보여주는 log의 index를 달리할 수 있다. 보통 Fluentd -\u003e Elasticsearch -\u003e Kibana로 log를 보낼 것인데, 만약 Search Guard를 적용했을 경우 Fluentd의 설정도 변경해줘야한다. http가 아닌 https로 보내야 하기 때문에.. 보통 fluentd.conf 파일을 다음과 같이 작성하고 있었다면, \u003cmatch **\u003e # Add your log tag to show in \u003c\u003e. @type copy \u003cstore\u003e # Add your log tag to show in \u003c\u003e. @type file # Leave log file in path. path /fluentd/log/docker_data.log \u003c/store\u003e \u003cstore\u003e @type elasticsearch host {elasticsearch_ip} port 9200 logstash_format true index_name logstash type_name logstash logstash_prefix logstash \u003c/store\u003e \u003c/match\u003e scheme https ssl_verify false user admin password admin ssl_version TLSv1_2 를 추가해서 다음과 같이 작성해야한다. 그러면 fluentd 에서 Elasticsearch로 log를 잘 보낼 것이다. \u003cmatch **\u003e # Add your log tag to show in \u003c\u003e. @type copy \u003cstore\u003e # Add your log tag to show in \u003c\u003e. @type file # Leave log file in path. path /fluentd/log/docker_data.log \u003c/store\u003e \u003cstore\u003e @type elasticsearch host {elasticsearch_ip} port 9200 scheme https ssl_verify false user admin password admin ssl_version TLSv1_2 logstash_format true index_n","date":"2019-02-24","objectID":"/searchguard/:0:0","tags":["elasticsearch","kibana"],"title":"Search guard kibana plugin","uri":"/searchguard/"},{"categories":["ansible"],"content":"Ansible을 공부하면서 AWS의 VPC를 생성해보고, peering까지 해보자. ","date":"2019-01-24","objectID":"/ansible1/:0:0","tags":["ansible","aws","devops"],"title":"Create aws vpc using ansible","uri":"/ansible1/"},{"categories":["ansible"],"content":"vpc 설정 ","date":"2019-01-24","objectID":"/ansible1/:1:0","tags":["ansible","aws","devops"],"title":"Create aws vpc using ansible","uri":"/ansible1/"},{"categories":["ansible"],"content":"Ansible playbook으로 VPC 생성 이번 글은 Ansible을 이용해 VPC를 생성하고 수동으로 VPC peering을 해보고자 한다. Set up your AWS VPC with Ansible 2.0 먼저 playbook을 다음과 같은 구조로 만들것이다. ├── playbook.yml ├── inventory ├── vars.yml ├── roles/ │ ├── vpc/ │ │ ├── tasks/ │ │ │ ├── main.yml main파일인 playbook.yml 을 다음과 같이 정의한다. - hosts:localroles:- vpc 다음은 playbook을 실행할때 사용할 inventory file이다. [local]localhost ansible_connection=local Note [localhost -\u003e localhost]: FAILED! =\u003e {“changed”: false, “msg”: “Python modules \"botocore\" or \"boto3\" are missing, please install both”} 이런 에러가 발생 할수도 있다. 이때는 inventory file을 변경해줘야한다. [local]localhost[webserver] localhost 뒤에 추가 해줘야한다. [local]localhost ansible_connection=local ansible_python_interpreter=python3.6[webserver] 마지막으로 variables을 정의하자. ---## AWS Credentialsaws_access_key:\"THISISMYAWSACCESSKEY\"aws_secret_key:\"ThisIsMyAwSSecretKey\"aws_region:\"eu-west-1\"## VPC Informationvpc_name:\"My VPC\"vpc_cidr_block:\"10.0.0.0/16\"## For Security Group Rule#my_ip: \"X.X.X.X\"## Subnetspublic_subnet_1_cidr:\"10.0.0.0/24\"```yaml이제 VPC를 만들 파일을 만들어보자.```yaml---# roles/vpc/tasks/main.yml# First task : creating the VPC.# We are using the variables set in the vars.yml file.# The module gives us back its result,# which contains information about our new VPC. # We register it in the variable my_vpc.- name:Create VPCec2_vpc_net:name:\"{{ vpc_name }}\"cidr_block:\"{{ vpc_cidr_block }}\"region:\"{{ aws_region }}\"aws_access_key:\"{{ aws_access_key }}\"aws_secret_key:\"{{ aws_secret_key }}\"state:\"present\"register:my_vpc# We now use the set_fact module # to save the id of the VPC in a new variable.- name:Set VPC ID in variableset_fact:vpc_id:\"{{ my_vpc.vpc.id }}\"# Creating our only Subnet in the VPC.# A subnet needs to be located in an Availability Zone (or AZ).# Again, we register the results in a variable for later.- name:Create Public Subnetec2_vpc_subnet:state:\"present\"vpc_id:\"{{ vpc_id }}\"cidr:\"{{ public_subnet_1_cidr }}\"az:\"{{ aws_region }}a\"region:\"{{ aws_region }}\"aws_access_key:\"{{ aws_access_key }}\"aws_secret_key:\"{{ aws_secret_key }}\"#map_public: \"yes\" # vpc가 instance를 생성할때 자동으로 public ip를 할당하게 하는 옵션 ( default: false )resource_tags:Name:\"Public Subnet\"register:my_public_subnet# We save the id of the Public Subnet in a new variable.- name:Set Public Subnet ID in variableset_fact:public_subnet_id:\"{{ my_public_subnet.subnet.id }}\"# Every VPC needs at least one Internet Gateway.# This component allows traffic between the VPC and the outside world.- name:Create Internet Gateway for VPCec2_vpc_igw:vpc_id:\"{{ vpc_id }}\"region:\"{{ aws_region }}\"aws_access_key:\"{{ aws_access_key }}\"aws_secret_key:\"{{ aws_secret_key }}\"state:\"present\"register:my_vpc_igw# We save the id of the Internet Gateway in a new variable.- name:Set Internet Gateway ID in variableset_fact:igw_id:\"{{ my_vpc_igw.gateway_id }}\"# Now we set up a Route Table. # We attach that Route Table to the Public Subnet.# The route we create here defines the default routing # of the table, redirecting requests to the Internet Gateway. # We don't see it here, but the route table will also contain # a route for resources inside the VPC, so that if we need # to reach an internal resource, we don't go to the Internet# Gateway.- name:Set up public subnet route tableec2_vpc_route_table:vpc_id:\"{{ vpc_id }}\"region:\"{{ aws_region }}\"aws_access_key:\"{{ aws_access_key }}\"aws_secret_key:\"{{ aws_secret_key }}\"tags:Name:\"Public\"subnets:- \"{{ public_subnet_id }}\"routes:- dest:\"0.0.0.0/0\"gateway_id:\"{{ igw_id }}\" 이제 실행해보자. $ ansible-playbook playbook.yml -i inventory -e @vars.yml PLAY [local] **************************************************************************************** TASK [Gathering Facts] ****************************************************************************** ok: [localhost] TASK [vpc : Create VPC] ***************************************************************************** changed: [localhost] TASK [vpc : Set VPC ID in variable] ***************************************************************** ok: [localho","date":"2019-01-24","objectID":"/ansible1/:1:1","tags":["ansible","aws","devops"],"title":"Create aws vpc using ansible","uri":"/ansible1/"},{"categories":["ansible"],"content":"Peering ansibleansible \" ansible AWS vpc에 들어오면 피어링 연결 목록이 있다. 생성을 누르고 설정을 해준다. ansibleansible \" ansible 피어링 연결 이름 태그에는 이름을 써주고 로컬 VPC는 현재 접속중인 리전의 VPC를 선택한다. 피어링할 다른 VPC 선택 항목에서는 아까 다른 리전에 생성한 VPC를 써준다. 만약 다른 계정이라면 다른 계정을 선택. 현재 리전의 VPC 피어링도 가능하다. ansibleansible \" ansible 상태가 수락 대기중이다. 연결 할 리전의 VPC에 들어가서 수락을 해준다. ansibleansible \" ansible 수락하게 되면 상태가 활성화 되면서 연결에 성공하였다. ansibleansible \" ansible 하지만 각 VPC에 연결된 Instance끼리 통신은 안될 것이다. 그 이유는 라우팅 테이블에 명시를 해줘야한다. 라우팅 테이블 왼쪽항목에 라우팅 테이블이 있고 선택하면 아까 생성한 VPC에 연결된 라우팅 테이블이 있을것이다. ansibleansible \" ansible 선택을 하고 밑쪽에 Routes에 들어가 Edit routes를 클릭한다. ansibleansible \" ansible Add route를 선택하고 추가해준다. ansibleansible \" ansible internet Gateway를 선택하고 다른 리전에 있는 internet gateway의 ID를 입력해준다. save하면 이제 통신이 잘 될것이다!!! ","date":"2019-01-24","objectID":"/ansible1/:1:2","tags":["ansible","aws","devops"],"title":"Create aws vpc using ansible","uri":"/ansible1/"},{"categories":["kubernetes"],"content":"argo를 알아보자.","date":"2019-01-13","objectID":"/argo/","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"argo를 알아보자. argo는 컨테이너 워크플로우 솔루션이다. 워크플로우 스펙을 YAML 파일로 정의하고, 실행할때 마다 컨테이너를 생성해서 작업을 수행한다. Open source container-native workflow engine for Kubernetes. argo argo 공식홈페이지에 나와있는 example을 실행해보자. 먼저 yaml 파일을 다음과 같이 생성한다. apiVersion:argoproj.io/v1alpha1kind:Workflow #new type of k8s specmetadata:generateName:hello-world- #name of workflow specspec:entrypoint:whalesay #invoke the whalesay templatetemplates:- name:whalesay #name of templatecontainer:image:docker/whalesaycommand:[cowsay]args:[\"hello world\"]resources:#don't use too much resourceslimits:memory:32Micpu:100m 생성후 kubernetes에 submit 한다. $ argo submit hello-world.yaml 현재 워크플로우 list를 보려면 list 명령어를 이용한다. $ argo list NAME STATUS AGE DURATION hello-world-s6d7p Running 10m 10m 생성된 pod 의 상태를보려면 get 명령어를 이용 $ argo get hello-world-s6d7p Name: hello-world-s6d7p Namespace: default ServiceAccount: default Status: Running Created: Sun Jan 13 14:56:43 +0900 (11 minutes ago) Started: Sun Jan 13 14:56:52 +0900 (10 minutes ago) Duration: 10 minutes 52 seconds STEP PODNAME DURATION MESSAGE ● hello-world-s6d7p hello-world-s6d7p 10m 마찬가지로 log를 보려면 log 명령어를 이용 $ argo logs -w hello-world-xxx # get logs from all steps in a workflow argo logs -w hello-world-s6d7p hello-world-s6d7p: _____________ hello-world-s6d7p: \u003c hello world \u003e hello-world-s6d7p: ------------- hello-world-s6d7p: \\ hello-world-s6d7p: \\ hello-world-s6d7p: \\ hello-world-s6d7p: ## . hello-world-s6d7p: ## ## ## == hello-world-s6d7p: ## ## ## ## === hello-world-s6d7p: /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === hello-world-s6d7p: {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- hello-world-s6d7p: \\______ o __/ hello-world-s6d7p: \\ \\ __/ hello-world-s6d7p: \\____\\______/ $ argo logs hello-world-xxx-yyy # get logs from a specific step in a workflow argo logs hello-world-s6d7p _____________ \u003c hello world \u003e ------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ === {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- \\______ o __/ \\ \\ __/ \\____\\______/ delete $ argo delete hello-world-xxx ","date":"2019-01-13","objectID":"/argo/:0:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"ArgoUI CLI가 아닌 GUI로 확인하기 위해 argo-ui를 로컬 PC 로 포워딩 한다. $ kubectl -n argo port-forward deployment/argo-ui 8001:8001 argoargo \" argo ","date":"2019-01-13","objectID":"/argo/:1:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"연속작업 다음과 같이 워크플로우를 작성한다. apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:steps-spec:entrypoint:hello-hello-hello# This spec contains two templates: hello-hello-hello and whalesaytemplates:- name:hello-hello-hello# Instead of just running a container# This template has a sequence of stepssteps:- - name:hello1 #hello1 is run before the following stepstemplate:whalesayarguments:parameters:- name:messagevalue:\"hello1\"- - name:hello2a #double dash =\u003e run after previous steptemplate:whalesayarguments:parameters:- name:messagevalue:\"hello2a\"- name:hello2b #single dash =\u003e run in parallel with previous steptemplate:whalesayarguments:parameters:- name:messagevalue:\"hello2b\"# This is the same template as from the previous example- name:whalesayinputs:parameters:- name:messagecontainer:image:docker/whalesaycommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] Metadata에서 generateName: steps- 는 워크플로우 이름이다. 워크플로우는 templates부분에 정의 되어있는데 hello1과 hello2a를 보면 앞에 - - 2개의 -가 정의되어있고 hello2b 같은경우는 1개의 -로 되어있는 것을 볼 수 있다. 이는 hello2a, hello2b를 동시에 실행하는 것을 의미한다. 다시 실행해보자. $ argo submit --watch helloworld-multi.yaml Name: steps-nt72p Namespace: argo ServiceAccount: default Status: Succeeded Created: Sun Jan 13 15:39:55 +0900 (5 seconds ago) Started: Sun Jan 13 15:39:55 +0900 (5 seconds ago) Finished: Sun Jan 13 15:40:00 +0900 (now) Duration: 5 seconds STEP PODNAME DURATION MESSAGE ✔ steps-nt72p ├---✔ hello1 steps-nt72p-1112559031 1s └-·-✔ hello2a steps-nt72p-220736832 1s └-✔ hello2b steps-nt72p-271069689 2s --watch 옵션은 각 단계별 실행 상태와 워크플로우 구조를 볼 수 있다. argoargo \" argo step-~ 이라는 워크플로우가 생성되었다. Warning Notice: istio가 설치되어있는환경에서는 계속 running에서 대기중이던데 확인이 필요할 거 같다. istio가 적용안되어있는 namespace를 이용했더니 잘 작동했다. argoargo \" argo ","date":"2019-01-13","objectID":"/argo/:2:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"DAG를 이용한 워크플로우 정의 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:dag-diamond-spec:entrypoint:diamondtemplates:- name:echoinputs:parameters:- name:messagecontainer:image:alpine:3.7command:[echo, \"{{inputs.parameters.message}}\"]- name:diamonddag:tasks:- name:Atemplate:echoarguments:parameters:[{name: message, value:A}]- name:Bdependencies:[A]template:echoarguments:parameters:[{name: message, value:B}]- name:Cdependencies:[A]template:echoarguments:parameters:[{name: message, value:C}]- name:Ddependencies:[B, C]template:echoarguments:parameters:[{name: message, value:D}] DAG를 이용하면 좀 더 명시적으로 워크플로우 정의가 가능하다. A인 task가 실행하게 하고 B,C는 A에 의존성을 가지도록 하고 D는 B,C의 의존성을 가지게 해서 실행 순서는 A → B , C → D 형태가 된다. argoargo \" argo 이후 진행은 조대협님 블로그를 참고하였습니다. 컨테이너 기반 워크플로우 솔루션 Argo ","date":"2019-01-13","objectID":"/argo/:3:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"입력/출력값 전달 argo의 개념과 워크플로우의 개념을 이해했으면 task간 데이터를 어떻게 전달하는지 살펴보자. 입력값 전달 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:hello-world-parameters-spec:# invoke the whalesay template with# \"hello world\" as the argument# to the message parameterentrypoint:whalesayarguments:parameters:- name:messagevalue:hello worldtemplates:- name:whalesayinputs:parameters:- name:message #parameter declarationcontainer:# run cowsay with that message input parameter as argsimage:docker/whalesaycommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] 먼저 spec.arguments 부분에서 message라는 변수를 선언 하였고, 그값은 hello world로 되어 있다. 그리고 워크플로우의 whalesay 태스크에서 message 변수를 input 변수로 사용하도록 선언했다. input.parameters.message를 참조하여 message 변수의 값을 도커 컨테이터의 실행 변수로 넘기도록 하였다. CLI에서 값을 변경하려면 $ argo submit file.yaml -p {parameter name}={value} ","date":"2019-01-13","objectID":"/argo/:4:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"출력값 사용 apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:output-parameter-spec:entrypoint:output-parametertemplates:- name:output-parametersteps:- - name:generate-parametertemplate:whalesay- - name:consume-parametertemplate:print-messagearguments:parameters:# Pass the hello-param output from the generate-parameter step as the message input to print-message- name:messagevalue:\"{{steps.generate-parameter.outputs.parameters.hello-param}}\"- name:whalesaycontainer:image:docker/whalesay:latestcommand:[sh, -c]args:[\"echo -n hello world \u003e /tmp/hello_world.txt\"]#generate the content of hello_world.txtoutputs:parameters:- name:hello-param #name of output parametervalueFrom:path:/tmp/hello_world.txt #set the value of hello-param to the contents of this hello-world.txt- name:print-messageinputs:parameters:- name:messagecontainer:image:docker/whalesay:latestcommand:[cowsay]args:[\"{{inputs.parameters.message}}\"] 먼저 whalesay를 보면 outputs.parameters에 hello-param이라는 이름으로 output 변수를 정의하였고, output 내용은 /tmp/hello_world.txt 파일 내용으로 채워진다. 다음 print-message 컨테이너 정의 부분을 보면 input param으로 message라는 변수를 정의하였다. steps를 보면 print-message를 실행할때 message 변수의 값을 {{steps.generate-parameter.outputs.parameters.hello-param}} 로 정의해 print-message의 이전 스탭인 generate-parameter의 output param 중에 hello-param이라는 변수의 값으로 채우는 것을 볼 수 있다. ","date":"2019-01-13","objectID":"/argo/:5:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["kubernetes"],"content":"Artifact 워크 플로우 태스크에 대한 입/출력값을 parameter로 전달할 수 도 있지만, CI/CD 빌드 파이프라인에서는 소스코드, 빌드 바이너리가 될수 도 있고 빅데이터 파일일 수도 있다. 이 경우 parameter를 이용해서 넘기기는 부담이 된다. 이런경우 artifact라는 기능을 이용한다. 외부 저장소 AWS S3나 GCP GCS같은 곳에 저장할 수 있게하고 반대로 읽기도 가능하다. 나중에 kubeflow에서 텐서플로우 학습을 시키는 파이프라인이 있을때 학습된 모델을 S3나 GCS에 저장하도록 하는 작업을 할 수 있다. parameter → artifact로만 변경하면 된다. apiVersion:argoproj.io/v1alpha1kind:Workflowmetadata:generateName:artifact-passing-spec:entrypoint:artifact-exampletemplates:- name:artifact-examplesteps:- - name:generate-artifacttemplate:whalesay- - name:consume-artifacttemplate:print-messagearguments:artifacts:# bind message to the hello-art artifact# generated by the generate-artifact step- name:messagefrom:\"{{steps.generate-artifact.outputs.artifacts.hello-art}}\"- name:whalesaycontainer:image:docker/whalesay:latestcommand:[sh, -c]args:[\"cowsay hello world | tee /tmp/hello_world.txt\"]outputs:artifacts:# generate hello-art artifact from /tmp/hello_world.txt# artifacts can be directories as well as files- name:hello-artpath:/tmp/hello_world.txt- name:print-messageinputs:artifacts:# unpack the message input artifact# and put it at /tmp/message- name:messagepath:/tmp/messagecontainer:image:alpine:latestcommand:[sh, -c]args:[\"cat /tmp/message\"] ","date":"2019-01-13","objectID":"/argo/:6:0","tags":["kubernetes","argo"],"title":"Argo","uri":"/argo/"},{"categories":["go"],"content":"Go 마스터하기 책을 공부하면서 기록한다. ","date":"2019-01-07","objectID":"/mastergo01/:0:0","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"01. Go 언어와 운영체제 ","date":"2019-01-07","objectID":"/mastergo01/:1:0","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"유닉스 stdin, stdout, stderr 유닉스 OS는 OS에서 실행되는 프로세스를 위해 항상 세 가지 파일을 열어 둔다. 유닉스는 모든 것을 파일 취급한다. 심지어 프린터나 마우스도.. 유닉스에서는 양의 정수 값으로 된 파일 디스크립터 를 사용한다. 파일 디스크립터는 열린 파일에 접근하기 위한 내부 표현 수단으로, 긴 경로를 사용하는 것보다 훨씬 편리하다. 기본적으로 모든 유닉스 시스템은 /dev/stdin, /dev/stdout, /dev/stderr 세 가지의 특수한 표준 파일명을 사용한다. 각각에 대한 파일 디스크립터는 0, 1, 2이다. 이 세가지는 표준 입력, 표준 출력, 표준 에러 라고도 부른다. 또한 맥OS에서 파일 디스크립터 0은 /dev/fd/0을 통해 접근할 수 있고, 데비안 리눅스에서는 /dev/fd/0과 /dev/pts/0 둘 다 사용할 수 있다. Go 코드에서 표준 입력은 os.Stdin, 표준 출력은 os.Stdout, 표준 에러는 os.Stderr로 접근할 수 있다. /dev/stdin, /dev/stdout, /dev/stderr 또는 이에 대한 파일 디스크립터 값을 직접 사용해도 되지만, Go 언어에서 제공하는 표준 파일명인 os.Stdin, os.Stdout, os.Stderr로 접근하는 것이 바람직하고 안전하며 이식성에 유리하다. ","date":"2019-01-07","objectID":"/mastergo01/:1:1","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"표준 출력 사용하기 package main import( \"io\" \"os\" ) stdout을 이용 해보자. fmt 패키지 대신 io 패키지를 사용한다. os 패키지는 프로그램에서 커맨드 라인 인수를 읽고 os.Stdout에 접근하기 위해 사용한다. func main() { myString := \"\" arguments := os.Args if len(arguments) == 1 { myString = \"Please give me one argument\" } else { myString = arguments[1] } myString 변수에 화면에 출력할 텍스트를 담는다. io.WriteString(os.Stdout, myString) io.WriteString(os.Stdout, \"\\n\") } 여기서 io.WriteString 함수는 fmt.Print() 함수와 똑같은 방식으로 작동하지만 두 개의 매개변수만 받는다는 점이 다르다. 첫 번째 매개변수는 쓰려는 파일을 지정한다. 여기서는 os.Stdout이고, 두 번째 매개변수는 string 타입 변수이다. Note is.WriteString 함수에서 첫 번째 매개변수의 타입은 반드시 io.Write여야 한다. 이 인터페이스를 사용하려면 두 번째 매개변수를 바이트 슬라이스 로 지정해야 한다. 하지만 string도 상관없다고 한다. $ go run test.go Please give me one argument $ go run test.go hi hi $ go run test.go hi hello hi ","date":"2019-01-07","objectID":"/mastergo01/:1:2","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"표준 입력으로 부터 읽기 func main() { var f *os.File f = os.Stdin defer f.Close() scanner := bufio.NewScanner(f) for scanner.Scan() { fmt.Printnl(\"\u003e\", scanner.Text()) } } bufio.NewScanner()의 매개변수에 표준 입력을 지정해 호출하는 것을 볼 수 있다. 이 함수는 bufio.Scanner 변수를 리턴하는데, 이 값은 다시 Scan() 함수로 os.Stdin으로부터 한 줄씩 읽는 데 사용된다. ","date":"2019-01-07","objectID":"/mastergo01/:1:3","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"에러 출력 아까와 같은 코드를 보자. 한가지 다른점은 io.WriteString부분에 Stderr를 사용했다. package main import( \"io\" \"os\" ) func main() { myString := \"\" arguments := os.Args if len(arguments) == 1 { myString = \"Please give me one argument\" } else { myString = arguments[1] } io.WriteString(os.Stdout, \"This is Standard output \\n\") io.WriteString(os.Stderr, myString) io.WriteString(os.Stderr, \"\\n\") } $ go run test.go hi This is Standard output hi 결과를 보면 표준 출력과 표준 에러를 구분할 수 없다. bash(1) 셸을 사용한다면 표준 출력 데이터와 표준 에러 데이터를 구분할 수 있다. $ go run test.go 2\u003e/tmp/stdError This is Standard output $ cat /tmp/stdError Please give me one argument $ go run test.go hi This is Standard output hi 에러 출력을 무시하려면 /dev/null 디바이스로 리디렉션하면 된다. 그러면 이 값을 무시한다. $ go run test.go 2\u003e/dev/null This is Standard output 표준 출력과 표준 에러의 결과를 모두 같은 파일에 저장하고 싶다면, 표준 에러에 대한 파일 디스크립터(2)를 표준 출력에 대한 파일 디스크립터(1)로 리디렉션 하면된다. $ go run test.go \u003e/tmp/output 2\u003e\u00261 $ cat /tmp/output This is Standard output Please give me one argument ","date":"2019-01-07","objectID":"/mastergo01/:1:4","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["go"],"content":"로그 파일 작성하기 log 패키지를 사용하면 시스템 로그로 로그 메세지를 보낼 수 있다. 이 패키지의 syslog라는 Go 패키지를 사용하면 log level과 logging facility 를 지정할 수 있다. 로그 파일로 정보를 보내자 package main import ( \"fmt\" \"log\" \"log/syslog\" \"os\" \"path/filepath\" ) func main() { programName := filepath.Base(os.Args[0]) sysLog, err := syslog.New(syslog.LOG_INFO|syslog.LOG_LOCAL7, programName) syslog.New() 함수의 첫 번째 매개변수로 우선순위를 지정한다. 여기에 로그 종류와 로그 수준을 한꺼번에 조합해서 표현한다. 예를 들어 LOG_NOTICE | LOG_MAIL로 지정하면 로그 종류는 mail이고 로그 수준은 notice인 메시지를 보내게 된다. 또, 실제 수행하는 실행 파일의 이름을 지정하는 것이 바람직하다. 나중에 로그 파일을 보고 정보를 쉽게 찾기 위해서. if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\"LOG_INFO + LOG_LOCAL7: Logging in go\") sysLog, err = syslog.New(syslog.LOG_MAIL, \"Some program\") if err != nil { log.Fatal(err) } else { log.SetOutput(sysLog) } log.Println(\"LOG_MAIL: Logging in go\") fmt.Println(\"Will you see this?\") } 이 것을 실행하면 macOS에서는 $ go runtest.go Will you see this? 이렇게 나오고 로그 파일을 보면 로그가 나온다. $ grep LOG_MAIL /var/log/mail.log Jan 7 22:18:50 jaejin-ui-MacBook-Pro Some program[58311]: 2019/01/07 22:18:50 LOG_MAIL: Logging in go log.Fatal() log.Fatal()은 나쁜일이 발생해서 알려주자 마자 프로그램을 종료하고 싶을 때 사용한다. log.Panic() 다시 실행될 수 없을 정도로 오류가 발생하는 순간, 관련된 정보를 최대한 알 수 있게 해주는 함수이다. ","date":"2019-01-07","objectID":"/mastergo01/:1:5","tags":["go"],"title":"Operating system in golang","uri":"/mastergo01/"},{"categories":["kubernetes"],"content":"Istio란 sidecar pattern을 이용한 service mesh architecture 의 구현체인 오픈 플랫폼이다.","date":"2018-12-30","objectID":"/istio/","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio란 sidecar pattern을 이용한 service mesh architecture 의 구현체인 오픈 플랫폼이다. Istio #4 - Istio 설치와 BookInfo 예제 조대협님 블로그를 참고하였습니다. Istio 설치 쿠버네티스 클러스터가 준비되어 있다면, Istio를 설치한다. ","date":"2018-12-30","objectID":"/istio/:0:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Helm 설치 helm은 kubernetes의 nodejs의 npm 정도라고 생각하면 된다. $ cd ~ $ curl -L https://git.io/getLatestIstio | sh - $ cd istio-{version} $ export PATH=$PWD/bin:$PATH ","date":"2018-12-30","objectID":"/istio/:1:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Helm초기화 helm용 서비스 어카운트를 생성하고 helm을 초기화 한다. $ kubectl create -f install/kubernetes/helm/helm-service-account.yaml $ helm init --service-account tiller ","date":"2018-12-30","objectID":"/istio/:2:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio 설치 설치시 모니터링을 위해 모니터링 도구인 kiali, servicegraph, grafana 설치 옵션을 설정한다. helm install install/kubernetes/helm/istio \\ --name istio \\ --namespace istio-system \\ --set tracing.enabled=true \\ --set global.mtls.enabled=true \\ --set grafana.enabled=true \\ --set kiali.enabled=true \\ --set servicegraph.enabled=true 약간의 시간이 지나고 설치되어있는지 pod 목록을 출력하면 다음과 같이 나온다. pod목록pod목록 \" pod목록 ","date":"2018-12-30","objectID":"/istio/:3:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio 삭제 uninstall using kubectl $ kubectl delete -f $HOME/istio.yaml uninstall using Helm $ helm delete --purge istio If your Helm version is less than 2.9.0, then you need to manually cleanup extra job resource before redeploy new version of Istio chart: $ kubectl -n istio-system delete job --all delete the CRDs $ kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system BookInfo 서비스 예제 설치 ","date":"2018-12-30","objectID":"/istio/:4:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio의 sidecar injection 활성화 Istio는 Pod에 envoy를 sidecar 패턴으로 삽입하여, 트래픽을 컨트롤하는 구조이다. Istio는 이 sidecar를 Pod 생성시 자동으로 주입 (inject)하는 기능이 있는데, 이 기능을 활성화 하기 위해서는 kubernetes의 namespace에 istio-injection=enabled 라는 라벨을 추가해야한다. $ kubectl label namespace default istio-injection=enabled 추가 했다면, 다음과 같이 확인할 수 있다. labellabel \" label ","date":"2018-12-30","objectID":"/istio/:5:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Bookinfo 어플리케이션 배포 Bookinfo yaml 파일은 다음경로를 참고하면된다. istio/istio bookinfo.yaml 파일을 생성하고 다음 명령어로 실행하면 된다. $ kubectl apply -f bookinfo.yaml 배포 완료 후 kubectl get pod 명령어를 실행해보면 다음과 같이 productpage, detail, rating 서비스가 배포되고, reviews 서비스는 v1 ~ v3 까지 배포되는걸 확이 할 수 있다. podpod \" pod kubectl get svc를 이용해 배포되어 있는 서비스를 확인하자. svcsvc \" svc 모두 clusterIP 타입으로 배포 되어 있기 때문에 외부에서는 접근 불가하다. ","date":"2018-12-30","objectID":"/istio/:6:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"Istio gateway 설정 이 서비스를 외부로 노출 시키는데, kubernetes의 Ingress나 Service는 사용하지 않고, Istio의 gateway 를 이용한다. Istio의 gateway는 kubernetes의 커스텀 리소스 타입으로, Istio로 들어오는 트래픽을 받아주는 엔드포인트 역할을 한다. 여러 방법이 있지만 Istio에서는 디폴트로 배포되는 Gateway는 pod 형식으로 배포되어 Load Balancer 타입의 서비스로 서비스 된다. 먼저 Istio Gateway를 등록한 후에, Gateway를 통해 서비스할 호스트를 Virtaul Service로 등록한다. Virtaul ServiceVirtaul Service \" Virtaul Service 조대협님 블로그 아래는 bookinfo에 대한 Gateway를 등록하는 yaml 파일이다. apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# use istio default controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- \"*\" selector를 이용해서 gateway 타입을 istio에서 디폴트로 제공하는 Gateway를 사용하였다. 그리고 HTTP 프로토콜을 80 포트에서 받도록 했다. apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \"*\"gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- uri:exact:/logout- uri:prefix:/api/v1/productsroute:- destination:host:productpageport:number:9080 spec에서 gateways 부분에 앞에서 정의한 bookinfo-gateway를 사용하도록 한다. 이렇게 하면 앞에서 만든 Gateway로 들어오는 트래픽은 이 Virtual Service로 들어와서 서비스 되는데, 여기서 라우팅 룰을 정의한다. /productpage /login /logout /api/v1/products URL은 productpage:9080 으로 포워딩해서 서비스를 제공한다. Istio에 미리 설치 되어 있는 gateway를 살펴보면, Istio default gateway는 pod로 배포되어 있는데, istio=ingressgateway 라는 라벨이 적용되어 있다. gatewaygateway \" gateway service 도 확인해보면 다음과 같다. gatewaygateway \" gateway 이제 bookinfo를 istio gateway에 등록해서 외부로 서비스를 제공해보자. $ istioctl create -f bookinfo-gateway.yaml bookinfo-gateway.yaml 파일은 다음 링크에 있다. istio/istio create gatewaycreate gateway \" create gateway 게이트웨이 배포가 끝나면 위의 service의 EXTERNAL-IP를 이용해서 접속해보자. http://IP/productpage 로 접속해보면 아래와 같이 정상적으로 작동한다. productpageproductpage \" productpage ","date":"2018-12-30","objectID":"/istio/:7:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["kubernetes"],"content":"모니터링 툴 서비스 설치가 끝났으면 간단한 테스트와 함께 모니터링 툴을 이용하여 서비스를 살펴보자. Istio를 설치하면 Prometheus, Grafana, Kiali, Jaeget 등의 모니터링 도구가 기본적으로 인스톨 되어 있다. 각각의 도구를 이용해서 지표들을 모니터링 해보자. Grafana를 이용한 서비스별 지표 모니터링 Grafana를 이용해서 각 서비스들의 지표를 상세하게 모니터링 할 수 있다. 먼저 확인전에 아래 스크립트를 이용해 간단하게 부하를 주자. for i in {1..100}; do curl -o /dev/null -s -w \"%{http_code}\" http://35.197.159.13/productpage done 다음 Grafana 웹 콘솔에 접근해야하는데, Grafana는 외부 서비스로 노출이 안되도록 설정이 되어 있기 때문에 kubectl을 이용해서 Grafana 콘솔에 트래픽을 포워딩 하도록 하자. Grafana는 3000번 포트에서 돌고 있기 때문에, localhost:3000 → Grafana Pod의 3000번 포트로 트래픽을 포워딩 하도록 설정하자. kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 \u0026 다음 localhost:3000 으로 접속해보면 Grafana 화면이 나올 것이다. grafanagrafana \" grafana grafanagrafana \" grafana Jaeger를 이용한 트렌젝션 모니터링 다음은 jaeger를 이용해 개별 분산 트렌젝션에 대해서 각 구간별 응답 시간을 모니터링 할 수 있다. Istio는 각 서비스별로 소요 시간을 수집하는데, 이를 jaeger 오픈소스를 쓰면 쉽게 모니터링이 가능하다. 마찬가지로 jaeger pod로 포트를 포워딩 해야한다. kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 \u0026 이후에 접속하면 Jaeger화면이 보인다. 하지만 나는 error 가 발생한다. .. jaegerjaeger \" jaeger 천천히 해결해야겠다..ㅠㅜ Servicegraph를 이용한 서비스 토폴로지 모니터링 마이크로 서비스는 서비스간 호출 관계가 복잡해서, 각 서비스의 관계를 시각화 해주는 툴이 있으면 유용한데, 대표적인 도구로는 service graph 라는 툴과 kiali 라는 툴이 있다. Bookinfo 예제를 위한 Istio 설정에는 servicegraph가 디폴트로 설치되어 있다. 마찬가지로 외부에 노출하기 위해 포트 포워딩 한다. kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath='{.items[0].metadata.name}') 8088:8088 \u0026 http://localhost:8088/dotviz 로 접속해보면 서비스들의 관계를 볼 수 있다. kialikiali \" kiali 어렵다..ㅠ ","date":"2018-12-30","objectID":"/istio/:8:0","tags":["kubernetes","istio"],"title":"Istio install and deploy sample applications","uri":"/istio/"},{"categories":["go"],"content":"go 1.11 버전에서는 go modules 가 도입되어 시험적으로 사용할 수 있게 되었다. ","date":"2018-12-22","objectID":"/gomod/:0:0","tags":["go"],"title":"Go modules (mod)","uri":"/gomod/"},{"categories":["go"],"content":"GO111MODULE go 1.11 버전에서 go modules가 등장하며 기존 GOPATH 와 vendor/ 에 따라 동작하던 go 커맨드와의 공존을 위한 GO111MODULE 이라는 임시 환경변수가 생성되었다. on go 커맨드는 GOPATH 에 관계없이 go modules의 방식 대로 동작한다. off go modules은 전혀 사용되지 않고 기존에 사용되던 방식대로 GOPATH 와 vendor/ 를 통해 go 커맨드가 동작한다. auto GOPATH/src 내부에서의 go 커맨드는 기존의 방식대로 go 커맨드는 go modules의 방식대로 동작한다. 여기서는 on으로 사용해본다. export GO111MODULE=on 소스코드를 생성하고 소스코드 안에 모듈경로를 추가하면 간단히 go mod init 커맨드를 실행하면 된다. go.mod 파일이 만들어지면서 폴더구조는 다음과 같다. {project} main.go go.mod go.mod 파일에는 현재 모듈에 대한 정보와 코드에서 사용하고 있는 외부 패키지에 대한 의존성 정보가 담기게 된다. 이제 build 해보자. $ go build 자동으로 import된 패키지를 찾고 GOPATH/pkg/mod/ 디렉토리 하위에 버전에 따라 생성된다. 프로젝트 디렉토리에 go.sum 파일도 생성되는데, 이 파일은 설치된 모듈의 해시 값을 저장해두고, 매 go 커맨드가 실행되기 전에 설치 되어있는 모듈의 해시값과 go.sum에 저장된 해시 값을 비교하여 설치된 모듈의 유효성을 검증한다. 다시 한번 폴더구조는 {project} main.go go.mod go.sum *.exe 새로운 모듈을 추가하고 싶다면 go get \u003cmodule-path\u003e@\u003cmodule-query\u003e 커맨드를 이용하면 된다. 버전 지정 필요가 없다면 그냥 코드에 바로 import 시키고 go 명령어를 사용하면 된다. ","date":"2018-12-22","objectID":"/gomod/:1:0","tags":["go"],"title":"Go modules (mod)","uri":"/gomod/"},{"categories":["go"],"content":"Go 언어 에서는 dep 으로 의존성 관리를 하고 있었다. 하지만 Once Go 1.11 is out, it really makes very little sense for a new project not already using dep to start using it. Dep has many, many problems - some of which Sam acknowledged in the talk - and you avoid all of them, avoid taking the time to master a system that is going away, and help make Go modules better by simply using Go modules from the start. 이렇다… 그래도 dep을 한번 사용해보고자 한다. 일단 go 의 폴더 구조를 다음과 같이 만들었다. {GOPATH} bin pkg resources src web 이후에 web이라는 프로젝트에서 진행했다. 먼저 dep을 설치한다. $ go get -u github.com/golang/dep/cmd/dep $ dep version \u003e\u003e\u003e dep: version : v0.5.0 build date : 2018-08-16 git hash : 224a564 go version : go1.10.3 go compiler : gc platform : darwin/amd64 features : ImportDuringSolve=false 설치가 되었으면 dep init 으로 시작해보자. $ dep init 이 명령어를 실행하면 다음과 같은 파일들이 만들어진다. vender / Gopkg.toml Gopkg.toml은 초기화할 때 생성되는 파일로 dep의 동작을 감시하는 여러 규칙을 담고 있다. Gopkg.lock 프로젝트의 의존성 그래프의 스냅숏을 가진 파일이다. 이 파일은 dep ensure 할 때 변경된다. 초기화 후 만들어진 Gopkg.toml 파일에 gin 을 사용하기 위해 다음 내용을 추가한다. gin 은 go 의 웹 프레임워크이다. [[constraint]] name = \"github.com/gin-gonic/gin\" branch = \"master\" 이제 추가한 의존성을 설치한다. $ dep ensure 이건 수동으로 추가해주는 것이구 $ dep ensure -add 설치할 모듈명 으로 추가가 가능한데 위에서 만든 web 프로젝트가 아닌 다른 프로젝트를 하나 만들고 다시 $ dep init 초기화를 한다. $ dep ensure -add github.com/gin-gonic/gin 이렇게 설치 하면 되는데 no dirs contained any Go code 에러가 나면 go 코드가 없어서 난 것이니 코드를 생성한 뒤 진행한다. 설치 하게 되면 자동으로 Gopkg.toml 과 Gopkg.lock에 gin 에 관한 것들이 설치 된다. ","date":"2018-12-22","objectID":"/godep/:0:0","tags":["go"],"title":"Go dependency management tool (dep)","uri":"/godep/"},{"categories":["kubernetes"],"content":"MacOS에서 cluster를 구성한뒤 AWS에 노드들을 띄우는 작업을 해봅시다.","date":"2018-12-21","objectID":"/kops/","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"MacOS에서 cluster를 구성한뒤 AWS에 노드들을 띄우는 작업을 해봅시다. ","date":"2018-12-21","objectID":"/kops/:0:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"kubernetes cluster setting for mac ","date":"2018-12-21","objectID":"/kops/:1:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"설치 먼저 kubernetes를 다루기 위해 kubectl을 다운로드 한다. curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 에러가 날 것인데 최신 릴리즈 버전을 확인하자 curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt 예를들어 위의 결과가 이렇게 나왔다면. v1.12.2 curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.12.2/bin/darwin/amd64/kubectl 이런식으로 다운로드 받으면 된다. chmod +x ./kubectl 실행 모드로 변경하고 sudo mv ./kubectl /usr/local/bin/kubectl Move the binary in to your PATH ","date":"2018-12-21","objectID":"/kops/:1:1","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"설치 확인 kubectl version \u003e Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.2\", GitCommit:\"17c77c7898218073f14c8d573582e8d2313dc740\", GitTreeState:\"clean\", BuildDate:\"2018-10-24T06:54:59Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.6\", GitCommit:\"a21fdbd78dde8f5447f5f6c331f7eb6f80bd684e\", GitTreeState:\"clean\", BuildDate:\"2018-07-26T10:04:08Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} 이런식으로 client와 server가 둘다 나와야 잘 설치 된 것이다. ","date":"2018-12-21","objectID":"/kops/:1:2","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"AWS CLI AWS에 노드를 띄우기 위해 AWS의 설정을 해준다. $ sudo apt update $ sudo apt install -y awscli $ aws configure \u003e public, secret key 입력 ","date":"2018-12-21","objectID":"/kops/:1:3","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Kops 쉽게 클러스터를 전개 하기 위해 kops를 설치한다. $ wget https://github.com/kubernetes/kops/releases/download/1.10.0/kops-darwin-amd64 $ chmod +x kops-darwin-amd64 $ sudo mv kops-darwin-amd64 /usr/local/bin/kops 설치 완료되면 $ kops 명령어가 잘 먹을 것이다. kops로 전개한 AWS EC2에 접속하기 위한 key 생성을 해보자 ","date":"2018-12-21","objectID":"/kops/:1:4","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Key $ ssh-keygen -q -f ~/.ssh/id_rsa -N \"\" or $ kops create secret --name awskrug.k8s.local sshpublickey admin -i ~/.ssh/id_rsa.pub 이제 본격적인 Cluster를 만들어보자. ","date":"2018-12-21","objectID":"/kops/:1:5","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Cluster $ export KOPS_CLUSTER_NAME=awskrug.k8s.local $ export KOPS_STATE_STORE=s3://kops-awskrug-test $ aws s3 mb ${KOPS_STATE_STORE} --region us-east-1 환경 변수 설정을 해주고 S3에 데이터를 저장하기위해 버킷을 생성을 한다. kops create cluster \\ --cloud=aws \\ --name=${KOPS_CLUSTER_NAME} \\ --state=${KOPS_STATE_STORE} \\ --master-size=t2.medium \\ --node-size=t2.medium \\ --node-count=2 \\ --zones=us-east-1b,us-east-1b \\ --ssh-public-key=~/.ssh/k8s.pub \\ --network-cidr=10.10.0.0/16 원하는 대로 설정이 가능하고 --ssh-public-key=~/.ssh/k8s.pub 부분은 위에 Key 생성한 걸로 해주면 노드에 접속할 수 있다. 노드 접속은 ssh -i ~/.ssh/k8s admin@184.73.133.76 이런식으로 EC2 instance의 IP를 써주면 접속이 가능하다. ","date":"2018-12-21","objectID":"/kops/:1:6","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Update Cluster 클러스터 생성 하기 전 수정하기 $ kops edit cluster --name=${KOPS_CLUSTER_NAME} ","date":"2018-12-21","objectID":"/kops/:1:7","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Create Cluster kops update에 –yes 옵션을 주면 실제 클러스터가 생성된다. $ kops update cluster --name=${KOPS_CLUSTER_NAME} --yes ","date":"2018-12-21","objectID":"/kops/:1:8","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"Validate Cluster kops validate 명령으로 생성이 완료 되었는지 확인 할 수 있다. $ kops validate cluster --name=${KOPS_CLUSTER_NAME} create cluster 했는데 생성이 안되면 기다려야한다. 클러스터 생성까지 5~10분 정도소요되기 때문이다. ","date":"2018-12-21","objectID":"/kops/:1:9","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"클러스터 확인 및 수정 다음 명령어로 클러스터(master \u0026 node) 확인 할 수 있다. $ kops get ig AWS 인스턴스들을 정지 시키고 싶을때 혹은 다시 실행 시키고 싶을 때 $ kops edit ig nodes $ kops edit ig \u003cmaster_name\u003e maxSize, minSize를 0으로 바꾸면 인스턴스 생성이 중지된다. 또는 1이나 2같은 숫자를 쓰면 그만큼 생성 된다. update cluster configuration $ kops update cluster --yes Rollling update $ kops rolling-update cluster --yes $ kops rolling-update cluster --cloudonly --force --yes kops로 cluster를 구성하게되면 cluster의 정보가 amazon S3에 저장 되기 때문에 어느 환경에서든 kops와 kubectl만 깔려있으면 어디서든 cluster를 조작할 수 있다. ","date":"2018-12-21","objectID":"/kops/:2:0","tags":["kubernetes","kops"],"title":"Kubernetes cluster on aws with kops","uri":"/kops/"},{"categories":["kubernetes"],"content":"kubernetes","date":"2018-12-20","objectID":"/basic/","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"쿠버네티스의 클러스터 구조는 다음과 같다. 클러스터 전체를 관리하는 컨트롤러로써 마스터가 존재한다. 컨테이너가 배포되는 머신인 노드가 존재한다. 구성구성 \" 구성 조대협님 블로그 ","date":"2018-12-20","objectID":"/basic/:0:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"오브젝트 가장 기본적인 구성단위가 되는 기본 오브젝트와, 이 기본 오브젝트를 생성하고 관리하는 추가적인 기능을 가진 컨트롤러로 이루어진다. 그리고 이러한 오브젝트의 스펙 이외에 추가정보인 메타 정보들로 구성이 된다고 보면 된다. ","date":"2018-12-20","objectID":"/basic/:1:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"오브젝트 스펙 ( object spec ) 오브젝트들은 모두 오브젝트의 특성을 기술한 오브젝트 스펙으로 정의가 되고, 커맨드 라인을 통해서 오브젝트 생성시 인자로 전달하여 정의를 하거나 또는 yaml 이나 json 파일로 스펙을 정의할 수 있다. ","date":"2018-12-20","objectID":"/basic/:1:1","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"기본 오브젝트 ( basic object ) 쿠버네티스에 의해서 배포 및 관리되는 가장 기본적인 오브젝트는 컨테이너화되어 배포되는 애플리케이션의 워크로드를 기술하는 오브젝트로 pod, service, volume, namespace 4가지가 있다. pod → 컨테이너화된 애플리케이션 volume → 디스크 service → 로드밸런서 namespace → 패키지명 정도로 생각하면된다. ","date":"2018-12-20","objectID":"/basic/:1:2","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Pod pod는 쿠버네티스에서 가장 기본적인 배포 단위로 컨테이너를 포함하는 단위이다. apiVersion:v1kind:Podmetadata:name:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:8090 pod은 여러개의 컨테이너를 가질 수 있는데 왜 여러개로 나눠서 배포를 하냐? pod 내의 컨테이너는 IP와 port를 공유한다. 두개의 컨테이너가 하나의 pod를 통해서 배포되었을때 localhost를 통해서 통신이 가능하다. pod내에 배포된 컨테이너간에는 디스크 볼륨을 공유할 수 있다. ","date":"2018-12-20","objectID":"/basic/:2:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"volume pod가 동작할때 디폴트로 컨테이너마다 로컬 디스크를 생성해서 기동되는데 영구적이지 못하다. 따라서 DB같이 영구적으로 파일을 저장해야 하는 경우에는 컨테이너 리스타트에 상관 없이 파일을 영속적으로 저장해야 하는데 이러한 형태를 볼륨이라한다. 쿠버네티스는 github같은 오픈소스 기반의 외장 스토리지를 지원한다.. ","date":"2018-12-20","objectID":"/basic/:3:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"service pod 경우에는 동적으로 생성되고 자동으로 리스타트가 되면서 IP가 바뀌기 때문에, 로드밸런서에서 pod의 목록을 지정할 떄는 IP주소를 이용하는 것은 어렵다. 따라서 유연하게 선택해줘야하는데 이것이 라벨 과 라벨셀렉터 라는 개념이다. 서비스를 정의할때 어떤 pod를 서비스로 묶을 것인지 정의하는데 이를 셀렉터라고 한다. 서비스는 라벨 셀렉터에서 특정 라벨을 가지고 있는 pod만 선택하여 서비스에 묶게 된다. ","date":"2018-12-20","objectID":"/basic/:4:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"name space 네임스페이스는 한 쿠버네티스 클러스터내의 논리적인 분리단위라고 보면 된다. ","date":"2018-12-20","objectID":"/basic/:5:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"컨트롤러 앞의 4개의 기본오브젝트로 애플리케이션을 설정하고 배포하는 것이 가능하다. 이를 더 편하게 관리하기 위해 컨트롤러 라는 개념을 사용한다. Repication controller 지정된 숫자로 pod를 기동 시키고 관리하는 역할을 한다. Replicaset replication controller의 새버전이라 생각하면된다. Deployment Replication controller와 Replica Set의 좀더 상위 추상화 개념이다. 실제 운영은 이를 많이 사용한다. ","date":"2018-12-20","objectID":"/basic/:6:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"고급 컨트롤러 RC,RS,Deployment는 웹서버와 같은 일반적인 워크로드에 대해 pod를 관리하기 위한 컨트롤러이다. 실제 운영환경에서는 웹서버와 같은 일반적인 워크로드 이외에, 데이타베이스, 배치 작업, 데몬 서버와 같이 다양한 형태의 워크로드 모델이 존재한다. ","date":"2018-12-20","objectID":"/basic/:7:0","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Daemonset pod가 각각의 노드에서 하나씩만 돌게 하는 형태로 Pod를 관리하는 컨트롤러 이다. RC나 RS에 의해서 관리되는 Pod는 여러 노드의 상황에 따라 일반적으로 비균등적으로 배포가 되지만, DS에 의해 관리되는 pod는 모든 노드에 균등하게 하나씩만 배포 된다. 이런 형태의 워크로드는 서버의 모니터링이나 로그 수집 용도로 많이 사용되는데, DS의 다른 특징중 하나는, 특정 Node들에만 Pod가 하나씩만 배포 되도록 설정이 가능하다. 또한 DS는 특정 노드에만 pod를 배포할 수 있도록, pod의 node selector를 이용해서 라벨을 이용하여 특정 노드만을 선택할 수 있게 지원한다. ","date":"2018-12-20","objectID":"/basic/:7:1","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Job 워크로드 모델중에서 배치나 한번 실행되고 끝나는 형태의 작업이 있다. 예를 들어 원타임으로 파일 변환 작업을 하거나, 또는 주기적으로 ETL 배치 작업을 하는 경우에는 웹서버 처럼 계속 pod가 떠 있을 필요없이 작업할때만 pod를 띄우면된다. 이를 지원하는 컨트롤러를 job이라한다. Job을 정의할때는 보통 아래와 같이 컨테이너 스펙 부분에 image뿐만 아니라 컨테이너에서 Job을 수행하기 위한 커맨드를 입력한다. apiVersion:batch/v1kind:Jobmetadata:name:pispec:template:spec:containers:- name:piimage:perlcommand:[\"perl\",\"-Mbignum=bpi\",\"-wle\",\"print bpi(2000)\"]restartPolicy:NeverbackoffLimit:4 Job 컨트롤러에 의해서 실행된 pod는 command의 실행 결과에 따라서 job이 실패한지 성공한지를 판단한다. 만약 Job이 끝나기전에 비정상적으로 종료된다면? RC/RS에 의해서 관리 되고 있다면 계속 생성될것이다. 장애시 다시 시작하게 하거나 또는 장애시 다시 시작하지 않게 할 수 있다. ","date":"2018-12-20","objectID":"/basic/:7:2","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Cron job unix cron 처럼 시간에 따른 실행조건을 정의해놓을 수 있고, 이에 따라 job 컨트롤러를 실행하여, 정의된 pod를 실행할 수 있게 한다. apiVersion:batch/v1beta1kind:CronJobmetadata:name:hellospec:schedule:\"*/1 * * * *\"jobTemplate:spec:template:spec:containers:- name:helloimage:busyboxargs:- /bin/sh- -c- date; echo Hello from the Kubernetes clusterrestartPolicy:OnFailure 다른 점은 cronjob 스펙 설정 부분에 schedule 이라는 항목이 있고 반복 조건을 unix cron과 같이 설정하면 된다. ","date":"2018-12-20","objectID":"/basic/:7:3","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["kubernetes"],"content":"Statefulset RS/RC나 다른 컨트롤러로는 데이타베이스와 같이 상태를 가지는 애플리케이션을 관리하기가 어렵다. 그래서 이렇게 데이타 베이스등과 같이 상태를 가지고 있는 Pod를 지원하기 위해서 Statefulset이라는 것이 소개 되었는데 볼륨할때 다시 본다. ","date":"2018-12-20","objectID":"/basic/:7:4","tags":["kubernetes"],"title":"Kubernetes basics","uri":"/basic/"},{"categories":["blockchain"],"content":"스마트 컨트렉트를 쉽게 배포하기 위해서 truffle를 사용해보자. truffle init 초기화 완료되면 contracts, test, migrations 폴더가 생성되고 truffle.js, truffle-config.js가 생성된다. contracts 폴더에는 solidity언어로 작성한 컨트랙트들을 두는곳 이다. test폴더에는 컨트랙트를 테스트할 때 사용하는 파일을 두는 곳이다. truffle.js는 트러플에서 컨트랙트를 배포할 네트워크 설정, 컴파일러 언어 설정등 트러플과 관련된 설정을 하는 파일이다. truffle-config.js 파일은 truffle.js를 설정할 예시를 보여주는 파일이다. ","date":"2018-08-05","objectID":"/ethereum-dapp5/:0:0","tags":["blockchain","ethereum"],"title":"Deploying smart contracts with truffle","uri":"/ethereum-dapp5/"},{"categories":["blockchain"],"content":"트러플 설정하기 truffle.js module.exports = { // See \u003chttp://truffleframework.com/docs/advanced/configuration\u003e // to customize your Truffle configuration! networks: { development: { host: \"172.17.0.6\", port: \"8545\", network_id: \"*\", } } }; geth 실행시에 설정한 값들을 집어 넣고 잘 동작하는지 확인하자 truffle console 또는 truffle console -network development geth는 당연히 구동 상태여야한다. truffle(development)\u003e web3.eth.coinbase '0xff53da3bf5403bbe86117a65db3c1e93f8adb9d5' coinbase 주소가 나오면 잘 된거다. ","date":"2018-08-05","objectID":"/ethereum-dapp5/:1:0","tags":["blockchain","ethereum"],"title":"Deploying smart contracts with truffle","uri":"/ethereum-dapp5/"},{"categories":["blockchain"],"content":"컨트랙트 컴파일 트러플에서 컨트랙트를 컴파일 할 때 사용하는 명령어는 아래와 같다. truffle compile contracts/ 폴더 안에 들어있는 모든 파일이 컴파일 진행된다. 전에 작성했던 simple.sol을 컴파일해보자. pragma solidity ^0.4.23; contract Simple { uint256 data; function set (uint256 _data)public { data = _data; } function get()public view returns(uint256){ return data; } } root@d08fa5cce2b8:/home/truffle_example# truffle compile Compiling ./contracts/Migrations.sol... Compiling ./contracts/simple.sol... Writing artifacts to ./build/contracts build폴더를 만들고 해당 폴더에 컴파일이 완료된 정보가 담겨있는 파일들이 생성된다. build/contracts 폴더가 생성되고 json파일이 생성된걸 볼 수 있다. ","date":"2018-08-05","objectID":"/ethereum-dapp5/:2:0","tags":["blockchain","ethereum"],"title":"Deploying smart contracts with truffle","uri":"/ethereum-dapp5/"},{"categories":["blockchain"],"content":"컨트랙트 배포하기 컨트랙트를 배포하기 위해서는 migrations폴더에 배포할 컨트랙트에 맞게 파일을 하나 작성해야한다. migrations폴더 안에는 1_inital_migration.js 파일이 존재하는데 컨트랙트를 배포하기 위해서는 artifacts.require()로 배포를 원하는 컨트랙트의 정보를 획득 한 후 deployer.deploy()를 사용해서 배포를 하는 과정임을 알 수 있다. 수정안하는것이 좋다. 2_inital_migration.js 파일을 생성하고 var Simple = artifacts.require(\"Simple\"); module.exports = function(deployer) { deployer.deploy(Simple); }; 작성해주고 배포를 해보자.! geth는 miner.start()로 채굴시작하고 truffle migrate Using network 'development'. Running migration: 1_initial_migration.js Deploying Migrations... ... 0x896fe9568a631ba698daf89a6f8d2dbef1fe286f24c9e228ba58cc8d03134e33 Migrations: 0x9d5d062e7cd54ce3b22f79e9433d41cdc8c253bb Saving successful migration to network... ... 0x003250f43b24e185b6869feda9f1b1f1778f29aa1f3ac9add8115da18d449aa4 Saving artifacts... Running migration: 2_initial_migration.js Deploying Simple... ... 0x4be336fbffff3ee8ec8ecd3eb8fee0c4bae0b913d032b2ac00dd95ad1861a81c Simple: 0x0d5c931ba28407a74e589940c1538792b017e6b2 Saving successful migration to network... ... 0x38f8ec8bd3de407410285d3aec267a989c038e049079ab793bc649458d129155 Saving artifacts... 이런 식의 결과가 나오면 성공이다. gas limit 에러가 뜨면 truffle.js의 내용을 module.exports = { // See \u003chttp://truffleframework.com/docs/advanced/configuration\u003e // to customize your Truffle configuration! networks: { development: { host: \"172.17.0.6\", port: \"8545\", network_id: \"*\", gas: 2342414, } } }; 다음과 같이 gas를 추가해주고 어카운트 락 에러가 나면 언락을 해주면된다. ","date":"2018-08-05","objectID":"/ethereum-dapp5/:2:1","tags":["blockchain","ethereum"],"title":"Deploying smart contracts with truffle","uri":"/ethereum-dapp5/"},{"categories":["blockchain"],"content":"컨트랙트 사용하기 truffle console 들어가서 Simple.deployed().then((instance)=\u003e{return instance.get.call()}).then((result) =\u003e { data = result }); 를 입력해 data 변수에 컨트랙트에 저장된 값을 저장한다. data.toNumber() 0의 값이 출력되면 성공 !! Simple.deployed()를 호출하면 배포된 simple 컨트랙트의 객체가 반환된다. 해당 객체를 instance 변수로 받고 get함수를 호출해서 컨트랙트에 저장된 변수의 값을 가져온다. 컨트랙트이름.deployed()를 사용한다는점 기억하기. set 함수를 사용해 변수에 데이터를 저장해보자. Simple.deployed().then((instance) =\u003e { return instance.set(100) }) 제대로 수행이 되면 geth에 정보가 출력된다. \u003e INFO [09-01|10:49:03.989] Submitted transaction fullhash=0xd901f598d41332a4a292bca74874c335672f26588bdd3567bd499d51f3626e5c recipient=0x0d5c931bA28407A74E589940c1538792b017e6B2 이제 mining을 하고 get함수를 실행 해보자. Simple.deployed().then((instance)=\u003e{return instance.get.call()}).then((result) =\u003e { data = result }); data.toNumber(); 100이 출력 되면 성공이다 ! ex) Voting.deployed().then((instance)=\u003e{return instance.getNumOfCandidates.call()}).then((result) =\u003e {data = result}); data.toNumber() truffle(development)\u003e Voting.deployed().then((instance)=\u003e{return instance.addCandidate(0xc75711ce6897078ffd6af868fda11c8e9f0fc311,0xc75711ce6897078ffd6af868fda11c8e9f0fc312)}) { tx: '0xb5adb6bd95815ffacd1197bc68685cbea87b92dc62fd6cd03997e756d8e43259', receipt: { blockHash: '0x0047122a32f44fb2a200fc9cc7f30e89dd7501989b288e10a7e6fffd22dd70b0', blockNumber: 339, contractAddress: null, cumulativeGasUsed: 90823, from: '0xff53da3bf5403bbe86117a65db3c1e93f8adb9d5', gasUsed: 90823, logs: [ [Object] ], logsBloom: '0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000200000000001000000000000000000000080000000008000000000000000000000000000000000000000000000000000000', root: '0x774c811955165298d0d2271c4285144f9915ec111749c1988c800df0c3a8aaa1', to: '0x5f9431c70262d23273f4d9ddb4f15d3906497dc9', transactionHash: '0xb5adb6bd95815ffacd1197bc68685cbea87b92dc62fd6cd03997e756d8e43259', transactionIndex: 0 }, logs: [ { address: '0x5f9431c70262d23273f4d9ddb4f15d3906497dc9', blockNumber: 339, transactionHash: '0xb5adb6bd95815ffacd1197bc68685cbea87b92dc62fd6cd03997e756d8e43259', transactionIndex: 0, blockHash: '0x0047122a32f44fb2a200fc9cc7f30e89dd7501989b288e10a7e6fffd22dd70b0', logIndex: 0, removed: false, event: 'AddedCandidate', args: [Object] } ] } unlock personal.unlockAccount(\"0x6ea45f74c9803a9c3403c400975424e5825dfb70\") ","date":"2018-08-05","objectID":"/ethereum-dapp5/:3:0","tags":["blockchain","ethereum"],"title":"Deploying smart contracts with truffle","uri":"/ethereum-dapp5/"},{"categories":["blockchain"],"content":"node.js의 web3를 이용해서 ethereum network를 web에서 접속해보자. ","date":"2018-08-04","objectID":"/ethereum-dapp4/:0:0","tags":["blockchain","ethereum"],"title":"Connecting to ethereum network using web3","uri":"/ethereum-dapp4/"},{"categories":["blockchain"],"content":"eth 컨테이너 일단 eth node 에 geth를 실행시켜놔야한다. geth --datadir \"./data\" --identity \"mynetwork\" --networkid 1988 --rpc --rpcport 8545 --rpcapi \"db,net,web3,eth,personal\" --rpccorsdomain \"*\" --rpcaddr \"0.0.0.0\" --rpcvhosts=* --nodiscover console 그럼 web3 8545 포트로 rpc접속 가능하다. ","date":"2018-08-04","objectID":"/ethereum-dapp4/:0:1","tags":["blockchain","ethereum"],"title":"Connecting to ethereum network using web3","uri":"/ethereum-dapp4/"},{"categories":["blockchain"],"content":"node.js 컨테이너 docker node.js 컨테이너를 만들고 $ npm install web3 --save $ npm install ethereum/web3.js --save 두개의 npm모듈 설치하고 var Web3 = require(\"web3\"); web3 = new Web3(new Web3.providers.HttpProvider(\"http://relaxed_lamport:8545\")); var network_version = web3.version.network; console.log(network_version); 이 코드를 app.js에 넣고 실행하면 network_version ( 1988 )이 잘 출력된다. 내가 1988이라 설정해줬으므로 eth geth에서 .. Network versionNetwork version \" Network version ","date":"2018-08-04","objectID":"/ethereum-dapp4/:0:2","tags":["blockchain","ethereum"],"title":"Connecting to ethereum network using web3","uri":"/ethereum-dapp4/"},{"categories":["blockchain"],"content":"이더 전송하기 var accounts = web3.eth.accounts; //account 정보 가져오기 var from_account = accounts[0]; var to_account = accounts[1]; var transactionObj = { //transaction object 설정 from: from_account, to:to_account, value: 1000 }; web3.eth.sendTransaction(transactionObj) app.js 에 이 부분을 추가 해준다. 그전에 geth에서 **personal.unlockAccount(eth.accounts[0])**를 실행시켜줘야한다. 이것도 web에서 변경할 수 있는 것을 찾아야 할 것이다. TransactionTransaction \" Transaction 위가 geth , 밑에가 node.js 이다. 잘 전송되는것을 확인했다. 하지만 아직 전송이 되지않았다. 왜냐하면 블럭을 생성 안했기 떄문에.. geth에서 miner.start()를 실행해준다. ","date":"2018-08-04","objectID":"/ethereum-dapp4/:0:3","tags":["blockchain","ethereum"],"title":"Connecting to ethereum network using web3","uri":"/ethereum-dapp4/"},{"categories":["blockchain"],"content":"web3에서 스마트컨트랙트 사용하기 먼저 이전에 스마트컨트랙트 배포가 이루어진 후에 이루어져야한다. 스마트 컨트랙트의 address를 알아야하기 때문에.. app.js에 다음 코드를 추가한다. var contractAbi = [{\"constant\":false, \"inputs\":[{\"name\":\"_data\", \"type\":\"uint256\"}],\"name\":\"set\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"}]; var contractAddress = \"0xab2274abf1e4712bba53ffba1c53ba1e54fbeb4d\"; var contract = web3.eth.contract(contractAbi); var contractInstance = contract.at(contractAddress); var result = contractInstance.get(); console.log(result.toString()); var contractAddress는 스마트 컨트랙트의 주소이다. root@84b8979094ea:/home/web# node app.js 1988 1000 1000이 잘 출력된다. ","date":"2018-08-04","objectID":"/ethereum-dapp4/:0:4","tags":["blockchain","ethereum"],"title":"Connecting to ethereum network using web3","uri":"/ethereum-dapp4/"},{"categories":["blockchain"],"content":"이전에 만들었던 ethereum network에 스마트 컨트랙트를 배포해보자. $ apt-get install solc $ solc --version simple.sol 같은 파일을 만들어서 pragma solidity ^0.4.11; contract Simple{ uint256 data; function get() constant public returns(uint256) { return data; } function set (uint256 _data) public{ data= _data; } } 같이 저장하고 컴파일한다. $ solc simple.sol 성공했으면 아무런 결과가 안나온다. 스마트컨트랙트 배포할때 필요한 것이 abi bin 이다. root@9b88624ec1c4:/home# solc --abi simple.sol ======= simple.sol:Simple ======= Contract JSON ABI [{\"constant\":false,\"inputs\":[{\"name\":\"_data\",\"type\":\"uint256\"}],\"name\":\"set\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"}] root@9b88624ec1c4:/home# solc --bin simple.sol ======= simple.sol:Simple ======= Binary: 608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a72305820d55de6a6db862805a1312cf86887b07129afb5bd7a1313a1322c90bc1cb2a5070029 이걸 적어넣고 geth를 실행한다. \u003e var simpleAbi = [{\"constant\":false,\"inputs\":[{\"name\":\"_data\",\"type\":\"uint256\"}],\"name\":\"set\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"}] 위의 값을 입력해준다. \u003e simpleAbi [{ constant: false, inputs: [{ name: \"_data\", type: \"uint256\" }], name: \"set\", outputs: [], payable: false, stateMutability: \"nonpayable\", type: \"function\" }, { constant: true, inputs: [], name: \"get\", outputs: [{ name: \"\", type: \"uint256\" }], payable: false, stateMutability: \"view\", type: \"function\" }] 잘나온다. binarycode도 설정해줘야한다. 유의할점은 16진수임을 알려주기위해 앞에 0x 를 추가한다. \u003e var simpleBin = \"0x608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a72305820d55de6a6db862805a1312cf86887b07129afb5bd7a1313a1322c90bc1cb2a5070029\" undefined \u003e simpleBin \"0x608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a72305820d55de6a6db862805a1312cf86887b07129afb5bd7a1313a1322c90bc1cb2a5070029\" 잘되었다. 이제 컨트랙트를 만들어보자. eth.contract()를 이용해 abi를 설정해줘야한다. 그 후에 컨트랙트를 배포하는 계좌가 누구인지, 얼마만큼 가스를 소비할 것인지, 배포할 컨트랙트의 데이터가 무엇인지 명시해줘야한다. \u003e var simpleContract = eth.contract(simpleAbi) undefined \u003e var simpleTransferObject = {from: eth.accounts[0], data: simpleBin, gas: 2000000}; undefined 배포하기전에 첫번째 계좌에서 가스를 소비해야하니 락을 풀어주자 personal.unlockAccount(eth.accounts[0]); simpleContract.new(simpleTransferObject)를 입력하면 배포가 시작된다. \u003e var contractObj = simpleContract.new(simpleTransferObject); INFO [08-25|05:22:07.056] Submitted contract creation fullhash=0x788543be4447cee4f518fdc077d20c88b69a7993d4d0358794a1b46a810134d6 contract=0xAB2274abf1E4712BBA53FfBA1C53bA1e54fbeb4d undefined contractObj 객체를 통해 컨트랙트 정보를 얻을 수 있다. \u003e contractObj { abi: [{ constant: false, inputs: [{...}], name: \"set\", outputs: [], payable: false, stateMutability: \"nonpayable\", type:","date":"2018-08-03","objectID":"/ethereum-dapp3/:0:0","tags":["blockchain","ethereum"],"title":"Deploy smart contract","uri":"/ethereum-dapp3/"},{"categories":["blockchain"],"content":"배포된 컨트랙트 사용하기 배포된 컨트랙트를 사용하기 위해서는 컨트랙트의 abi와 주소값이 필요하다. eth.contract 함수에 abi값을 인자로 넘겨준다. \u003e contract2 = eth.contract([{\"constant\":false,\"inputs\":[{\"name\":\"_data\",\"type\":\"uint256\"}],\"name\":\"set\",\"outputs\":[],\"payable\":false,\"stateMutability\":\"nonpayable\",\"type\":\"function\"},{\"constant\":true,\"inputs\":[],\"name\":\"get\",\"outputs\":[{\"name\":\"\",\"type\":\"uint256\"}],\"payable\":false,\"stateMutability\":\"view\",\"type\":\"function\"}] ... ) { abi: [{ constant: false, inputs: [{...}], name: \"set\", outputs: [], payable: false, stateMutability: \"nonpayable\", type: \"function\" }, { constant: true, inputs: [], name: \"get\", outputs: [{...}], payable: false, stateMutability: \"view\", type: \"function\" }], eth: { accounts: [\"0x241530b417837cdcce8036a7b7f095995509d5c9\", \"0xca09b7785f4179988e14ee87cfe7e93933a002c7\"], blockNumber: 24, coinbase: \"0x241530b417837cdcce8036a7b7f095995509d5c9\", compile: { lll: function(), serpent: function(), solidity: function() }, defaultAccount: undefined, defaultBlock: \"latest\", gasPrice: 18000000000, hashrate: 0, mining: false, pendingTransactions: [], protocolVersion: \"0x3f\", syncing: false, call: function(), contract: function(abi), estimateGas: function(), filter: function(options, callback, filterCreationErrorCallback), getAccounts: function(callback), getBalance: function(), getBlock: function(), getBlockNumber: function(callback), getBlockTransactionCount: function(), getBlockUncleCount: function(), getCode: function(), getCoinbase: function(callback), getCompilers: function(), getGasPrice: function(callback), getHashrate: function(callback), getMining: function(callback), getPendingTransactions: function(callback), getProtocolVersion: function(callback), getRawTransaction: function(), getRawTransactionFromBlock: function(), getStorageAt: function(), getSyncing: function(callback), getTransaction: function(), getTransactionCount: function(), getTransactionFromBlock: function(), getTransactionReceipt: function(), getUncle: function(), getWork: function(), iban: function(iban), icapNamereg: function(), isSyncing: function(callback), namereg: function(), resend: function(), sendIBANTransaction: function(), sendRawTransaction: function(), sendTransaction: function(), sign: function(), signTransaction: function(), submitTransaction: function(), submitWork: function() }, at: function(address, callback), getData: function(), new: function() } at함수에 컨트랙트의 주소를 설정하면된다. \u003e var contractObj2 = contract2.at(\"0xab2274abf1e4712bba53ffba1c53ba1e54fbeb4d\") undefined \u003e contractObj2.get() 1000 위에 address를 가져와서 실행한결과로 1000이 잘 출력된다. ","date":"2018-08-03","objectID":"/ethereum-dapp3/:0:1","tags":["blockchain","ethereum"],"title":"Deploy smart contract","uri":"/ethereum-dapp3/"},{"categories":["blockchain"],"content":"이전에 geth로 생성한 private net를 rpc 옵션을 사용해서 클라이언트 앱 연동 (ethereum wallet)을 해보자. 연동할 앱은 remix이다. remix는 이더리움 스마트 컨트랙트를 만들 때 사용하는 언어인 solidity ide 툴이다. ethereum/remix-ide 다운을 받아서 압축을 푼뒤 index.html 을 실행하면 된다. 나는 도커로 했기 때문에 컨테이너의 80번 포트를 뚫어주고 접속하였다. IDEIDE \" IDE 컨테이너 80 번하고 host의 8888번 포트를 연결해서 접속한 결과이당 ","date":"2018-08-02","objectID":"/ethereum-dapp2/:0:0","tags":["blockchain","ethereum"],"title":"Preparing for deploy smart contract","uri":"/ethereum-dapp2/"},{"categories":["blockchain"],"content":"스마트 컨트랙트 작성하기 새로운 simple.sol 파일을 만들어서 pragma solidity ^0.4.11; contract Simple{ uint256 data; function get() constant public returns(uint256) { return data; } function set (uint256 _data) public{ data= _data; } } 다음과 같이 작성했다. IDEIDE \" IDE 작성후 오른쪽에 start to compil 을 누르면 밑에 녹색 바탕의 simple처럼 스마트 컨트랙트의 이름이 표시된다. ","date":"2018-08-02","objectID":"/ethereum-dapp2/:1:0","tags":["blockchain","ethereum"],"title":"Preparing for deploy smart contract","uri":"/ethereum-dapp2/"},{"categories":["blockchain"],"content":"Remix을 이용한 스마트 컨트랙트 배포 remix에서는 세 종류의 배포 방법을 제공한다. run 패널로 이동해서 environment를 클릭해보면 javascript vm , injected Web3 , Web3 provider 가 있다. IDEIDE \" IDE javascript vm을 선택하면 이더리움 노드 없이 가상으로 스마트 컨트랙트를 배포하고 실행해 볼 수 있다. 실제로 배포 하기 전 테스트용으로 사용하기 좋다. injected Web3는 metamask와 같은 브라우저 플러그인과 연동할 때 사용한다. 다른 클라이언트 앱과 연동할 때 사용하는 걸로 생각하면 된다. web3 providersms private network와 연동할 대 사용하는 옵션이다. 우선 javascript vm환경에서 제대로 실행되는지 확인해보자. IDEIDE \" IDE javascript vm을 선택하고 create를 누르면 밑에 get, set함수가 나타난다. 먼저 set옆에 100을 누른다음에 get을 누르면 100이 나오는 걸 확인할 수 있다. IDEIDE \" IDE ","date":"2018-08-02","objectID":"/ethereum-dapp2/:1:1","tags":["blockchain","ethereum"],"title":"Preparing for deploy smart contract","uri":"/ethereum-dapp2/"},{"categories":["blockchain"],"content":"private network와 연결하고 스마트컨트랙트 배포하고 사용하기 remix를 private network와 연동하기 위해서는 network를 실행 할 때 geth에 rpc 옵션을 설정해줘야 한다. ( Ethereum wallet과는 ipc를 통해 연결 한다.) –rpc 는 http-rpc 서버를 사용하게 하는 옵션이다. –rpcaddr은 rpc로 사용하는 주소를 명시하는 옵션이고 기본은 localhost이다. –rpcport는 rpc로 사용하는 포트를 지정하는 옵션이다. 기본은 8545이다. –rpcapi는 rpc로 접속한 앱에서 사용할 수 있는 api 종류를 지정하는 옵션이다. –rpccorsdomain은 network에 접속할 도메인을 지정할 수 있는 옵션이다. rpcaadr, rpcport는 기본으로 하고 rpcapi 는 ‘db, eth, net, web3’ 을 rpccorsdomain은 “*“을 지정한다. *는 모든 도메인에서 접근하게 하는것이다. 실제로 서비스 할 떄는 *를 사용하지 않는게 좋다. geth를 실행한다. $ geth --datadir \"./data\" --identity \"mynetwork\" --networkid 1988 --rpc --rpcport 8545 --rpcapi \"db,net,web3,eth\" --rpccorsdomain \"*\" --rpcaddr \"0.0.0.0\" console $ geth --datadir \"./data\" --identity \"mynetwork\" --networkid 1988 --rpc --rpcport 8545 --rpcapi \"db,net,web3,eth\" --rpccorsdomain \"*\" --rpcaddr \"0.0.0.0\" console 나는 docker 로 했기 때문에 rpcaddr 0.0.0.0을 추가해줬다. 이후 remix로 돌아와 remix의 environment를 web3 provider로 변경한다. 그러면 IDEIDE \" IDE 다음과 같이 경고가 뜨고 ok를 누른다. IDEIDE \" IDE 주소를 확인하고 ok IDEIDE \" IDE 연결에 성공했다!! IDEIDE \" IDE 하지만 에러가 발생하는데 스마트 컨트랙트를 배포할 때는 gas를 사용하게 된다. 그래서 컨트랙트를 배포하는 계좌에서 gas를 소비할 수 있게 락을 풀어줘야한다. geth 콘솔창으로 이동후에 \u003e personal.unlockAccount(eth.accounts[0]) Unlock account 0x0a39454d85c9ff43f5aba2025da36d67ccfa50d9 Passphrase: true 락을 풀어주고 다시 Remix로 이동해서 create를 클릭한다. IDEIDE \" IDE javascript vm 환경에서 했던 것과 다르게 바로 컨트랙트가 배포되지 않고 pending단계로 빠지는 것을 볼 수 있다. 채굴을 해줘야 마무리가 되기 때문에 geth에 가서 miner.start()를 해준다. \u003e INFO [08-18|08:26:59.024] Submitted contract creation fullhash=0x7e4c9193d46ec3cfda05dc021d339351fc3a7259ec83fc74e99e602ec5f069e7 contract=0x78dAD2e8384B6ce2052f0626eCb51b6b0A5d77b4 ","date":"2018-08-02","objectID":"/ethereum-dapp2/:1:2","tags":["blockchain","ethereum"],"title":"Preparing for deploy smart contract","uri":"/ethereum-dapp2/"},{"categories":["blockchain"],"content":"docker를 이용해서 이더리움 노드 3개와 웹서버 1개를 만들고 웹서버에서 회원가입을 하고 투표를 하는 정보들을 이더리움 네트워크에 저장시켜보는 실습을 해본다. $ docker pull ubuntu $ docker run -it -p 10001:10001 ubuntu ubuntu os에 설치하려고한다. 이유는 설치하는 과정을 보여주려고…. $ apt-get install software-properties-common $ apt-get install software-properties-common python-software-properties $ add-apt-repository -y ppa:ethereum/ethereum $ apt-get update $ apt-get install ethereum 이후에 $ geth $ geth version \u003e\u003e\u003e WARN [08-17|08:21:11.660] Sanitizing cache to Go's GC limits provided=1024 updated=666 Geth Version: 1.8.13-stable Git Commit: 225171a4bfcc16bd12a1906b1e0d43d0b18c353b Architecture: amd64 Protocol Versions: [63 62] Network Id: 1 Go Version: go1.10.1 Operating System: linux GOPATH= GOROOT=/usr/lib/go-1.10 라고 하면 잘 실행 될 것이다. 아래의 명령어를 사용하면 geth console에 명령어를 입력 할 수 있다. $ geth --dev console 준비할 것이 2가지 있다. private network에서 사용할 계좌 제네시스 블록 정보. ","date":"2018-08-01","objectID":"/ethereum-dapp1/:0:0","tags":["blockchain","ethereum"],"title":"Setup private ethereum blockchain network","uri":"/ethereum-dapp1/"},{"categories":["blockchain"],"content":"계좌 생성하기 $ geth --datadir \"./data\" account new —datadir 옵션은 network가 구동될 때 블록정보, 계좌정보를 저장할 폴더를 지정하는 옵션이다. $ ls data/keystore/ \u003e\u003e\u003e UTC--2018-08-17T08-24-17.766131786Z--49a13ee73d2fc8c1578ce1512530aa1724ee42b7 계좌주소 출력하기 $ geth --datadir \"./data\" account list \u003e\u003e\u003e WARN [08-17|08:26:02.392] Sanitizing cache to Go's GC limits provided=1024 updated=666 INFO [08-17|08:26:02.392] Maximum peer count ETH=25 LES=0 total=25 Account #0: {49a13ee73d2fc8c1578ce1512530aa1724ee42b7} keystore:///home/data/keystore/UTC--2018-08-17T08-24-17.766131786Z--49a13ee73d2fc8c1578ce1512530aa1724ee42b7 다음과 같이 쓰면 생성된 계좌의 리스트가 출력된다. ","date":"2018-08-01","objectID":"/ethereum-dapp1/:1:0","tags":["blockchain","ethereum"],"title":"Setup private ethereum blockchain network","uri":"/ethereum-dapp1/"},{"categories":["blockchain"],"content":"제네시스 블록 생성하기 private network를 구동하기 위해서는 첫 번째 블록을 생성해야한다. { \"config\": { \"chainId\": 15, \"homesteadBlock\": 0, \"eip155Block\": 0, \"eip158Block\": 0 }, \"difficulty\": \"200000000\", \"gasLimit\": \"2100000\", “coinbase”: \"0x{위에 생성한 계좌주소}\", \"alloc\": { \"0x{위에 생성한 계좌주소}\": { \"balance\": \"1000000000000000000000000\" }, } } 제네시스 블록을 만들때 필요한 json 파일의 구성이다. config는 생성할 private network의 버전을 정하는 설정이다. chainId, homesteadBlock, eip155Block, eip158Block 모두 이더리움 속성과 관련된 설정이다. chainId는 replay protection을 사용하는 설정이다. homestead는 이더리움의 두 번째 메인 릴리즈를 사용하기 위해 설정하는 속성이고 0으로 설정하면 해당 릴리즈를 사용한다. eip155은 gas 비용에 관련된 하드포커 설정이며 eip158은 state cleanig과 관련된 설정이다. difficulty 는 이더리움에서 채굴의 난이도를 설정하는 옵션 gas limit 한 블록이 담을 수 있는 gas의 수치를 말한다. gas 수치가 높을 수록 한 블록이 담을 수 있는 거래가 많아진다. coinbase는 network를 구동할 때 기본으로 사용할 계좌를 설정하는 옵션 ( 채굴할 때 코인을 받을 계좌 ) alloc 는 private network를 구동할 떄 미리 계좌에 이더를입금 시킬지를 설정하는 옵션이다. 설정하기 원하는 계좌와 이더 값을 명시한다. 제네시스 json파일을 생성후 $ geth --datadir \"./data\" init \"./genesis.json\" \u003e\u003e\u003e WARN [08-17|08:31:55.253] Sanitizing cache to Go's GC limits provided=1024 updated=666 INFO [08-17|08:31:55.254] Maximum peer count ETH=25 LES=0 total=25 INFO [08-17|08:31:55.254] Allocated cache and file handles database=/home/data/geth/chaindata cache=16 handles=16 INFO [08-17|08:31:55.261] Writing custom genesis block INFO [08-17|08:31:55.261] Persisted trie from memory database nodes=1 size=150.00B time=41.229µs gcnodes=0 gcsize=0.00B gctime=0s livenodes=1 livesize=0.00B INFO [08-17|08:31:55.261] Successfully wrote genesis state database=chaindata hash=e47869…3ca16c INFO [08-17|08:31:55.261] Allocated cache and file handles database=/home/data/geth/lightchaindata cache=16 handles=16 INFO [08-17|08:31:55.264] Writing custom genesis block INFO [08-17|08:31:55.264] Persisted trie from memory database nodes=1 size=150.00B time=37.247µs gcnodes=0 gcsize=0.00B gctime=0s livenodes=1 livesize=0.00B INFO [08-17|08:31:55.265] Successfully wrote genesis state database=lightchaindata hash=e47869…3ca16c 성공적으로 genesis block이 생성 되었다고 출력된다. ","date":"2018-08-01","objectID":"/ethereum-dapp1/:2:0","tags":["blockchain","ethereum"],"title":"Setup private ethereum blockchain network","uri":"/ethereum-dapp1/"},{"categories":["blockchain"],"content":"Private network 구동하기 첫 번째 블록을 만들었으니 네트워크 실행해보자. private network를 구동할 때 유의해야 할 옵션은 —networkid이다. networkid를 1로 지정시 메인 이더리움에 연결이 되고 2는 테스트 네트워크로 사용중이다. 따라서 1과 2를 제외한 수를 지정해야 한다. 또한 –identity 옵션을 사용해 네트워크에 이름을 지정할 수 있다. networkid와 identity는 node들 간에 연결 할 때 사용되는 옵션이다. 그리고 –mine 옵션을 설정해서 geth를 구동했을 때 채굴을 할 수 있게 한다. –nodiscover 옵션을 사용해서 다른 노드를 찾는 설정을 끈다. $ geth --networkid 1988 --identity \"mynetwork\" --datadir \"./data\" --nodiscover console 아래와 같이 콘솔창이 나오면 성공이다. \u003e\u003e\u003e ... 생략 INFO [08-17|08:35:18.788] Etherbase automatically configured address=0x49A13ee73D2FC8C1578cE1512530Aa1724EE42b7 coinbase: 0x49a13ee73d2fc8c1578ce1512530aa1724ee42b7 at block: 0 (Thu, 01 Jan 1970 00:00:00 UTC) datadir: /home/data modules: admin:1.0 debug:1.0 eth:1.0 miner:1.0 net:1.0 personal:1.0 rpc:1.0 txpool:1.0 web3:1.0 \u003e eth.accounts를 입력하면 현재 생성되어 있는 계좌들이 출력된다. \u003e eth.accounts [\"0x49a13ee73d2fc8c1578ce1512530aa1724ee42b7\"] eth.accounts[0]은 첫번째 생성된 계좌를 출력 \u003e eth.accounts[0] \"0x49a13ee73d2fc8c1578ce1512530aa1724ee42b7\" 이더 잔고를 조회 할때는 eth.getBalance()함수를 사용하고 인자로 계좌 주소를 적어준다. \u003e eth.getBalance(eth.accounts[0]) 1e+24 새로운 계좌를 만들어보면 \u003e personal.newAccount() Passphrase: Repeat passphrase: \"0xaefa6e0f4b91fd67e31498f82f34ae95762aac71\" \u003e eth.accounts [\"0x49a13ee73d2fc8c1578ce1512530aa1724ee42b7\", \"0xaefa6e0f4b91fd67e31498f82f34ae95762aac71\"] 두개가 출력된걸 확인 할 수 있다. 이제 이더를 옮겨보자 반드시 해야하는 작업이 이더를 전송하려는 계좌의 락을 풀어줘야한다.!!! \u003e personal.unlockAccount(eth.accounts[0]) Unlock account 0x0a39454d85c9ff43f5aba2025da36d67ccfa50d9 Passphrase: true 이제 보내보자 \u003e eth.sendTransaction({from: eth.accounts[0], to:eth.accounts[1], value:100}) INFO [08-18|06:27:14.550] Submitted transaction fullhash=0xd592ae1540ae288979caa5fe8bcfc4bc41c625190b978bb505587b241fdb346e recipient=0xa6ae554222c1Ac7cB98eEF8abFd3b8f8CD9C2817 \"0xd592ae1540ae288979caa5fe8bcfc4bc41c625190b978bb505587b241fdb346e\" 전송을 해도 잔고에 변화가 없다. \u003e eth.getBalance(eth.accounts[0]) 1e+24 \u003e eth.getBalance(eth.accounts[1]) 0 블록 체인이기 때문에 채굴이 되어야 발생한 거래가 적용이 된다. 채굴을 해보자 $ miner.start() $ minet.stop() INFO [08-18|06:32:51.972] Generating DAG in progress epoch=1 percentage=97 elapsed=3m4.253s INFO [08-18|06:32:53.809] Generating DAG in progress epoch=1 percentage=98 elapsed=3m6.089s INFO [08-18|06:32:56.898] Generating DAG in progress epoch=1 percentage=99 elapsed=3m9.179s INFO [08-18|06:32:56.905] Generated ethash verification cache epoch=1 elapsed=3m9.184s INFO [08-18|06:35:20.852] Successfully sealed new block number=1 hash=a9c709…37cf8b INFO [08-18|06:35:20.866] 🔨 mined potential block number=1 hash=a9c709…37cf8b INFO [08-18|06:35:20.869] Commit new mining work number=2 txs=0 uncles=0 elapsed=3.373ms INFO [08-18|06:35:36.470] Successfully sealed new block number=2 hash=7f96ee…bf362a INFO [08-18|06:35:36.473] 🔨 mined potential block number=2 hash=7f96ee…bf362a Generating DAG in progress 가 반복되다가 블럭이 생성되는 경우가 있다.!! \u003e eth.getBalance(eth.accounts[0]) 1.0000099999999999999999e+24 \u003e eth.getBalance(eth.accounts[1]) 100 이후에 이더가 전송된 것을 확인 가능하다.!! ","date":"2018-08-01","objectID":"/ethereum-dapp1/:3:0","tags":["blockchain","ethereum"],"title":"Setup private ethereum blockchain network","uri":"/ethereum-dapp1/"},{"categories":["blockchain"],"content":"간단한 geth 명령어는 다음과 같다. 블록 갯수 체크 \u003e eth.blockNumber 채굴에 성공시 보상받는 계좌 조회 \u003e eth.coinbase 다른 계정으로 옮기고 싶을때 \u003e miner.setEtherbase(eth.accounts[1]) \u003e eth.coinbase 진행 기다리는 트랜잭션 확인 \u003e eth.pendingTransactions [{ blockHash: null, blockNumber: null, from: \"0x241530b417837cdcce8036a7b7f095995509d5c9\", gas: 90000, gasPrice: 18000000000, hash: \"0x8aff56402bbbe8547dca097ba443a3d668bbac4cd73477e8613fc2c74ca8b002\", input: \"0x\", nonce: 1, r: \"0xc029343dac203271bee3de5d73a586e8740a547342b61f3aebdbbbb0ac4bd0a8\", s: \"0x27112e6e766ddfccf55123d057fe130f7b39e99f76b36c0aefa140168cdd36f9\", to: \"0xca09b7785f4179988e14ee87cfe7e93933a002c7\", transactionIndex: 0, v: \"0x41\", value: 1000 }] ","date":"2018-08-01","objectID":"/ethereum-dapp1/:3:1","tags":["blockchain","ethereum"],"title":"Setup private ethereum blockchain network","uri":"/ethereum-dapp1/"}]